{"./":{"url":"./","title":"Introduction","keywords":"","body":"My Awesome BookMy Awesome Book 欢迎！！！ Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/command-helps.html":{"url":"linux-common-knowledge/command-helps.html","title":"linux常用知识","keywords":"","body":"命令使用文档命令使用文档 whatis command简要说明命令作用正则用法: whatis -w 'loca*' info command说明更加详细 man command查看命令的说明文档 which command查看命令的安装(二进制)路径 whereis command查看命令的搜索路径 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-30 17:04:03 "},"linux-common-knowledge/tar.html":{"url":"linux-common-knowledge/tar.html","title":"tar/split 解压缩操作","keywords":"","body":"tar 解压缩命令1. 压缩2. 解压3. 分卷压缩说明:4. 分卷解压5. 批量解压6. gzip 与 tartar 解压缩命令 1. 压缩 tar -czvf 20180725baike.tar.gz 2018-07-25/ .. 1. options说明 -c //创建一个压缩文件 -z //具有gzip的属性, 一般gz结尾 -v //压缩过程显示档案 -f //后接压缩文件名, f必须在options的末尾 2. 解压 tar -xzvf 20180725baike.tar.gz 2018-07-25/ .. - options说明 -x 解压缩文件指令 其他的对应上一条命令 3. 分卷压缩 分卷实际上用到的是split命令,用来切割文件 tar cvzpf - baike | split -d -b 50m - baike.tar.gz. 说明: 将eclipse文件夹分卷压缩 表示命令的输入和输出参数 每卷50m baike.tar.gz. 是压缩文件的起始文件 压缩完以后会命名尾x00, x01, x02 4. 分卷解压 cat baike.tar.gz.* > eclipse.tar.gz //先合并 tar -xzvf eclipse.tar.gz //解压 cat baike.tar.gz.* | tar -xzvf // 一步解决 5. 批量解压 ls *.tar.gz | xargs -n1 tar xzvf # tar命令支持批量解压, 如 tar *, 所以要用脚本 6. gzip 与 tar # .tar文件相当于一个文件夹 # 压缩 gzip xx.tar # 将自动生成xx.tar.gz的压缩文件, 并替换掉之前的xx.tar # 解压 gzip -d xx.tar.gz # 将解压文件为xx.tar并替换掉原来的xx.tar.gz Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/rsync-chuan-shu-tong-bu-wen-jian.html":{"url":"linux-common-knowledge/rsync-chuan-shu-tong-bu-wen-jian.html","title":"rsync  传输同步文件","keywords":"","body":"rsync同步1. 示例rsync同步 1. 示例 rsync -rvptgo maxiaoteng@59.110.160.185:~/crawler/baike_scrapy/data2/html _files/baike/20180725baike.tar.gz /data/ftp/public/crawler_data/baike/ 这个命令共4部分: rsync options -rvptgo -r 递归拷贝目录 -v 显示命令执行详细信息 -p 保持文件属性 -t 保持文件时间信息 -g 保持文件属组信息 -o 保持文件属主信息 origin_path maxiaoteng@59.110.160.185:~/crawler/baike_scrapy/data2/baike/20180725baike.tar.gz target_path /data/ftp/public/crawler_data/baike/ rsync -rvptgo --password-file=/root/crawler/news/rsync.ps maxiaoteng@59.110.160.185:/home/maxiaoteng/crawler/news/data/ /root/crawler/news/data Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/supervisor.html":{"url":"linux-common-knowledge/supervisor.html","title":"supervisor 进程守护","keywords":"","body":"supervisor 进程守护安装生成配置文件启动常用管理命令配置进程web界面管理 可视化进程管理开机自启动supervisor 进程守护 supervisor使用python编写的进程守护应用, 可以在进程意外中断后重启 安装 sudo yum install supervisor # 或者 sudo pip install supervisor 生成配置文件 sudo echo_supervisord_conf > /etc/supervisord.conf # 如果提醒没有权限, 可以先echo_supervisord_conf 获取内容, 创建配置文件后复制进去 # 修改conf文件的配置, supervisor将加载文件夹下的所有conf文件: [include] files = /etc/supervisor/conf.d/*.conf conf文件demo[program:scrapyd] command=scrapyd ; the program (relative uses PATH, can take args) process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;directory=~ ; directory to cwd to before exec (def no cwd) autostart=true ; start at supervisord start (default: true) startsecs=1 ; # of secs prog must stay up to be running (def. 1) startretries=3 ; max # of serial start failures when starting (default 3) autorestart=True ; when to restart if exited after running (def: unexpected) stopsignal=KILL ; signal used to kill process (default TERM) 启动 supervisord # 启动supervisor服务 supervisord -c /etc/supervisord.conf supervisorctl -c /etc/supervisord.conf status > mongodb RUNNING pid 2366, uptime 0:01:00 ... 常用管理命令 重启supervisor service supervisor restart 启动相应进程 supervisorctl start app 停止进程 supervisorctl start app 停止所有进程 supervisorctl start all 载入新的配置文件 supervisorctl reload 更新修改的配置文件, 没改变的不影响 supervisorctl update 配置进程 配置文件位置:/etc/supervisor/conf.d/.. 每个进程设一个配置文件, 以conf结尾 内容: [program:mongodb] directory = /root/amazon_server # 运行目录 command = /usr/bin/mongod -port 27017 --dbpath /vr/lib/mongo autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 stopsignal= KILL 错误, 返回127是命令行错误 web界面管理 可视化进程管理 修改supervisord.conf的 inet_http_server部分 [inet_http_server] ; inet (TCP) server disabled by default port=0.0.0.0:9001 ; 开放公网ip(ip_address:port specifier, *:port for all iface) username=xxxxxxx ; (default is no username (open server)) password=xxxxxxx ; (default is no password (open server)) 开机自启动 unix自启动, 需要将命令添加到/etc/rc.local 添加内容: /usr/bin/supervisord -c /etc/supervisord.conf 查看两个文件的绝对位置: find / -name supervisord > /usr/local/python/bin/supervisord Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-05 15:30:00 "},"linux-common-knowledge/crontab.html":{"url":"linux-common-knowledge/crontab.html","title":"Crontab","keywords":"","body":"Cron1. 命令的使用直接编辑2. 命令格式3. 不发送电子邮件4. 查看日志排除问题5. crontab设置scrapy脚本的定时任务6. 环境变量的问题7. docker中Cron 1. 命令的使用 列出当前任务crontab -l 备份当前任务列表 crontab -l > my_cron 编辑一个命令文件 nano my_crontab 将编辑好的命令生效 crontab my_crontab 重启crondservice crond restart 直接编辑 不建议这么做，通常是编辑文件然后生效，这样确保有备份 crontab -e # 任何用户都可以用 vim /etc/crontab # 仅限root用户, 通常用于给其他用户指定定时任务, 或者需要root来运行任务 2. 命令格式 # 文件格式说明 # ——分钟（0 - 59） # | ——小时（0 - 23） # | | ——日（1 - 31） # | | | ——月（1 - 12） # | | | | ——星期（0 - 7，星期日=0或7） # | | | | | # * * * * * 被执行的命令 命令用空格分隔， 不用的, 任意可能的使用*代替 多个值用','分割 */2 每两分钟，每两天。。。(0-23)/211点前每两小时一次 3. 不发送电子邮件 如果输出结果来自crontab, 那么cron守护进程会使用电子邮件发送给用户, 位置/var/spool/mail/ec2-user, 需要将命令重定向到 >/dev/null 2>&1 4. 查看日志排除问题 位置: /var/log/cron 5. crontab设置scrapy脚本的定时任务 编写shell脚本 ``` #!/bin/sh . ~/.bash_profile source /etc/profile # 切换到scrpay命令下 cd /home/ec2-user/crawler/mangoplate_scrapy/ # 依次启动爬虫 /usr/local/bin/scrapy list /usr/local/bin/scrapy crawl codetable /usr/local/bin/scrapy crawl mangoplate # 去重 /usr/bin/python3 deduplicate.py ``` 6. 环境变量的问题 由于crontab只加载/ect/environment，并不加载/etc/profile和~/.bash_profile，所以需要在脚本里手动添加环境变量 . /etc/profile . ~/.bash_profile export xx/xx/xx.conf # 可以导入需要的配置文件 考虑到crontab中设置复杂不一, 应该规范shell命令的格式, 确保每次使用绝对路径, 适用于crontab的运行 crontab的任务只有一行运行shell脚本的命令即可 如果不适用shell脚本，也可以这样： 0 10 * * 5 source ~/.bashrc;cd /home/xiaoteng/code/aspex_tickets/src && python3 all_tickets.py >/dev/null 2>&1 7. docker中 由于docker默认不启动，启动容器后执行如下: # 后台启动cron /usr/sbin/crond -i Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-21 22:17:03 "},"linux-common-knowledge/shell-command.html":{"url":"linux-common-knowledge/shell-command.html","title":"shell command","keywords":"","body":"开头注释变量等待sleep字符串环境变量shell流程控制并行运行多个命令sh样本对目录和文件的操作错误 开头 #!/bin/sh 脚本以.sh结尾, 并且要使用chmod +x filename.sh来使文件为可执行脚本 注释 注释以\" # \" 来开始, 占据一行 变量 定义变量时,变量名不加美元符号($) a=\"shell\" 变量名和等号之间不能有空格, 除了显式赋值, 还可以用语句赋值 for file in 'ls /etc' 使用变量用${}表示(花括号虽然可以不用, 但尽量使用) echo \"a is ${a}\" 等待sleep sleep 5 # 默认为秒 sleep 5s sleep 5m sleep 5h sleep 5d 字符串 单引号的变量和转义字符是无效的, 只会原样输出 双引号可以有变量和转义字符, 比如: your_name='qinjx' str=\"Hello, I know your are \\\"$your_name\\\"! \\n\" 环境变量 使用export导入环境变量, 命令可以直接使用 # 运行命令的两种方法 /usr/bin/python /XXX/xx.py export shell流程控制 if语句 if condition then comamnd1 command2 command3 elif condition2 command4 else command5 fi for for var in item1 item2... itemn do command1 ... commandn done while while condition do command done 并行运行多个命令 & #!/bin/sh # 启动爬虫 /usr/bin/python3 /home/ec2-user/crawler/baemin/baeminApp3.0.py 0 >/dev/null 2>&1 & /usr/bin/python3 /home/ec2-user/crawler/baemin/baeminApp3.0.py 1 >/dev/null 2>&1 & 解释: 最后的 &符号可以使命令并行运行 >重定向符号 /dev/null # 代表空设备文件 2> # 代表stderr标准错误 &1 # 中的&表示等同,就是2的输出等同于标准输出 1 # 表示系统标准输出 >/dev/null等同于1>/dev/null >/dev/null 2>&1 # 表示正常输出重定向到null, 错误输出重定向到1, 也是null 2>&1 >/dev/null # 表示错误输出重定向到1, 即屏幕, 然后正确输出重定向到null sh样本 兼容crontab的运行, 设置全面的环境变量和路径 #!/bin/sh # 指定脚本解释器 # 生效当前用户的配置, 通常用在crontab中 source ~/.bash_profile source /etc/profile # 添加环境变量, 比如scrapy创建在了/usr/local/bin # 可以通过echo $PATH来测试是否添加成功 SCRAPY_HOME=/usr/local/bin export SCRAPY_HOME PATH=$PATH:$SCRAPY_HOME export PATH # 生效虚拟环境 source /data2/home/maxiaoteng/miniconda3/bin/activate /data2/home/maxiaoteng/miniconda3/envs/sjy_conda # 切换到目录下 cd /home/ec2-user/uber/crawler/python/zomato/ # 启动爬虫 /usr/bin/python3 zomato_apac.py #!/bin/sh # 启动爬虫 /usr/bin/python3 /home/ec2-user/crawler/baemin/baeminApp3.0.py 0 >/dev/null 2>&1 & /usr/bin/python3 /home/ec2-user/crawler/baemin/baeminApp3.0.py 1 >/dev/null 2>&1 & 对目录和文件的操作 判断操作 #!/bin/bash # 如果数据文件存在,先删除 data_path=/home/maxiaoteng/ss.txt if [ ! -f $data_path ];then echo 不存在 else echo 存在, 我删掉了 rm -f $data_path fi 说明 -d # 判断目录是否存在 -f # 判断文件是否存在 -x # 判断路径是否存在并有可执行权限 -n # 判断变量是否为空 if [ ! -n $var ] ; then echo var 变量为空 fi = # 判断变量是否相等 if [ $var1 = $var2 ]; then echo var1和var2相等 fi 错误 syntax error: unexpected end of file错误 是因为\\r\\n不兼容引起的 解决: dos2unix file.sh Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-27 14:59:51 "},"linux-common-knowledge/expect.html":{"url":"linux-common-knowledge/expect.html","title":"expect command","keywords":"","body":"expectexpect 和shell类似, 主要用于带交互的命令脚本, 比如rsync执行时输入密码: #!/bin/expect set timeout 30 spawn rsync -rvptgo maxiaoteng@59.110.160.185:/home/maxiaoteng/crawler/news/data/ /root/crawler/news/data expect \"password:\" send \"XXXXXX\\r\" interact #!/usr/bin/expect set timeout 360 set host [lindex $argv 0] set port [lindex $argv 1] set username [lindex $argv 2] set password [lindex $argv 3] set src_file [lindex $argv 4] set dest_file [lindex $argv 5] #spawn scp $src_file $username@$host:$dest_file spawn scp -P $port -r $username@$host:$src_file $dest_file expect { \"(yes/no)?\" { send \"yes\\n\" expect \"*assword:\" { send \"$password\\n\"} } \"*assword:\" { send \"$password\\n\" } } expect \"100%\" expect eof #!/bin/sh src_dir=/data/huiyzl/runtime/ dest_dir=/data/huiyzl/ host=219.135.214.146 port=60203 username=root password=LENOVOap123 # 目录不存在，则创建，如果存在先删除再创建 if [ ! -d $src_dir ]; then mkdir -p $src_dir else rm -rf $src_dir mkdir -p $src_dir fi # 将远程服务器上的文件拷贝到本机 ./expect_scp $host $port $username $password $src_dir $dest_dir echo \"end\" Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/wen-jian-ji-mu-lu-guan-li.html":{"url":"linux-common-knowledge/wen-jian-ji-mu-lu-guan-li.html","title":"文件及目录管理","keywords":"","body":"文件及目录管理创建和删除目录切换列出目录项查找目录和文件 find/location查看文件内容改变文件和目录权限文件创建别名, 创建软链接/硬链接管道和重定向设置环境变量应用举例文件及目录管理 创建和删除 创建: mkdir 删除: rm 删除非空目录: rm -r file_name 删除日志: rm *log 移动: mv 复制: cp 复制目录: cp -r 查看当下目录文件个数 find ./ |wc -l 复制目录: cp -r source_dir dest_dir 目录切换 切换位置: cd 切换上一级: cd .. 或: cd - 切换到home: cd ~ or cd 显示当前路径: pwd 更改当前工作路径为path: cd path 列出目录项 ls 按时间排序, 以列表方式显示: ls -lrt 查找目录和文件 find/location 搜寻文件或目录: find ./ -name 'core*' | xargs file 查找目标文件夹是否有obj文件 find ./ -name '*.o' 递归删除目录及子目录的.o文件 find ./ -name '*.o' -exec rm {} \\ find是实时查找, 要想更快, 可以使用locationlocation为文件系统建立索引数据库, 需要定期更新 location string # 寻找包含string的路径 updatedb # 更新索引 查看文件内容 cat, vi, head, tail, more cat -n # 显示的同时显示行号 ls -al | more # 按页显示列表内容 # 查看前10行 head -10 file_name # 显示文件第一行 head -1 file_name # 查看文件差距: diff file1 file2 # 查看后10行 tail -10 file_name # 动态显示文本更新 tail -f file_name 改变文件和目录权限 chmod 改变文件读写执行属性 chmod a+x myscript 文件创建别名, 创建软链接/硬链接 ln cc ccAgain : 硬链接, 删除一个, 另一个仍能找到 ln -s cc ccNew : 软链接, 删除源, 另一个无法使用 管道和重定向 批处理命令连接执行使用| 串联使用分号; 前面成功才执行,否则不执行使用&& 前面失败, 后面才执行使用|| ls /etc && echo suss! || echo failed. 等价: if ls /etc; then echo suss; else echo fail; fi 重定向 ls /etc > list 2> &1 # 将标准输出和标准错误输出重定向到同一文件 等价: ls /etc &> list echo aa >> a.txt 清空文件: : > a.txt 设置环境变量 .profile文件 PATH=$APPDIR:/opt/app/soft/bin:$PATH:/usr/local/bin:$TUXDIR/bin:$ORACLE_HOME/bin;export PATH 应用举例 查看文件中包含AAA 不包含BBB 的文件行数 cat log.log |grep AAA | grep -v BBB | wc -l Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/nano-editor.html":{"url":"linux-common-knowledge/nano-editor.html","title":"nano editor","keywords":"","body":"nano editor安装启动导航复制 剪切和粘贴nano editor 安装 # 如果没有请安装 yum install nano 启动 nano # 将新建一个文本文件 nano filename # 打开, 如果不存在则新建 Ctrl+G 显示帮助文档 Ctrl + X 退出 Ctrl + O 保存文档 Meta 键对应 Alt 导航 Ctrl + F/B 光标左右移动 Ctrl + P/N 光标上下移动 Ctrl + A/E 光标移动到行首/行尾 Ctrl + Y/V 上下换页 Ctrl + Spaces 光标移动到下一个字 Ctrl + _ + 行号,列好 移动到指定位置 复制 剪切和粘贴 Ctrl + ^ 起始位置选择 Meta(Alt)+ ^ 复制 Ctrl + k 剪切 Ctrl + u 粘贴 Ctrl + k 删除整行 Ctrl + w 搜索 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/tmux.html":{"url":"linux-common-knowledge/tmux.html","title":"tmux","keywords":"","body":"tmux 命令tmux 命令 tmux 采用c/s模型，输入tmux命令， 相当于开启了一个服务器（server），默认新建一个会话（session）,会话中默认新建一个窗口（window），窗口中默认新建一个面板（pane）。 新建一个session tmux [new -s 会话名 -n 窗口名] 默认进入第一个session tmux at [-t 会话名] 列出所有会话 tmux ls 关闭指定会话， 还有kill-server， kill-window， kill-pane tmux kill-session -t 会话名 按下 ctr+b以后 s 列出所有会话 c 创建窗口 w 列出所有窗口 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/history.html":{"url":"linux-common-knowledge/history.html","title":"history","keywords":"","body":"history命令history命令 参考： https://linuxtoy.org/archives/history-command-usage-examples.html history # 列出所有的历史命令，相当于cat了一个文件 ctrl + R # 在历史命令中搜索， enter直接执行，方向键会进入修改 执行上一条命令的四种方法： ↑ ！！ ！-1 Ctrl + P 配置 # vi ~/.bash_profile # 配置保存历史命令的总行数 HISTSIZE=450 HISTFILESIZE=450 # 历史命令的文件保存位置 HISTFILE=/root/.commandline_warrior # 历史命令是否去重 HISTCONTROL=ignoredups 由于history相当于cat了保存历史命令的文件，所以可以执行一些常规用法 1. history | grep xx # 过滤制定的命令 2. history | tail -5 # 显示最后5条命令 剔除历史文件中重复的命令 # export HISTCONTROL=erasedups 清楚历史记录 # history -c Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"linux-common-knowledge/ps.html":{"url":"linux-common-knowledge/ps.html","title":"ps 进程查看器","keywords":"","body":"Process Status -- ps进程查看1.Process Status -- ps进程查看 ps显示进程快照，使用top动态的显示进程信息 1. linux上进程有五种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGTSTP, SIGTTIN, SIGTTOU信号后停止运行运行) ps工具表示进程的五种状态 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C 列出指定命令的状况 –lines 每页显示的行数 –width 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 输出列的含义 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 进程的ID PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何 常用 # 显示所有进程 ps -A # 显示指定用户 ps -u root # 显示所有进程，连同命令行 ps -ef # ps grep 组合 ps -ef|grep ssh # 列出所有正在内存中的程序 ps aux Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-01 18:34:54 "},"linux-common-knowledge/vim.html":{"url":"linux-common-knowledge/vim.html","title":"vim使用","keywords":"","body":"使用vim使用vim basic q # 退出 wq # 保存退出 q! # 强制退出 yy # 复制一行 nyy # 复制多行 p # 粘贴 set number # 显示行号 set nonumber # 隐藏行号 u # 还原上一次操作 U # 还原整行的操作 删除 dw # 删除一个单词 d$ # 删除至行尾 dd # 删除一行 ndd # 删除多行 p # 将上次删除的内容复制到光标之后 更改 r # r 后输入要替换的字符 ce # 更改一个单词 c$ # 更改一行 定位 Ctrl + G # 显示当前的位置 G # 跳转到文件最后一行 line_no + G # 跳转到指定行 gg # 跳转到第一行 Ctrl + f # 下翻页 Ctrl + b # 上翻页 搜索 / + keyword # 搜索特定的词 n/N # 下/上搜索 ? #逆向查找使用 ctrl + o # 回到之前位置，可多次执行 vim 中执行外部命令 :! # Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-30 17:10:08 "},"linux-common-knowledge/system-setting.html":{"url":"linux-common-knowledge/system-setting.html","title":"系统设置","keywords":"","body":"系统设置1. 修改dns查询添加配置重启服务注意: em1对应show命令查询的name执行如下, /etc/resolv.conf 便发生了更改将文件变成只读, 修改成自己期望的即可系统设置 1. 修改dns ``` vim /etc/resolv.conf nameserver 172.24.73.130 nameserver 172.24.73.131 ``` centos7引入新的机制 重启服务 ```查询 nmcli connection show 显示: NAME UUID TYPE DEVICE em1 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 802-3-ethernet em1 添加配置 nmcli con mod em01 ipv4.dns \"114.114.114.114 8.8.8.8\" 重启服务 nmcli con up em1 注意: em1对应show命令查询的name - Ubuntu修改方式: sudo vim /etc/resolvconf/resolv.conf.d/base # 添加多条 nameserver 8.8.8.8 nameserver 8.8.8.8 执行如下, /etc/resolv.conf 便发生了更改 resolvconf -u - 方法三, 简单粗暴 将文件变成只读, 修改成自己期望的即可 chattr +i /etc/resolv.conf chattr -i /etc/resolv.conf ## 2. 自启动 1. chkconfig 查看自启动的服务 2. 生效禁用 sudo systemctl disable/enable squid.service # 禁用输出 Removed symlink /etc/systemd/system/multi-user.target.wants/squid.service. ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-05 15:33:03 "},"linux-common-knowledge/other.html":{"url":"linux-common-knowledge/other.html","title":"other","keywords":"","body":"other1. top命令2. 查看端口命令3. kill命令4. 快捷键命令行技巧5. 运维命令6. 语言问题7. debian设置时区other 为终端设置代理 set http_proxy=http://127.0.0.1:1080 set https_proxy=http://127.0.0.1:1080 查看ip # 已被弃用 ifconfig: command not found #等价于 ip addr show diff # 比较文件 diff file1 file2 # 比较文件夹 diff -urNa dir1 dir2 curl curl -O http://mirror.bit.edu.cn/apache/kafka/2.1.0/kafka_2.11-2.1.0.tgz # 用于下载文件，按服务器上的文件名保存 curl -o filename url # 文件下载后重命名为filename curl --interface ppp0(192.168.0.0) https://httpbin.org/ip # 指定网络出口 1. top命令 ```Shell top ``` 查看内存占用等 使用 e/E 切换内存显示的单位 2. 查看端口命令 netstat netstat -tunlp # 用于显示tcp, udp的端口和进程的相关情况 -t (tcp) 仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化为数字 -l 仅列出在Listen(监听)的服务状态 -p 显示建立相关链接的程序名 netstat -tunlp |grep 8000 # 查看8000端口的占用情况 lsof 是一个列出当前系统打开文件的工具 lsof lsof -i:8080：查看8080端口占用 lsof abc.txt：显示开启文件abc.txt的进程 lsof -c abc：显示abc进程现在打开的文件 lsof -c -p 1234：列出进程号为1234的进程所打开的文件 lsof -g gid：显示归属gid的进程情况 lsof +d /usr/local/：显示目录下被进程开启的文件 lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长 lsof -d 4：显示使用fd为4的进程 lsof -i -U：显示所有打开的端口和UNIX domain文件 3. kill命令 netstat查看到占用,使用kill杀掉进程 kill -9 pid kill pid kill 给予root权限，文件夹的所有文件 sudo chmod -R 777 ./dir/* sudo chown -R maxiaoteng ./dir/* # 修文件所有人 4. 快捷键 命令行技巧 删除整行 ctrl+u 删除前一个单词 ctrl+w 不删除直接下一行重新输入 ctrl+c 5. 运维命令 一条命令杀进程 ps -efww|grep -w 'process_test.py'|grep -v grep|cut -c 9-15|xargs kill -9 ps -ef 查看所有进程命令 grep -w 是强制PATTERN仅完全匹配字符 grep -v grep 列出的进程中去除含有grep的进程 cut -c 9-15 截取pid号 kill -9 强行杀掉制定进程(通过杀掉关联进程) 6. 语言问题 nas是debian系统, 无法输入和显示中文 添加语言支持 apt install -y locale locales dpkg-reconfigure locales # 选择Zh开头的 # 输入法不一定需要 配置locale 默认locale: /etc/default/locale ~/.bashrc 优先级最高, 在这里设置 export LANG=\"en_US.UTF-8\" export LC_CTYP=\"en_US.UTF-8\" export LC_NUMERIC=\"en_US.UTF-8\" export LC_TIME=\"en_US.UTF-8\" export LC_COLLATE=\"en_US.UTF-8\" export LC_MONETARY=\"en_US.UTF-8\" export LC_MESSAGES=\"en_US.UTF-8\" export LC_PAPER=\"en_US.UTF-8\" export LC_NAME=\"en_US.UTF-8\" export LC_ADDRESS=\"en_US.UTF-8\" export LC_TELEPHONE=\"en_US.UTF-8\" export LC_MEASUREMENT=\"en_US.UTF-8\" export LC_IDENTIFICATION=\"en_US.UTF-8\" export LC_ALL= 生效 source ~/.bashrc 7. debian设置时区 # vim /etc/timezone Asia/Shanghai # ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 或者 # cp -a /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-03 13:38:55 "},"version-control/svn-git-commands.html":{"url":"version-control/svn-git-commands.html","title":"版本控制","keywords":"","body":"svn - git 基本操作1. 功能2. 撤销修改3. 配置git4. 解决每次git pull/push都要输入密码的问题5. 远程代码库回滚6. 分支操作7. clone 指定分支svn - git 基本操作 1. 功能 初始化仓库 git init 检出代码到本地 # git git clone git_url # svn svn checkout svn_url 将文件添加到版本库 # git git add . # 添加所有内容到版本库 git add file_name # 添加指定文件到版本库 # svn svn add new_file 提交更改 # git git commit -m '更改说明' # 提交更改到本地版本库 git push # 推送代码到默认线上仓库 git push gitlab master # 推送代码到指定的线上仓库 git push -u github master # 将会修改设置默认的推送流 # svn svn commit -m '更改说明' # 提交更改到版本库, 并推送到线上 重置暂存区 git reset --hard 显示提交记录 git log # 显示commit记录 git log --graph --oneline firstbranch_name secondbranch_name # 按图形显示 检出某一版本 git checkout commit_id # 检出某一id的状态 git checkout branch_name # 检出指定分支的代码 更新本地代码 # git git pull git pull remote_name branch_name # svn svn update svn update -r m svn_path # 更新指定版本的代码 查看当前状态 # git git status # svn svn status 查看代码库的信息 # git git remote -v # 显示远程仓库的链接 # svn svn info # 显示仓库所有信息 svn info |grep URL/http cat .svn/entries |grep http 比较工作区和代码库的差异 # git git diff # 比较工作区和暂存区的差异 git diff commit_1 commit_2 # 比较两次提交的差异 git diff --staged # 比较暂存区和代码库的差异 # svn svn diff remote 管理 # 1. 查看remote git remote -v # 2. 增加新的remote** git remote add gitlab git_url git push gitlab master # 3. 修改remote的url** git remote set-url gitlab git_url rm 撤销已加入追踪的文件 # 删除本地和仓库 git rm file_name git rm -r directory/ # 只删除仓库, 保留本地文件 git rm --cached file_name # svn svn rm --keep-local my_important_file # 将只删除版本库的文件，而不删除本地 2. 撤销修改 首先了解一下,git有三个概念(工作区, 暂存区 和 仓库区) 通常有几种情况: 撤销工作区的修改, 还没有add到版本库 # git git checkout . # 撤销全部更改 git checkout XX.file # 撤销特定文件 # svn svn revert -r path_name # 丢弃未提交的代码 svn revert file_name 撤销暂存区的修改, add到了暂存区,还没有commit git reset --hard git reset HEAD //撤销add的所有更改 git reset HEAD XX.file //撤销XX.file的添加 撤销版本库里面的修改 git revert HEAD //撤销之前的commit git revert HEAD^ //撤销前前一次的commit git revert commit-id //撤销指定版本, 也会作为一次提交commit 注意: git revert是恢复指定版本的修改, 作为一次新提交上传,版本会递增 3. 配置git git config --list 查看配置 全局修改，建议将最常用的账号配置到globle中 git config --global user.name xxx git config --global user.email xxx@xxx.xxx 当前项目 git config user.name xxx git config user.email xxx@xxx.xxx 4. 解决每次git pull/push都要输入密码的问题 `git config --global credential.helper store` **基本每次都要配置如下:** ``` git config --global user.name maxiaoteng git config --global user.email maxiaoteng@yunfutech.com git config --global credential.helper store ``` 5. 远程代码库回滚 本地代码库回滚 git reset --hard commit-id :回滚到commit-id，讲commit-id之后提交的commit都去除 git reset --hard HEAD~3：将最近3次的提交回滚 远程代码库回滚 git checkout the_branch git pull git branch the_branch_backup //备份一下这个分支当前的情况 git reset --hard the_commit_id //把the_branch本地回滚到the_commit_id git push origin :the_branch //删除远程 the_branch git push origin the_branch //用回滚后的本地分支重新建立远程分支 git push origin :the_branch_backup 6. 分支操作 新建分支 git checkout -b iss53 # 相当于 git branch iss53 git checkout iss53 切换分支 git checkout master # 注意切换分支的时候最好保持一个清洁的工作区域 合并到master git checkout master git merge hotfix # 如果master修改了影响开发分支的内容，可以 git merge master 删除分支 git branch -d hotfix # 合并之后，旧的分支不再有用，可以删除 git branch -D hotfix # 未合并过的分支如果删除会提醒，用D强制删除 冲突解决 # 冲突的文件会保留在工作区，修改后使用git add # 一旦暂存，表示冲突解决，然后git status，git commit 来提交即可 查看当前分支 git branch git branch -v # 查看各分支当前的最新版本 git branch --merged # 查看合并到当前分支的其他分支 git branch --no-merged # 查看未合并的分支 7. clone 指定分支 clone指定分支 # 默认clone master git clone XXX.git # 制定 git clone -b branch-name XXX.git 如果clone了master，其他分支隐藏 # 查看 git branch -a # 快速检出分支 # detached Head， 所做修改不会提交到任何分支 git checkout origin/feature # 快速创建一个本地分支 git checkout -b feature origin/feature git checkout -t origin/feature #会在本地创建一个和远程分支同名 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 13:51:09 "},"version-control/git-advanced.html":{"url":"version-control/git-advanced.html","title":"git 进阶","keywords":"","body":"git 进阶git 与 hookswebhooksgit 进阶 git 与 hooks git 在执行特定的重要动作时能触发自定义的脚本，包含客户端和服务端。 客户端钩子由诸如提交和合并这样的操作所调用， 服务器端的钩子作用于诸如接收被推送的提交这样的联网操作。 你可以随心所欲地运用这些钩子。hooks目录： .git/hooks可以执行shell脚本， 以及Perl，Python和Ruby等 webhooks 在用户push了代码之后， 自动回调一个你设定的http地址. 这是一个通用的解决方案, 可以根据需求来编写自己的脚本(比如发邮件,自动部署等) webhooks的请求方式是POST，body可以是JSON和formdata, 注意其中的密码是明文 { \"before\": \"fb32ef5812dc132ece716a05c50c7531c6dc1b4d\", \"after\": \"ac63b9ba95191a1bf79d60bc262851a66c12cda1\", \"ref\": \"refs/heads/master\", \"user_id\": 13, \"user_name\": \"123\", \"user\": { \"name\": \"123\", \"username\": \"test123\", \"url\": \"https://gitee.com/oschina\" }, \"repository\": { \"name\": \"webhook\", \"url\": \"http://git.oschina.net/oschina/webhook\", \"description\": \"\", \"homepage\": \"https://gitee.com/oschina/webhook\" }, \"commits\": [ { \"id\": \"ac63b9ba95191a1bf79d60bc262851a66c12cda1\", \"message\": \"1234 bug fix\", \"timestamp\": \"2016-12-09T17:28:02 08:00\", \"url\": \"https://gitee.com/oschina/webhook/commit/ac63b9ba95191a1bf79d60bc262851a66c12cda1\", \"author\": { \"name\": \"123\", \"email\": \"123@123.com\", \"time\": \"2016-12-09T17:28:02 08:00\" } } ], \"total_commits_count\": 1, \"commits_more_than_ten\": false, \"project\": { \"name\": \"webhook\", \"path\": \"webhook\", \"url\": \"https://gitee.com/oschina/webhook\", \"git_ssh_url\": \"git@gitee.com:oschina/webhook.git\", \"git_http_url\": \"https://gitee.com/oschina/webhook.git\", \"git_svn_url\": \"svn://gitee.com/oschina/webhook\", \"namespace\": \"oschina\", \"name_with_namespace\": \"oschina/webhook\", \"path_with_namespace\": \"oschina/webhook\", \"default_branch\": \"master\" }, \"hook_name\": \"push_hooks\", \"password\": \"pwd\" } Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"markdown/basic.html":{"url":"markdown/basic.html","title":"Markdown","keywords":"","body":"1. 表格https://github.com/younghz/Markdown 1. 表格 序号------ 关键词--------- 描述---- 采用对象 靠左 居中 靠右 大众点评 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-09-20 15:57:22 "},"markdown/Page_internal_jump.html":{"url":"markdown/Page_internal_jump.html","title":"Markdown 实现页内跳转","keywords":"","body":"Markdown 页内跳转方法一: Github Flavored Markdown Anchor应该能跳转到这里方法二: html 语法实现Markdown 页内跳转 方法一: Github Flavored Markdown Anchor 支持github, 其他需要安装插件 文本: [点击跳转](##应该能跳转到这里) ## 应该能跳转到这里 显示效果: 点击跳转 我是占位符 我是占位符 应该能跳转到这里 方法二: html 语法实现 支持github和gitbook等. 写法: 定义锚点 跳转回这里 ![照片占位](http://placeimg.com/640/480/any) ![照片占位](http://placeimg.com/640/480/any) 定义跳转点 点击这里跳转回去 效果展示: 定义锚点 跳转回这里 定义跳转点 点击这里跳转回去 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-09-20 16:00:11 "},"markdown/python-markdown.html":{"url":"markdown/python-markdown.html","title":"Python Markdown","keywords":"","body":"支持将Markdown格式的文本渲染成HTML页面渲染safe标签支持将Markdown格式的文本渲染成HTML页面 pip install markdown 渲染 # extensions是markdown的扩展支持, 包括扩展语法, 代码高亮和目录生成 post.body = markdown.markdown(post.body, extensions=[ 'markdown.extensions.extra', 'markdown.extensions.codehilite', 'markdown.extensions.toc', ]) safe标签 见template Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/built_in_module/built_in_module.html":{"url":"python-basics/built_in_module/built_in_module.html","title":"内置模块","keywords":"","body":"内置模块内置模块 CSV json 日志logging os 正则表达式 random time datetime subprocess multiprocessing html urllib string Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-30 15:43:06 "},"python-basics/built_in_module/re-model.html":{"url":"python-basics/built_in_module/re-model.html","title":"正则表达式","keywords":"","body":"正则表达式元字符()简单模式重复匹配使用正则表达式示例正则表达式 参考ubuntu import re # 要求从开头匹配 match(pattern, string, flags=0) # 匹配整个文本 search in: re.match('test', 'test') 元字符() . 匹配任意字符 ^在开头使用'^'将匹配类别的补集,比如: 5匹配除5之外的任意字符 $ *字符类中的*并不匹配字符*,而是指定前一个字符可以被匹配零次或更多次 +和*类似, 只是要求至少要出现一次 ? 用法和* + 类似, 匹配0次或1次 { {m,n}匹配m{2,4}匹配有2个,3个或4个m的字符串, m默认为0,n默认无穷大 [\"[\" 和 \"]\"用来制定一个字符类别, 比如: [abc], [a-c], 将匹配abc的任一字符 \\后面可加不同的元字符, 来取消元字符功能, 只作为普通字符. 比如: [][\\], 将匹配] [ 和 \\ \\d 匹配任何十进制数；它相当于类 [0-9] \\D 匹配任何非数字字符；它相当于类 [^0-9] \\s 匹配任何空白字符；它相当于类[ \\t\\n\\r\\f\\v] \\S 匹配任何非空白字符；它相当于类 [^ \\t\\n\\r\\f\\v] \\w 匹配任何字母数字字符；它相当于类 [a-zA-Z0-9_] \\W 匹配任何非字母数字字符；它相当于类[^a-zA-Z0-9_] 以上字符集可以包含在字符类中, 比如: [\\s,.] 匹配所有空白字符 , . | ( ) 注意: 元字符在类别中不起作用, 比如: [abc$]将匹配'$', '$'作为普通字符 简单模式 字符匹配, 不定长匹配 重复匹配 除了匹配不定长字符集,另一个功能就是可以指定正则表达式的一部分的重复次数 使用正则表达式 编译正则表达式 import re p = re.compile('ab*') print(p) # 如果匹配反斜杠\\, 尽量使用raw字符串, r'\\strt', r'\\n'表示'\\'和'n'两个字符, '\\n'表示换行. 执行匹配 match() 决定RE是否在字符串刚开始的位置匹配search() 扫描字符串, 找到RE匹配的位置findall() 找到RE匹配的所有子串,并把他们作为一个列表返回finditer() 找到RE匹配的所有子串,并把他们作为一个迭代器返回 如果没有匹配, march和search返回None, 成功的话,返回MatchObject实例 取回结果MatchObject 有几个方法: group() 返回被RE匹配的字符串 start() 返回匹配开始的位置 end() 返回匹配结束的位置 span() 返回一个元组,包含匹配(开始,结束)的位置 示例 替换字符串中的换行符import re # 构造pattern, 使用sub替换, 参数(pattern, '', str, count, flag) # ''用来替换的内容, str是操作的字符串, count数量 pattern = re.compile(r'[\\r\\n]') new_s = re.sub(pattern, ' ', '22222 \\r 33333 \\n 44444 \\r\\n 55555') # 或者直接使用 new_s = re.sub(r'[\\r\\n]', ' ', '22222 \\r 33333 \\n 44444 \\r\\n 55555') print(new_s) 分隔字符串 import re re.split('[_#|]','this_is#a|test') >> ['this', 'is', 'a', 'test'] 使用非贪婪匹配 import re tags = re.findall(r\"mm(.+?)ss\", target_str) # . 匹配所有 # + 至少一个 # ？ 非贪婪匹配，查找符合条件及停止 # mmkhgoiurwss 返回 khgoiurw # 不包含括号，将匹配所有 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-15 11:17:10 "},"python-basics/built_in_module/os-model.html":{"url":"python-basics/built_in_module/os-model.html","title":"os Model","keywords":"","body":"OS Model关于 os.path 和 fileOS Model 关于 os.path 和 file __file__ # 如果当前文件调用, 显示绝对路径, 其他文件调用, 则只显示filename 使用当前文件的绝对路径: os.path.dirname(os.path.abspath(__file__)) 其中: abspath = realpath Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/built_in_module/logging.html":{"url":"python-basics/built_in_module/logging.html","title":"logging模块","keywords":"","body":"python logging1. 等级2. 实践2. 配置3. 遇到的bugpython logging logging可以发送到控制台, 文件, 网络 1. 等级 a. logging.CRITICAL b. logging.ERROR c. logging.WARNING d. logging.INFO e. logging.DEBUG 2. 实践 使用__name__作为logger的名称 便于知道日志来自哪个模块 异常捕捉时使用traceback记录 logger.error('Failed to open file', exc_info=True) logger.exception('Failed to open file') 不要在模块层次创建logger 因为在模块层面获得的logger，在加载了配置文件后都将失效 而应该在使用时获得 import logging def foo(): logger = logging.getLogger(__name__) logger.info('Hi, foo') class Bar(object): def __init__(self, logger=None): self.logger = logger or logging.getLogger(__name__) def bar(self): self.logger.info('Hi, bar') 使用json或YAML记录配置 logging.json { \"version\": 1, \"disable_existing_loggers\": false, # 如果为True,则模块层面获取的logger都失效 \"formatters\": { \"simple\": { \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\" } }, \"handlers\": { \"console\": { \"class\": \"logging.StreamHandler\", \"level\": \"DEBUG\", \"formatter\": \"simple\", \"stream\": \"ext://sys.stdout\" }, \"info_file_handler\": { \"class\": \"logging.handlers.RotatingFileHandler\", \"level\": \"INFO\", \"formatter\": \"simple\", \"filename\": \"info.log\", \"maxBytes\": 10485760, \"backupCount\": 20, \"encoding\": \"utf8\" }, \"error_file_handler\": { \"class\": \"logging.handlers.RotatingFileHandler\", \"level\": \"ERROR\", \"formatter\": \"simple\", \"filename\": \"errors.log\", \"maxBytes\": 10485760, \"backupCount\": 20, \"encoding\": \"utf8\" } }, \"loggers\": { \"my_module\": { \"level\": \"ERROR\", \"handlers\": [\"console\"], \"propagate\": \"no\" } }, \"root\": { \"level\": \"INFO\", \"handlers\": [\"console\", \"info_file_handler\", \"error_file_handler\"] } } 读取json配置 import json import logging.config def setup_logging( default_path='logging.json', default_level=logging.INFO, env_key='LOG_CFG' ): \"\"\"Setup logging configuration \"\"\" path = default_path value = os.getenv(env_key, None) if value: path = value if os.path.exists(path): with open(path, 'rt') as f: config = json.load(f) logging.config.dictConfig(config) else: logging.basicConfig(level=default_level) 2. 配置 基础配置 import logging logFilename = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../log/ele.log') log_dir = os.path.split(logFilename)[0] if not os.path.exists(log_dir): os.makedirs(log_dir) logging.basicConfig( level = logging.DEBUG, # 定义输出到文件的log级别， format = '%(asctime)s %(filename)s : %(levelname)s %(message)s', # 定义输出log的格式 datefmt= '%Y-%m-%d %A %H:%M:%S', # 时间 filename = logFilename, # log文件名 filemode = 'w' ) 日志是线程安全的，但是多进程会有问题，通常解决思路： 每个进程单独写一个文件 使用socketHandler，启一个线程或进程单独处理 多进程mulitprocessing中的Queue队列存储日志，单独一个进程处理 3. 遇到的bug 1. [多进程+ 多线程 + logging遇到的问题](https://mozillazg.com/2016/09/python-threading-multiprocessing-logging-equal-deadlock.html) 1. 主要问题是创建子进程时, 如果使用multiprocessing, 会fork父进程的所有的状态, 此时如果某个线程正在写日志, logging的锁将被fork到子进程中, 导致子进程logging锁无法释,产生死锁 2. 解决办法: 使用subprocess, 或只使用多进程+logging Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-27 15:04:39 "},"python-basics/built_in_module/csv-model.html":{"url":"python-basics/built_in_module/csv-model.html","title":"CSV模块","keywords":"","body":"csv 模块普通读写文件按dict读写文件其他读写超大文件时csv 模块 csv用来读写csv文件 import csv 普通读写文件 with open(origin_path,'r',encoding='utf-8') as fr: csv_reader=csv.reader(fr) addresses={} for row in csv_reader: if csv_reader.line_num==1: continue 按dict读写文件 写文件 ``` if not os.path.exists(data_path): with open(data_path, 'w', encoding='utf-8', newline='') as f: csv_writer = csv.DictWriter(f, fieldnames=field) csv_writer.writeheader() with open(data_path, 'a', encoding='utf-8', newline='') as f: csv_writer = csv.DictWriter(f, fieldnames=field) csv_writer.writerow(item) ``` 读文件 读取的row类型: `collections.OrderedDict`, 有get属性, 使用dict(row)转为普通dict ``` with open(author_links_path, 'r', encoding='utf-8', newline='') as f: csv_reader = csv.DictReader(f, fieldnames=self.author_link_field) for row in csv_reader: if csv_reader.line_num == 1: continue links.append(dict(row)) ``` 其他 用在csv的read和writer方法, delimiter=',' # 分隔符 quotechar='\"' # 引用符, 将内容用引号包含,避免歧义 读写超大文件时 参考stackoverflow 报错: _csv.Error: field larger than field limit (131072) - 原因: csv文件包含特别大的field, 需要修改最大显示 - 解决代码 ``` import sys import csv maxInt = sys.maxsize decrement = True while decrement: # decrease the maxInt value by factor 10 # as long as the OverflowError occurs. decrement = False try: csv.field_size_limit(maxInt) except OverflowError: maxInt = int(maxInt/10) decrement = True ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/built_in_module/json.html":{"url":"python-basics/built_in_module/json.html","title":"json","keywords":"","body":"JSON格式1. 序列化和反序列化2. 序列化其他第三方 demjsonJSON格式 用于序列化, 内置包picking, 将变量序列化成bytes类型, 兼容差,仅适用Python 1. 序列化和反序列化 json默认支持序列化Python的数据类型: list, dict等 import json d = dict(name='Bob', age=20, score=88) # 序列化 s = json.dumps(d) s == '{\"age\": 20, \"score\": 88, \"name\": \"Bob\"}' s = json.dumps(s, separators=(',', ':'), ensure_ascii=False) # separators 用来解决:后面带空格问题, ensure_ascii解决中文编码问题 # 反序列化 d = json.loads(s) 2. 序列化其他 序列化类 ``` import json class Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = score # 实现一个方法用于序列化 def student2dict(std): return { 'name': std.name, 'age': std.age, 'score': std.score } # 反序列化 def dict2student(self, d): return Student(d['name'], d['age'], d['score']) s = Student('Bob', 20, 88) # 便可序列化和反序列化对象 str = json.dumps(s, default=student2dict) student_class = json.loads(json_str, object_hook=dict2student) # 序列化的高阶写法 json_s = json.dumps(s, default=lambda obj: obj.__dict__) ``` 第三方 demjson http://deron.meranda.us/python/demjson/ 用于编码和解码json数据,data = [ { 'a' : 1, 'b' : 2, 'c' : 3, 'd' : 4, 'e' : 5 } ] json = demjson.encode(data) >> data == [{\"a\":1,\"b\":2,\"c\":3,\"d\":4,\"e\":5}] text = demjson.decode(json) >> {'a': 1, 'c': 3, 'b': 2, 'e': 5, 'd': 4} Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-26 11:49:09 "},"python-basics/built_in_module/random.html":{"url":"python-basics/built_in_module/random.html","title":"ramdom","keywords":"","body":"RandomRandom random() # 0.0 ~ 1.0 randint(3, 6) # 随机整形数 uniform(4, 6) # 指定随机数的范围 choice(seq) # 返回seq的随机元素 getrandbits(n) # 以长整型返回n个随机位 shuffle(seq) # 随机打乱一个序列 sample(seq, n) # 返回seq中选择n个随机且独立的元素 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 16:31:44 "},"python-basics/built_in_module/time.html":{"url":"python-basics/built_in_module/time.html","title":"time","keywords":"","body":"time 模块time 模块 time归类在Generic Operating System Services中，是围绕着 Unix Timestamp 进行的，其所能表述的日期范围被限定在 1970 - 2038 之间。 time.time() # 返回float类型的时间 now_time = time.time() print(now_time) // → 1532328411.9794893 int(time.time()) 返回int的时间，通常做时间戳 print(int(time.time()) // → 1532328411 time.localtime(time.time()) 返回一个对象time.structtime, 可以用 localtime.year 得到当前时间的年 local_time = time.localtime(time.time()) print(localtime) // → time.structtime(tmyear=2018, tmmon=7, tmmday=23, tmhour=14, tmmin=46, tmsec=51, tmwday=0, tmyday=204, tm_isdst=0) print(type(localtime) // → time.strftime(formate, value) 格式化时间 format = '%Y - %m - %d - %H - %M - %S' // month和day要小写 dt = time.strftime(format, value) // → 2018 - 07 - 23 - 15 - 16 - 19 time.strptime() # 反格式化时间 nowtime = time.strptime(strtime, format) // → 返回time.structtime类型的对象 print(nowtime) // → time.structtime(tmyear=2018, tmmon=7, tmmday=23, tmhour=15, tmmin=16, tmsec=19, tmwday=0, tmyday=204, tmisdst=-1) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-26 13:38:37 "},"python-basics/built_in_module/datetime.html":{"url":"python-basics/built_in_module/datetime.html","title":"datetime","keywords":"","body":"datetime timetimedatetime time datetime比time高级了不少，可以理解为datetime基于time进行了封装，提供了更多实用的函数。在datetime 模块中包含了几个类，具体关系如下: 对象 timedelta # 主要用于计算时间跨度 tzinfo # 时区相关 time # 只关注时间 date # 只关注日期 datetime # 同时有时间和日期 datetime 属性 datetime.year datetime.month datetime.day datetime.hour datetime.minute datetime.second datetime.microsecond datetime.tzinfo datetime.date() # 返回date对象 datetime.time() # 返回time对象 datetime.replace(name=value) # 替换各个属性，因为是只读，只有这样才能修改 datetime.timetuple() # 返回time.struct_time对象 datetime.strftime(format) # 按照format进行格式化 datetime.strptime(date_string, format) # 按照format解析时间 datetime.datetime.fromtimestamp(str_timestamp) # 转换时间戳为datetime 实例的方法 datetime.today() # 当前时间，localtime datetime.now([tz]) # 当前默认时间 localtime,datetime.datetime实例 datetime.utcnow() # 当前UTC时间 datedelta In [1]: import datetime In [2]: time_now = datetime.datetime.now() In [3]: time_now Out[3]: datetime.datetime(2014, 10, 27, 21, 46, 16, 657523) In [4]: delta1 = datetime.timedelta(hours=25) In [5]: print(time_now + delta1) 2014-10-28 22:46:16.657523 In [6]: print(time_now - delta1) 2014-10-26 20:46:16.657523 其他 上周五的算法 ''' weeks = { 'MONDAY': 0, 'TUESDAY': 1, 'WEDNESDAY': 2, 'THURSDAY': 3, 'FRIDAY': 4, 'SATURDAY': 5, 'SUNDAY': 6, } datetime.date.today() - datetime.timedelta(days=time.localtime().tm_wday - 4 + 7) ''' strptime的其他: %Y 年 2016 %m 月 12 %d 日(在月中) 25 %a 周（英文简写） Mon %A 周（英文全写） Monday %b 月（英文简写） Dec %B 月（英文全写） December %H 时(24) 18 %p AM或者PM PM %I 时(12) 6 %M 分 28 %S 秒 32 %f 微秒 4321 %c: 日期时间的字符串表示。（如： 04/07/10 10:43:39） %j: 日在年中的天数 [001,366]（是当年的第几天） %U: 周在当年的周数当年的第几周），星期天作为周的第一天 %w: 今天在这周的天数，范围为[0, 6]，6表示星期天 %W: 周在当年的周数（是当年的第几周），星期一作为周的第一天 %x: 日期字符串（如：04/07/10） %X: 时间字符串（如：10:43:39） %y: 2个数字表示的年份 %z: 与utc时间的间隔 （如果是本地时间，返回空字符串） %Z: 时区名称（如果是本地时间，返回空字符串） 链接：https://www.jianshu.com/p/937d6415c96f time 生成时间 import time time.time() int(time.time()) # 10位整数 datetime转time import datetime now = datetime.datetime.now() now.timestamp() Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-04-01 11:25:10 "},"python-basics/built_in_module/subprocess.html":{"url":"python-basics/built_in_module/subprocess.html","title":"subprocess","keywords":"","body":"Subprocess并发与并行用subprocess模块管理子进程Popen详解这里要注意一下Queue模块的学习。Subprocess subprocess的目的就是启动一个新的进程并且与之通信。 subprocess模块中只定义了一个类: Popen。可以使用Popen来创建进程，并与进程进行复杂的交互。它的构造函数如下： The subprocess option:用来执行其他的可执行程序的，即执行外部命令。 他是os.fork() 和 os.execve() 的封装。 他启动的进程不会把父进程的模块加载一遍。使用subprocess的通信机制比较少，通过管道或者信号机制. The multiprocessing option:用来执行python的函数，他启动的进程会重新加载父进程的代码。可以通过Queue、Array、Value等对象来通信。 并发与并行 并发: 交错使用cpu实行任务，本质还是顺序执行 并行: 多核CPU同时执行，效率翻倍 用subprocess模块管理子进程 Python启动的多个子进程是可以并行运行的。子进程将会独立于父进程而运行，这里的父进程指的Python解释器。 import subprocess proc = subprocess.Popen(['echo', 'Hello subprocess'], stdout=subprocess.PIPE) out, err = proc.communicate() print(out.decode('utf-8')) Python 子进程从父进程中解耦，父进程可以运行跟多条平行的子进程 import subprocess def run_some(): proc = subprocess.Popen(['cmd'], stdout=subprocess.PIPE, stdout=subprocess.PIPE) return proc procs = [] for _ in range(10): proc = run_some(x) procs.append(proc) for proc in procs: proc.communicate() # 通过comunicate，等待这些子进程完成I/O工作并终结 # 如果不希望交互操作,使用shell执行 subpros = [] for _ in range(10): subpro = subprocess.Popen(['cmd'], shell=True) subpors.append(subpro) for subpro in subpors: subpro.wait() Python向子进程输送数据 import os def run_openssl(data): env = os.environ.copy() env['password'] = b'\\xe24U' proc = subprocess.Popen( ['openssl', 'enc', '-des3', '-pass', 'env:password'], env=env, stdin=subprocess.PIPE, stdout=subprocess.PIPE ) proc.stdin.write(data) proc.stdin.flush() # 确保子进程获得输入 return proc procs = [] for _ in range(3): data = os.urandom(10) proc = run_openssl(data) procs.append(proc) other_proc = run_other(proc.stdout) # 将前一个进程的输出为输入 other_procs.append(other_proc) for proc in procs: out, error = proc.communicate() # 通过comunicate，等待这些子进程完成I/O工作并终结 print(out) Python子进程超时设置 for proc in procs: try: proc.communicate(timeout=0.1) # 如果子进程在0.1s内没有结束，将抛出异常，可以强行终止 except subprocess.TimeoutExpired: proc.terminate() proc.wait() Popen详解 初始化 subprocess.Popen(args, bufsize=0, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=False, shell=False, cwd=None, env=None, universal_newlines=False, startupinfo=None, creationflags=0) # 参数args可以是字符串或者序列类型（如：list，元组），用于指定进程的可执行文件及其参数。如果是序列类型，第一个元素通常是可执行文件的路径。我们也可以显式的使用executeable参数来指定可执行文件的路径。 参数 参数stdin, stdout, stderr分别表示程序的标准输入、输出、错误句柄。他们可以是PIPE，文件描述符或文件对象，也可以设置为None，表示从父进程继承。 如果参数shell设为true，程序将通过shell来执行。 参数env是字典类型，用于指定子进程的环境变量。如果env = None，子进程的环境变量将从父进程中继承。 subprocess.PIPE 在创建Popen对象时，subprocess.PIPE可以初始化stdin, stdout或stderr参数。表示与子进程通信的标准流。 subprocess.STDOUT 创建Popen对象时，用于初始化stderr参数，表示将错误通过标准输出流输出。 Popen的方法 Popen.poll() 用于检查子进程是否已经结束。设置并返回returncode属性。 Popen.wait() 等待子进程结束。设置并返回returncode属性。 Popen.communicate(input=None) 与子进程进行交互。向stdin发送数据，或从stdout和stderr中读取数据。可选参数input指定发送到子进程的参数。Communicate()返回一个元组：(stdoutdata, stderrdata)。注意：如果希望通过进程的stdin向其发送数据，在创建Popen对象的时候，参数stdin必须被设置为PIPE。同样，如果希望从stdout和stderr获取数据，必须将stdout和stderr设置为PIPE。 Popen.send_signal(signal) 向子进程发送信号。 Popen.terminate() 停止(stop)子进程。在windows平台下，该方法将调用Windows API TerminateProcess（）来结束子进程。 Popen.kill() 杀死子进程。 Popen.stdin，Popen.stdout ，Popen.stderr ， 官方文档上这么说： stdin, stdout and stderr specify the executed programs’ standard input, standard output and standard error file handles, respectively. Valid values are PIPE, an existing file descriptor (a positive integer), an existing file object, and None. Popen.pid 获取子进程的进程ID。 Popen.returncode 获取进程的返回值。如果进程还没有结束，返回None。 简单的用法 执行 p=subprocess.Popen(\"dir\", shell=True) p.wait() # shell参数根据你要执行的命令的情况来决定，上面是dir命令，就一定要shell=True了，p.wait()可以得到命令的返回值。 # 如果上面写成a=p.wait()，a就是returncode。那么输出a的话，有可能就是0【表示执行成功】。 进程通讯 如果想得到进程的输出，管道是个很方便的方法，这样：p=subprocess.Popen(\"dir\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) (stdoutput,erroutput) = p.communicate() p.communicate会一直等到进程退出，并将标准输出和标准错误输出返回，这样就可以得到子进程的输出了。 例子2: p=subprocess.Popen('ls',shell=True,stdout=subprocess.PIPE) stdoutput,erroutput = p.communicate('/home/zoer') print stdoutput[0] print erroutput # 上面的例子通过communicate给stdin发送数据，然后使用一个tuple接收命令的执行结果。 # 上面，标准输出和标准错误输出是分开的，也可以合并起来，只需要将stderr参数设置为subprocess.STDOUT就可以了，这样子： 如果你想一行行处理子进程的输出，也没有问题： p=subprocess.Popen(\"dir\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) while True: buff = p.stdout.readline() if buff == '' and p.poll() != None: break 死锁 但是如果你使用了管道，而又不去处理管道的输出，那么小心点，如果子进程输出数据过多，死锁就会发生了，比如下面的用法：p=subprocess.Popen(\"longprint\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) p.wait() longprint是一个假想的有大量输出的进程，那么在我的xp, Python2.5的环境下，当输出达到4096时，死锁就发生了。当然，如果我们用p.stdout.readline或者p.communicate去清理输出，那么无论输出多少，死锁都是不会发生的。或者我们不使用管道，比如不做重定向，或者重定向到文件，也都是可以避免死锁的。 管道连接 subprocess还可以连接起来多个命令来执行。 在shell中我们知道，想要连接多个命令可以使用管道。 在subprocess中，可以使用上一个命令执行的输出结果作为下一次执行的输入。例子如下： p1=subprocess.Popen('cat ff',shell=True,stdout=subprocess.PIPE) p2=subprocess.Popen('tail -2',shell=True,stdin=p1.stdout,stdout=subprocess.PIPE) print p2.stdout print p2.stdout.read() 例子中，p2使用了第一次执行命令的结果p1的stdout作为输入数据，然后执行tail命令。 下面是一个更大的例子。用来ping一系列的ip地址，并输出是否这些地址的主机是alive的。代码参考了python unix linux 系统管理指南。 #!/usr/bin/env python from threading import Thread import subprocess from Queue import Queue num_threads=3 ips=['127.0.0.1','116.56.148.187'] q=Queue() def pingme(i,queue): while True: ip=queue.get() print 'Thread %s pinging %s' %(i,ip) ret=subprocess.call('ping -c 1 %s' % ip,shell=True,stdout=open('/dev/null','w'),stderr=subprocess.STDOUT) if ret==0: print '%s is alive!' %ip elif ret==1: print '%s is down...'%ip queue.task_done() #start num_threads threads for i in range(num_threads): t=Thread(target=pingme,args=(i,q)) t.setDaemon(True) t.start() for ip in ips: q.put(ip) print 'main thread waiting...' q.join();print 'Done' 在上面代码中使用subprocess的主要好处是，使用多个线程来执行ping命令会节省大量时间。 假设说我们用一个线程来处理，那么每个 ping都要等待前一个结束之后再ping其他地址。那么如果有100个地址，一共需要的时间=100*平均时间。 如果使用多个线程，那么最长执行时间的线程就是整个程序运行的总时间。【时间比单个线程节省多了】 这里要注意一下Queue模块的学习。 pingme函数的执行是这样的：启动的线程会去执行pingme函数。pingme函数会检测队列中是否有元素。如果有的话，则取出并执行ping命令。这个队列是多个线程共享的。所以这里我们不使用列表。【假设在这里我们使用列表，那么需要我们自己来进行同步控制。Queue本身已经通过信号量做了同步控制，节省了我们自己做同步控制的工作=。=】 代码中q的join函数是阻塞当前线程。下面是e文注释 > Queue.join() 　　Blocks until all items in the queue have been gotten and processed(task_done()). 学习Processing模块的时候，遇到了进程的join函数。进程的join函数意思说，等待进程运行结束。与这里的Queue的join有异曲同工之妙啊。processing模块学习的文章在这里。 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-28 14:58:57 "},"python-basics/built_in_module/multiprocessing.html":{"url":"python-basics/built_in_module/multiprocessing.html","title":"multiprocessing","keywords":"","body":"multiprocessing用法multiprocessing 用来执行python的函数，他启动的进程会重新加载父进程的代码。可以通过Queue、Array、Value等对象来通信。 用法 普通用法 from multiprocessing import Process def p_job(x): ... pros = [] for i in range(5): p = Process(target=p_job, args=(i,)) p.start() pros.append(p) # 先启动子进程，再join，父进程等待 for pro in pros: pro.join() 进程池 Pool from multiprocessing import Pool import os, time, random def long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start))) if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-26 11:47:06 "},"python-basics/built_in_module/html.html":{"url":"python-basics/built_in_module/html.html","title":"html","keywords":"","body":"HTML 实体1. 使用HTML 实体 有些字符，像( 主要是爬台湾贸易数据网站和日本的网站，遇到html返回的以&#开头的字符，python的print输出自动转成字符串，但是没法处理，随发现html有转义的方法 1. 使用 ``` 600&#19975;&#20870; (11) from html import unescape unescape(\"200&#x4E07;&#x5186; (3156)\") # 即可显示正常字符 ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-25 13:48:54 "},"python-basics/built_in_module/urllib.html":{"url":"python-basics/built_in_module/urllib.html","title":"urllib","keywords":"","body":"urlliburllib 构造请求 import urllib # &后面的组合生成 urllib.parse.urlencode(query_data, doseq=True) # query_data是一个dict # 如果一个key中是list，doseq可以确保正确的编码 # 如果需要构造cookie或其他内容，需要对单独的str编码 urllib.parse.quote(str) # 默认将空格转码成%20， 如果需要转成`+`,使用下面的 urllib.parse.quote_plus(str) [+和%20的区别](https://stackoverflow.com/questions/2678551/when-to-encode-space-to-plus-or-20/2678602) 解析请求 urllib.parse.parse_qs('age=23&name=%E5%BC%A0%E4%B8%89') {'age': ['23'], 'name': ['张三']} urllib.parse.parse_qsl('age=23&name=%E5%BC%A0%E4%B8%89') [('age', '23'), ('name', '张三')] # 解析字符串 urllib.parse.unquote(query) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-27 15:29:48 "},"python-basics/built_in_module/string.html":{"url":"python-basics/built_in_module/string.html","title":"string","keywords":"","body":"string 模块string 模块 提供字符串和数字组合 用法 >>> import string >>> string >>> string.digits '0123456789' >>> string.hexdigits '0123456789abcdefABCDEF' >>> string.octdigits '01234567' >>> string.punctuation '!\"#$%&\\'()*+,-./:;?@[\\\\]^_`{|}~' >>> string.ascii_lowercase 'abcdefghijklmnopqrstuvwxyz' >>> string.ascii_uppercase 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' >>> string.ascii_letters 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' 关系 printable = digits + ascii_letters + punctuation + whitespace hexdigits = digits + 'abcdef' + 'ABCDEF' whitespace = ' \\t\\n\\r\\v\\f' 随机字符串 ''.join([random.choice(string.digits+string.ascii_lowercase) for _ in range(20)]) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-30 15:44:13 "},"python-basics/modules/modules.html":{"url":"python-basics/modules/modules.html","title":"其他模块","keywords":"","body":"模块模块 Paramiko apscheduler PyExecJS Matplotlib Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-19 09:43:13 "},"python-basics/modules/paramiko.html":{"url":"python-basics/modules/paramiko.html","title":"paramiko","keywords":"","body":"paramiko 模块简介安装paramiko 模块 简介 安装 sudo pip install paramiko Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-26 14:21:59 "},"python-basics/modules/apscheduler.html":{"url":"python-basics/modules/apscheduler.html","title":"apscheduler","keywords":"","body":"apschedulerapscheduler apscheduler是python的定时任务框架， 可以将定时任务持久化 三种出发模式 cron 定时任务触发器 interval 循环任务触发器 date 一次性任务触发器 使用 from apscheduler.schedulers.blocking import BlockingScheduler import time def print_time(x): time1 = time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(time.time())) print(time1,\"模式:{}\".format(x)) def clear_job(job_id): scheduler.remove_job(job_id) print(\"{}任务被删除了\".format(job_id)) def cron_job(): sched = BlockingScheduler() sched.add_job(func=print_time, args=('定时任务', ), trigger='cron', hour='*/2', minute=1, id='cron_job') sched.add_job(func=print_time, args=('循环任务', ), trigger='interval', seconds=3, minute=1, id='interval_job') sched.add_job(func=clear_job, args=('cron_job', ), trigger='date', next_run_time='2019-06-28 18:09:00', id='date_job') sched.start() 一些说明 定时任务设置hours * 表示每 * /2 # 偶数 x,y,z # 组合 一次任务 next_run_time = '2019-06-28 18:09:00' next_run_time = date(2019, 06, 28) next_run_time = datetime(2019, 06, 28, 1, 0, 2) # 未指定时间将立即执行 周期执行任务 seconds = 2 # 表示每2s执行一次 start_date='2018-12-11 09:30:00', end_date='2018-12-15 11:00:00' # 也可以添加开始停止时间 cron api year (int|str) – 4-digit year month (int|str) – month (1-12) day (int|str) – day of the (1-31) week (int|str) – ISO week (1-53) day_of_week (int|str) – number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun), 每周第一天是星期一 hour (int|str) – hour (0-23) minute (int|str) – minute (0-59) second (int|str) – second (0-59) start_date (datetime|str) – earliest possible date/time to trigger on (inclusive) end_date (datetime|str) – latest possible date/time to trigger on (inclusive) timezone (datetime.tzinfo|str) – time zone to use for the date/time calculations (defaults to scheduler timezone) jitter (int|None) – advance or delay the job execution by jitter seconds at most. Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-03 16:37:48 "},"python-basics/modules/pyexecjs.html":{"url":"python-basics/modules/pyexecjs.html","title":"PyExecJS","keywords":"","body":"PyExecJS python执行js代码PyExecJS python执行js代码 install pip install PyExecJS demo import execjs js_str = \"\"\"var re = function(){ var fo = new Fingerprint() return fo.getCanvasFingerprint() } \"\"\" ctx = execjs.get().compile(js_str) t = ctx.call('re') # or t =execjs.get().eval(js_str) print(t) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-03-24 18:00:21 "},"python-basics/modules/tesserocr.html":{"url":"python-basics/modules/tesserocr.html","title":"OCR","keywords":"","body":"tesserocr OCR图片识别文字tesserocr OCR图片识别文字 参考https://cuiqingcai.com/5189.html install tesseract linux yum install -y tesseract macos brew install imagemagick brew install tesseract brew install tesseract-lang 查看支持语言 tesseract --list-langs 测试 tesseract image.png result -l eng && cat result.txt install tesserocr pip3.6 install pillow 图片工具 CFLAGS='-stdlib=libc++ -mmacosx-version-min=10.7' pip3.6 install tesserocr 参考https://diverse.space/2018/10/%E5%9C%A8-Mojave-%E4%B8%8B%E7%BC%96%E8%AF%91-SpiderMonkey demo import tesserocr from PIL import Image image = Image.open('ocr_demo_eng.jpg') print(tesserocr.image_to_text(image)) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-01 17:57:40 "},"python-basics/modules/matplotlib.html":{"url":"python-basics/modules/matplotlib.html","title":"Matplotlib","keywords":"","body":"Matplotlib1. 线图 plot2. 离散项目使用条形图Matplotlib [TOC] code demo见matplot.py 1. 线图 plot plot_demo() 2. 离散项目使用条形图 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-20 10:37:43 "},"python-basics/encode.html":{"url":"python-basics/encode.html","title":"编码","keywords":"","body":"编码问题1. 辅助函数2. 默认编码3. html解析编码依据4. 编码判断编码问题 Python3有两种表示字符序列的类:bytes 和 str。 bytes的实例包含8个二进制位，str的实例包含Unicode字符。 转换时必须使用encode和decode操作 编写程序时，通常将编码和解码放在最外围函数，程序的核心代码应该使用Unicode字符类型，而且不要对字符编码做任何假设。 这样程序可以接受多种类型的文本编码，又保证输出的文本信息只采用一种编码形式 (来自Effective Python) 1. 辅助函数 to_str def to_str(bytes_or_str): if isInstance(bytes_or_str, bytes): value = bytes_or_str.decode('utf-8') else: value = bytes_or_str return value to_bytes def to_bytes(bytes_or_str): if isInstance(bytes_or_str, str): value = bytes_or_str.encode('utf-8') else: value = bytes_or_str return value 2. 默认编码 python3 默认utf-8 仅适用于转字符串, 文件open()根据系统确定默认的编码格式 python3的open函数添加了encoding，要求操作接受str实例，如果读写bytes，使用rb或wb s1 = b'test' s2 = s1.decode('utf-8') type(s1) >> 3. html解析编码依据 4. 编码判断 字符串 import chardet body = b'...xxtestxx...' encode_info = chardet.detect(body) # body是bytes类型 encode_str = encode_info.get('encoding') print(encode_info, encode_str) >> {'encoding': 'ascii', 'confidence': 1.0, 'language': ''} item['body'] = body.decode(encode_str) # 正确解码为str 判断文件的编码 import chardet # 获取文件编码类型 def get_encoding(file): # 二进制方式读取，获取字节数据，检测类型 with open(file, 'rb') as f: # 对于大文件,最好使用readline return chardet.detect(f.readline())['encoding'] Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-28 13:45:29 "},"python-basics/iteration_generator_iterable.html":{"url":"python-basics/iteration_generator_iterable.html","title":"迭代器 生成器 可迭代对象","keywords":"","body":"迭代器 生成器 可迭代对象迭代器 生成器 可迭代对象 Iteration Generator Iterable Iterator 可迭代对象有 __iter__ 方法 迭代器是在可迭代对象基础上, 加__next__ 方法 Python中 string, list, dict, tuple, deque都是可迭代操作的, 但不是迭代器 Generator 列表生成器 set，list， dict都适用 a = [1, 2, 3, 4, 5, 6] squares = [x ** 2 for x in a] even_squares = [x ** 2 for x in a if x % 2 == 0] # 矩阵 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] flat = [x for x in row for row in matrix] >> flat = [1, 2, 3, 4, 5, 6, 7, 8, 9] squared = [[x ** 2 for x in row] for row in matrix] >> squared = [[1, 4, 9], [16, 25, 36], [49, 64, 81]] # 再复杂的结构，建议使用for循环 # 列表推到每个值都要创建一个全新列表，占用大量内存 两种生成方法 生成器就是把[] 换成 ()g = (x * x for x in range(10)) 实现yield函数 # 实现了yield的函数 def mygen(n): now = 0 while now 激活生成器 next(g) generator.send(None) 生成器的四种状态 GEN_CREATED # 等待开始执行 GEN_RUNNING # 解释器正在执行（只有在多线程应用中才能看到这个状态） GEN_SUSPENDED # 在yield表达式处暂停 GEN_CLOSED # 执行结束 from inspect import getgeneratorstate def mygen(n): now = 0 while now 生成器的异常状态 抛出: StopIteration Iterable collections.Iterable 可迭代对象 生成器(generator) string, list, dict, tuple, deque... Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-30 10:57:32 "},"python-basics/shu-ju-jie-gou.html":{"url":"python-basics/shu-ju-jie-gou.html","title":"数据结构","keywords":"","body":"数据结构列表 List集合 Set字典 Dict数据结构 列表 List 集合 Set 字典 Dict Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/shu-ju-jie-gou/lie-biao-list.html":{"url":"python-basics/shu-ju-jie-gou/lie-biao-list.html","title":"列表 List","keywords":"","body":"列表 List列表 List Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/shu-ju-jie-gou/ji-heset.html":{"url":"python-basics/shu-ju-jie-gou/ji-heset.html","title":"集合 Set","keywords":"","body":"集合 Set初始化方法不可变集合转换集合 Set 特点: 无序不重复 初始化 # 初始化一个集合 a = set() # {}是空字典 方法 添加 # 添加一个元素 s.add(m) # 更新多个元素 s.update([2, 3]) 删除 s.remove(4) # 如果不存在则抛出异常 s.discard(4) # 不抛出异常 不可变集合 fs = frozenset('hello') > frozenset({'h', 'e', 'l', 'o'}) # 执行更改操作将抛出异常 转换 list to setl = [1, 2, 3, 1] s = set(l) > {1, 2, 3} set to listlist({1, 2, 3}) > [1, 2, 3] Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/shu-ju-jie-gou/zi-dian-dict.html":{"url":"python-basics/shu-ju-jie-gou/zi-dian-dict.html","title":"字典 Dict","keywords":"","body":"字典 Dict字典 Dict Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/process_thread_coroutine.html":{"url":"python-basics/process_thread_coroutine.html","title":"进程 线程 和协程","keywords":"","body":"进程 线程 和 协程1. Process 进程2. Thread 线程3. ThreadLocal4. 进程 VS 线程5. 分布式进程6. Coroutine 协程进程 线程 和 协程 Process Thread Coroutine 线程是最小的执行单元, 进程由至少一个线程组成.进程和线程的调度完全由操作系统调度 1. Process 进程 os模块封装了常见的系统调用, fork()调用一次,返回两次.因为父进程可以fork多个子进程 父进程返回子进程的pid 子进程返回0,子进程.getppid()返回父进程的pid multiprocessing 模块提供了Process类来代表一个进程对象 启动一个进程 from multiprocessing import Process import os # 子进程要执行的代码 def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid())) if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') 进程池 Pool from multiprocessing import Pool import os, time, random def long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start))) if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') SubProcess 很多时候,子进程并不是自身,而是一个外部进程,我们创建子进程还需要控制输入和输出. 下面演示了在进程中运行命令行 import subprocess print('$ nslookup www.python.org') r = subprocess.call(['nslookup', 'www.python.org']) print('Exit code:', r) 子进程的输入 相当于先输入命令nslookup,然后输入内容 import subprocess print('$ nslookup') p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n') print(output.decode('utf-8')) print('Exit code:', p.returncode) 进程间通信 from multiprocessing import Process, Queue import os, time, random # 写数据进程执行的代码: def write(q): print('Process to write: %s' % os.getpid()) for value in ['A', 'B', 'C']: print('Put %s to queue...' % value) q.put(value) time.sleep(random.random()) # 读数据进程执行的代码: def read(q): print('Process to read: %s' % os.getpid()) while True: value = q.get(True) print('Get %s from queue.' % value) if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 2. Thread 线程 常用模块threading, 封装了(_thread) 启动一个线程 import time, threading # 新线程执行的代码: def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n >> %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name) print('thread %s is running...' % threading.current_thread().name) t = threading.Thread(target=loop, name='LoopThread') t.start() t.join() print('thread %s ended.' % threading.current_thread().name) Lock 多线程和多进程最大的不同在于: 多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响， 多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。 ``` import time, threading # 假定这是你的银行存款: balance = 0 def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - n def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() t1 = threading.Thread(target=run_thread, args=(5,)) t2 = threading.Thread(target=run_thread, args=(8,)) t1.start() t2.start() t1.join() t2.join() print(balance) ``` 由于Python的GIL锁(Global Interpreter Lock),多线程只能占用1核,交替跑,要想实现多核多任务的程序,需要使用多进程实现. 3. ThreadLocal 一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 ``` import threading # 创建全局ThreadLocal对象: local_school = threading.local() def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name)) def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student() t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A') t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B') t1.start() t2.start() t1.join() t2.join() ``` 4. 进程 VS 线程 实现多任务,通常我们会设计Master-Worker模式,Master负责分配任务,Worker负责执行任务. 多进程稳定但占用资源多, 多线程占用资源少但易崩溃. 而且线程切换影响效率 计算密集型任务适合C语言开发,切换任务会占用CPU使用时间, IO密集型任务则是开发效率高最好. 异步IO 5. 分布式进程 6. Coroutine 协程 生成器为我们引入了暂停函数执行（yield）的功能。当有了暂停的功能之后，人们就想能不能在生成器暂停的时候向其发送一点东西（其实上面也有提及：send(None)）。这种向暂停的生成器发送信息的功能通过 PEP 342 进入 Python 2.5 中，并催生了 Python 中协程的诞生。根据 wikipedia 中的定义 ```协程是为非抢占式多任务产生子程序的计算机程序组件，协程允许不同入口点在不同位置暂停或开始执行程序。``` - 注意从本质上而言，协程并不属于语言中的概念，而是编程模型上的概念。 - 子程序调用是通过栈实现的，一个线程就是执行一个子程序。 优势 子程序切换不是线程切换,由程序自身控制,没有线程切换的开销 不需要多线程的锁机制,效率高 使用多核CPU可以 多进程+ 协程 用法 def jumping_range(N): index = 0 while index 重点是jump = yield index这个语句。分成两部分： yield index 是将index return给外部调用程序。 jump = yield 可以接收外部程序通过send()发送的信息，并赋值给jump Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-26 11:00:42 "},"python-basics/decorators.html":{"url":"python-basics/decorators.html","title":"装饰器(Decorators)","keywords":"","body":"Decorators 装饰器1. 闭包2. 装饰器Decorators 装饰器 主要还是利用了Python能够接收和返回函数的功能 1. 闭包 实例 def zoo(acount): def _inner(): print(acount) return _inner 2. 装饰器 实例 ## 定义一个装饰器, 将原来函数的参数输出一下, 就用到了所有情况 def demo(func): def _warpper(*args, **kwargs): print('我是装饰器demo的功能') print('输出的args和kwargs来自被装饰函数, args:{},kwargs: {} '.format(args, kwargs)) func(*args, **kwargs) return _warpper # 使用 @demo def func(*args, **kwargs): print('来自原始函数', args, kwargs) # 测试 func(2, 3, 5, name='mm', **{\"age\": 8}) >> 我是装饰器demo的功能 输出的args和kwargs来自被装饰函数, args:(2, 3, 5),kwargs: {'name': 'mm', 'age': 8} 来自原始函数 (2, 3, 5) {'name': 'mm', 'age': 8} Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-basics/tips.html":{"url":"python-basics/tips.html","title":"others","keywords":"","body":"Tips1. 字符串2. list (append extend)3. 保存图片4. 包的导入5. 生成随机数6. 加密7. del8. Python执行Shell命令9. enumerate 代替range10. zip平行遍历多个迭代器Tips 1. 字符串 format % >>> print('percent: {:.2%}'.format(42/50)) percent: 84.00% >>> print('percent: {:.2f}%'.format(42/50*100)) percent: 84.00% 2. list (append extend) ``` mxt = ['a', 'b', 'c'] mm = ['d', 'e'] mxt.append('d') > mxt = ['a', 'b', 'c', 'd'] mxt.append(mm) > mxt = ['a', 'b', 'c', ['d', 'e']] mxt.extend(mm) > mxt = ['a', 'b', 'c', 'd', 'e'] ``` 3. 保存图片 ``` cat_img = response.content with open(image_path, 'wb') as f: f.write(cat_img) f.close() ``` 4. 包的导入 对于不再当前文件的python模块, 导入方法 被导入包:文件夹下创建 init.py 导入包:model_dir = os.path.join(os.path.split(os.path.abspath(__file__))[0], 'aa/bb/') sys.path.append(model_dir) from my_model import model_or_function 5. 生成随机数 js常用的, 生成一个时间戳 + 随机数 import time import random cert = int(time.time()*1000) + round(random.random(), 3) print('生成随机数: {}'.format(cert)) 6. 加密 import hashlib hl = hashlib.md5() hl.update(url.encode(encoding='utf-8')) new = hl.hexdigest() 7. del 由于python都是引用，而python有GC机制，所以，del语句作用在变量上，而不是数据对象上。 ``` a = 1 b = a c = a del a del b print(c) # c == 1 # 对于list li=[1,2,3,4,5] #列表本身不包含数据1,2,3,4,5，而是包含变量：li[0] li[1] li[2] li[3] li[4] first=li[0] #拷贝列表，也不会有数据对象的复制，而是创建新的变量引用 del li[0] print(li) #输出[2, 3, 4, 5] print(first) #输出 1 ``` 8. Python执行Shell命令 os 返回外部程序的运行结果 import os os.system('ls') popen() 返回一个类文件对象，调用read()或readline()可以输出内容 import os output = os.popen('ps -u root') print(output) commands import commands commands.getstatusoutput('ls') commands.getoutput('ls') subprocess subprocess比os.system更灵活 import subprocess subprocess.call('pwd', shell=True) 9. enumerate 代替range ``` # 为了同时获取索引和列表的值 for i in range(len(my_list)): print(i, my_list[i]) # 使用enumerate, i其实就是计数 for i, item in enumerate(my_list): print(i, item) # 也可以指定i的开始值 # i将从1开始 for i, item in enumerate(my_list， 1): print(i, item) ``` 10. zip平行遍历多个迭代器 ``` # python3 zip相当于生成器，逐次产生元组 for name, count in zip(list_name, list_count): print(name, count) # 如果迭代器长度不一致，要使用itertool.zip_longest ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-30 11:10:03 "},"crawler/summary.html":{"url":"crawler/summary.html","title":"概述","keywords":"","body":"爬虫涉及方面工具篇1. 抓包工具2. HTTP请求库和爬虫框架3. 页面解析4. 模拟请求5. 反反爬技巧6. 反爬策略7. 爬虫优化爬虫涉及方面 熟悉网页抓取原理和技术 设计爬虫策略和防屏蔽规则 提升网页抓取的效率和质量 分布式抓取架构设计和维护 数据清洗和入库 大数据处理（hadoop、Spark、Hive、HBase、Kaflka） 抓取协议设计与开发 工具篇 1. 抓包工具 Web抓包工具 Chrome 开发者工具(首选) Tamper Chrome(可以拦截请求，修改后继续请求) Tamper Data(Firefox上的，和2类似) Fiddler(chrome开发者工具无法实现的功能再用) 移动应用抓包 Charles抓包分析http和https请求(首选) Fiddler抓包(优先使用上一个) Mitmproxy(使用python编写，是命令行程序) Wireshark和Tcpdump(不使用http协议的app) 2. HTTP请求库和爬虫框架 requests(可以代替python原生的请求库)(首选) httpie(命令行库) curl aiohttp(支持异步io的库) hyper(支持http2的库) scrapy和scrapy-redis 3. 页面解析 lxml(推荐lxml和xpath结合使用)(首选) BeautifulSoup(bs4)(推荐配合lxml使用) pyquery(不推荐) html5lib(不常用) json protobuf ocr ocr调用百度接口 4. 模拟请求 Web Splash(支持异步)(首选) chromedrier + Chrome + selenium (使用无界面模式降低资源占用) Phantomjs(已被无界面的Chrome替代) 移动App UiAutomator + mitmproxy Appium + mitmproxy 5. 反反爬技巧 防止被ban的技巧 并发逐步增加，参考scrapy，大致了解对方网站对并发的限制情况 伪装不同的请求，包括使用代理ip和user-agent随机 验证码破解 Cookie 6. 反爬策略 反爬策略 7. 爬虫优化 爬虫优化 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-01 17:56:44 "},"crawler/requests.html":{"url":"crawler/requests.html","title":"Requests","keywords":"","body":"Requests1. 请求参数2. cookies的使用3. session的使用4. responseRequests 1. 请求参数 headers proxies get请求的参数dict params = {} res = requests.get(params=params) cookies 参数,dict cookies = { XX ... } res = requests.get(cookies=cookies) post请求的data参数, dict body = {} res = requests.post(data=body) #如果是json,则使用json.dumps() 有个问题, 如果需要发送的body里面有中文, 则会被编码 2. cookies的使用 cookies = response.cookies # cookies类型: cookies = response.cookies.get_dict() # cookies 返回, cookies['sig'] = tmp_cookies.get('sig') # 可以使用get拿取需要的cookie res = requests.post(url=post_url, data=data, cookies=cookies) # 包含cookies访问 3. session的使用 session = requests.Session() response = session.get('url') session_dict = session.json() # 返回： {'cookies': {\"abc\": '123'}} 4. response r = requests.get('url') 1. r.text # 返回的文本 2. r.json() # 返回的结果转成JSON格式 3. r.status_code 4. r.url 5. r.request # 请求对象 5. r.request.headers['User-Agent'] # 查看请求头 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-10 18:35:44 "},"crawler/xpath.html":{"url":"crawler/xpath.html","title":"XPATH","keywords":"","body":"XPATH1. 用法2. 常用规则XPATH [TOC] XPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。 XPath基于XML的树状结构，提供在数据结构树中找寻节点的能力。 1. 用法 基本用法 from lxml import etree html = response.text # requests和scrapy都可以, html要求是str page_source = etree.HTML(html) tr = page_source.xpath('//div[@class=\"chunk\"]/table/tr[1]') page_source HTML方法有自动修正不全tag的功能, tostring()方法转成源文件, 为bytes类型, 需要decode为str 也可以直接读取文本文件进行解析 from lxml import etree html = etree.parse('./test.html', etree.HTMLParser()) result = etree.tostring(html) print(result.decode('utf-8')) 对于xml的响应 from lxml import etree page_source = etree.XML(response.content) # requests page_source = etree.XML(response.body) # scrapy # 如果使用text, '可能报错: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration'. 2. 常用规则 / # 从当前节点选取所有子节点, 返回list // # 从当前节点选取所有子孙节点, 返回list . # 选取当前节点 .. # 选取当前节点的父节点 @ # 选取属性 [@] # 增加属性限定 如://div[@class=\"xx-name\"], 父节点下所有class是xx-name的div /text() # 文本获取, 返回list 属性多值匹配 # 某个节点的属性有多个值, 比如, //li[contains(@class, \"li\")] # 选择所有包含li的标签 //li[contains(@class, \"li\") and contains(@class, \"tag\")] # 能选择上面那个标签 //li[contains(@class, \"li\") and @name=\"item\"] # 可以同时选择多个属性条件 | 或操作 //li[contains(@class, \"li\")] | //li[@name=\"item\"] # 用法和and类似 按序选择 # xpath的list选择是从1开始, last()表示最后一个 节点轴 html = etree.HTML(text) result = html.xpath('//li[1]/ancestor::*') # 返回li的所有祖先节点, list result = html.xpath('//li[1]/ancestor::div') # 返回li的所有div祖先节点, list result = html.xpath('//li[1]/attribute::*') # 返回li的所有属性值, list result = html.xpath('//li[1]/child::a[@href=\"https://ask.hellobi.com/link1.html\"]') # 返回li的子节点a result = html.xpath('//li[1]/descendant::span') # 返回li的所有span子孙节点, list result = html.xpath('//li[1]/following::*[2]') # 返回li的后续的第2个节点 result = html.xpath('//li[1]/following-sibling::*') # 返回li的后续所有同级节点, list result = html.xpath('//li[1]/preceding-sibling::h6[1]') # 返回li的前面的同级节点, list, [1]指最接近li[1]的, 依次往前 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-11 13:31:37 "},"crawler/selenium/selenium.html":{"url":"crawler/selenium/selenium.html","title":"selenium","keywords":"","body":"分享, 尝试多让浏览器执行JS脚本来简化执行 比如: 之前需要不断下拉来加载数据, 可靠性不是很高, 内存占用也很高, 成功了较低 改善: 使用js发送ajax请求, 然后将返回内容插入到浏览器中, 再用js移除, 这样不仅降低了内存占用, 而且加载速度快了很多, 成功率显著提高 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/selenium/selenium-start.html":{"url":"crawler/selenium/selenium-start.html","title":"安装和基本操作","keywords":"","body":"安装和基本操作1. 安装selenium2. 安装浏览器驱动3. 快速入门4. 调试安装和基本操作 1. 安装selenium sudo pip install selenium sudo python3 -m pip install selenium 2. 安装浏览器驱动 当你使用PhantomJS时, selenium会提醒PhantomJS已经被弃用,并且 PhantomJS 不再更新. 同时为了便于调试, windows上用的chrome, 所以选择chrome windows 安装都很简单, 主要说明linux安装 环境: Amazon Linux chrome安装: 安装说明 chromedriver安装: 安装说明 3. 快速入门 启动浏览器 def launch_driver(): # Chrome配置 option = webdriver.ChromeOptions() option.add_argument('headless') # 无界面, 服务器使用 prefs = {\"profile.managed_default_content_settings.images\": 2} # 不加载图片 option.add_experimental_option(\"prefs\", prefs) driver = webdriver.Chrome(chrome_options=option) driver.implicitly_wait(10) return driver 加载页面 driver = launch_driver() driver.get(url) driver的一些属性 driver.title # 页面标题 driver.page_source # 页面源代码 4. 调试 有的时候页面出现未知标签, 本地又不方便复现, 就需要保留代码调试 # 截图 driver.save_screenshot(\"codingpy.png\") # 保留源代码 print(driver.page_source) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/selenium/ye-mian-jiao-hu.html":{"url":"crawler/selenium/ye-mian-jiao-hu.html","title":"页面交互","keywords":"","body":"页面交互1. 基本操作2. 填写表格3. 一些js脚本4. 判断是否可见, 最好用attribute判断, 而不是is_displayed()页面交互 1. 基本操作 选中元素后, 可以对元素操作如下: 模拟按键 send_keys() element.send_keys('some text', keys.ARROW_DOWN) # 发送内容, 模拟按键 清理input或textarea的内容 element.clear() 点击 element.click() 2. 填写表格 选择下拉列表 from selenium.webdriver.support.ui import Select select = Select(driver.find_element_by_name('name')) select.select_by_index(index) # 按索引选择 select.select_by_visible_text('text') # 按显示内容选择 select.select_by_value(value) # 按value选择 # 取消选择 select.deselect_all() 提交表单 element.click() # webdriver对每个元素都有submit()的方法, 如果对表单某个元素使用, 将找到最近的form并提交, 如果不再表单内, 将抛出异常 element.submit() 3. 一些js脚本 执行js脚本的方法 driver.execute_script(\"script.js\") driver.execute_script(\"arguments[0].value = '你猜一下';\", search_button) #带参数 比如: # 移除某个标签class的hide部分 driver.execute_script(\"$('.button.load-more.hide').removeClass('hide')\") # 移除整个dom标签 driver.execute_script(\"$('#menuContent').remove()\") # 下拉到底部 driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") 修改element的属性, 一般使用js就可以 new_class_name = 'new_class' driver.execute_script(\"$('.ratingcontent').class=argument[0]\", new_class_name) driver.execute_script(\"$('.ratingcontent').class='new_class'\") 4. 判断是否可见, 最好用attribute判断, 而不是is_displayed() # 习惯做法 if element.get_attribute('style') == 'display: none;': print('那么该元素在页面上看不到') # 之前的做法 result = element.is_displayed() Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/selenium/seleniumcha-zhao-yuan-su.html":{"url":"crawler/selenium/seleniumcha-zhao-yuan-su.html","title":"selenium查找元素","keywords":"","body":"查找元素1. 以下会查找出符合条件的第一个元素4. 异常查找元素 1. 以下会查找出符合条件的第一个元素 find_element_by_id login_form = driver.find_element_by_id('loginForm') find_element_by_name``` username = driver.find_element_by_name('username') 3. find_element_by_xpath login_form = driver.find_element_by_xpath(\"//form[@id='loginForm']\") 4. find_element_by_link_text 和 find_element_by_partial_link_text 这是针对a标签的 Continue continue_link = driver.find_element_by_link_text('Continue') continue_link = driver.find_element_by_partial_link_text('Conti') 5. find_element_by_tag_name 针对标签 heading1 = driver.find_element_by_tag_name('h1') 6. find_element_by_class_name content = driver.find_element_by_class_name('content') 7. find_element_by_css_selector content = driver.find_element_by_css_selector('p.content') ## 2. 查找多个元素, 使用elements, 返回list 1. find_elements_by_id 2. find_elements_by_name 3. find_elements_by_xpath 4. find_elements_by_link_text 5. find_elements_by_partial_link_text 6. find_elements_by_tag_name 7. find_elements_by_class_name 8. find_elements_by_css_selector ## 3. 还有一种方法 from selenium.webdriver.common.by import By driver.find_element(By.XPATH, '//button[text()=\"Some text\"]') driver.find_elements(By.XPATH, '//button') ``` 4. 异常 如果找不到任何元素，会抛出NoSuchElementException异常 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/selenium/selenium-explicit-wait.html":{"url":"crawler/selenium/selenium-explicit-wait.html","title":"显式等待(explicit wait)和隐式等待 (implicit wait)","keywords":"","body":"显式等待(explicit wait)和隐式等待 (implicit wait)1. 显式等待(explicit wait)2. 隐式等待 (implicit wait)3. 一直使用显示等待, 忘记隐式等待显式等待(explicit wait)和隐式等待 (implicit wait) 1. 显式等待(explicit wait) #封装了一个方法 def check_element_by_xpath(web_driver, value): try: # 判断是否加载完毕 element = WebDriverWait(web_driver, 10).until( expected_conditions.presence_of_element_located((By.XPATH, value)) ) return element except Exception as e: # print('加载超时', e) # print(traceback.format_exc()) return -1 finally: driver.quit() 这段代码会等待10秒，如果10秒内找到元素则立即返回，否则会抛出TimeoutException异常，WebDriverWait默认每500毫秒调用一下ExpectedCondition直到它返回成功为止。ExpectedCondition类型是布尔的，成功的返回值就是true,其他类型的ExpectedCondition成功的返回值就是 not null 2. 隐式等待 (implicit wait) driver.implicitly_wait(10) 如果某些元素不是立即可用,隐式等待告诉WebDriver去等待一段时间后查找元素 隐式等待是设置该WebDriver的实例的生命周期 3. 一直使用显示等待, 忘记隐式等待 StackOverflow问答 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/splash.html":{"url":"crawler/splash.html","title":"Splash","keywords":"","body":"Splash1. splash的优势2. Splash的安装Splash 1. splash的优势 selenium + Chrome + Chromedriver的方案需要调起浏览器，占用资源较多，无法实现异步和大规模爬取需求 Splash是一个Javascript渲染服务。 Splash是一个实现了HTTP API的轻量级浏览器 Splash是用Python实现的，同时使用Twisted和QT。Twisted（QT）用来让服务具有异步处理能力，以发挥webkit的并发能力。 通过安装配置，实现一个页面渲染服务器，返回渲染后的页面，便于爬取，便于规模应用。 2. Splash的安装 提供docker安装 安装教程sudo docker pull scrapinghub/splash sudo docker run -it -p 8050:8050 scrapinghub/splash # 服务启动在：localhost:8050 http API api文档 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/Scrapy/scrapy_start_up.html":{"url":"crawler/Scrapy/scrapy_start_up.html","title":"Scrapy","keywords":"","body":"开始一个scrapyscrapyspider传入参数1. 定时任务解决-- coding: utf-8 ---- coding: utf-8 --开始一个scrapy scrapy startproject my_scrapy cd my_scrapy scrapy genspider -t basic first_spider xx.com scrapy list scrapy crawl first_spider scrapy scrapy #等价于 /usr/bin/python36 /usr/local/bin/scrapy # 所以可以使用这种方法来指定python版本 spider传入参数 scrapy crawl spider1 -a params 1. 定时任务解决 apscheduler定时任务 ``` -- coding: utf-8 -- from multiprocessing import Process import traceback from apscheduler.schedulers.blocking import BlockingScheduler import sys sys.path.append('..') from utils.scrapy_tools import start_up_spider def crawl_scrapy_job_basic_info(): pros = [] for i in range(6): p = Process(target=start_up_spider, name='worker {}'.format(i+1), args=('as_scrapy_job_categories',)) pros.append(p) p.start() for pro in pros: pro.join() def cron_job(): sched = BlockingScheduler() sched.add_job(func=crawl_as_scrapy_job_company, trigger='cron', hour='07', minute=1, day='11,26', id='crawl_as_scrapy_job_company') sched.start() if __name__ == '__main__': cron_job() ``` 日志配置 ```-- coding: utf-8 -- from scrapy import cmdline import os import datetime import logging def start_up_spider(spider_name): root_logger = logging.getLogger() log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../logs') if not os.path.exists(log_dir): os.makedirs(log_dir) datetime_str = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d_%H:%M:%S') log_path = os.path.join(log_dir, '{}_{}.log'.format(spider_name, datetime_str)) file_handler = logging.handlers.TimedRotatingFileHandler( filename=log_path, when='midnight', backupCount=20, encoding=\"utf8\", ) formater = logging.Formatter(\"%(asctime)s %(processName)-10s %(filename)-15s %(lineno)-3s %(levelname)-8s %(message)s\") file_handler.setFormatter(formater) file_handler.setLevel(logging.DEBUG) root_logger.addHandler(file_handler) s = \"scrapy crawl {} --logfile={}/{}_{}.log \".format(spider_name, log_dir, spider_name, datetime_str) cmdline.execute(s.split()) ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-11-01 11:23:53 "},"crawler/Scrapy/scrapy-setting.html":{"url":"crawler/Scrapy/scrapy-setting.html","title":"Scrapy setting","keywords":"","body":"scrapy配置配置优先级settings.py配置scrapy配置 配置优先级 命令行选项 -s 最高 scrapy crawl myspider -s LOG_FILE=scrapy.log 每个spider的custom_settings class Spider(RedisSpider): name = 'spider_name' redis_key = xxx custom_settings = { 'REDIRECT_ENABLED': True, 'COOKIES_ENABLED': True, } settings.py 每个命令的默认setting 默认的setting settings.py配置 网站爬虫规则，关闭robots.txt遵循 # Obey robots.txt rules ROBOTSTXT_OBEY = False 下载延迟 import random DOWNLOAD_DELAY = random.randint(1, 2) 并发 CONCURRENT_REQUESTS = 64 是否启用cookie # COOKIES_ENABLED = False # 默认生效中，像浏览器一样，一般无需管理 COOKIES_ENABLED = True # 如果使用自定义cookie就把COOKIES_ENABLED设置为True COOKIES_ENABLED = False # 如果使用settings的cookie就把COOKIES_ENABLED设置为False extensions EXTENSIONS = { 'scrapy.extensions.telnet.TelnetConsole': None, # CloseSpiderRedis是设置redis没有任务后自动关闭的扩展, 如果不用bo_lib, 需要手动加入(见[extensions.md](extensions.md)) 'bo_lib.scrapy_tools.CloseSpiderRedis': 100, } CLOSE_SPIDER_AFTER_IDLE_TIMES = 5 ITEM的处理(pipeline) ITEM_PIPELINES = { # 'weapon.pipelines.WeaponPipeline': 300, # 'weapon.mongoPipeline.MongoPipeline': 301, 'weapon.mysql_pipelines.MySQLPipeline': 301, } 文件编码 FEED_EXPORT_ENCODING = 'utf-8' LOG的配置 LOG_ENCODING = 'UTF-8' 数据库配置 MYSQL_HOST = 'localhost' MYSQL_DATABASE = 'military' MYSQL_POST = 3306 MYSQL_USER = 'root' # MYSQL_PASSWORD = '' MYSQL_PASSWORD = 'PnS_cDEZhMb4p8M3' # mongodb配置 MONGO_URI = \"localhost:27017\" MONGO_DATABASE = 'military' 配置数据版本VERSION date = datetime.datetime.now().strftime(\"%Y-%m-%d\") VERSION = date 配置item pipelines # Configure item pipelines ITEM_PIPELINES = { 'mangoplate_scrapy.pipelines.MangoplateScrapyPipeline': 300, } 配置自动限速 下载延迟计算: 通过计算建立TCP连接到接收HTTP header之间的时间来测量的. 自动计算来确定合理的延迟和并发数, 注意: 'CONCURRENT_REQUESTS_PER_DOMAIN' 计算出的并发不高于预设置的 'CONCURRENT_REQUESTS_PER_IP' 计算出的并发不高于预设置的 'DOWNLOAD_DELAY' 计算出的延迟不会低于预设置的 # Enable and configure the AutoThrottle extension (disabled by default) # See https://doc.scrapy.org/en/latest/topics/autothrottle.html AUTOTHROTTLE_ENABLED = True # 启用自动限速扩展 # The initial download delay AUTOTHROTTLE_START_DELAY = 5 # 初始下载延迟 # The maximum download delay to be set in case of high latencies AUTOTHROTTLE_MAX_DELAY = 60 # 高延迟情况下最大的下载延迟 # The average number of requests Scrapy should be sending in parallel to each remote server AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0 # 开启时的并发 # Enable showing throttling stats for every response received: AUTOTHROTTLE_DEBUG = False # debug模式, true将会展示每个接收到的response, 可以查看限速参数是如何调整的. log配置 cur_dir = os.path.dirname(os.path.realpath(__file__)) logFilename = os.path.join(cur_dir, '../../crawlerOutput/{}/log/roundmenu.log'.format(VERSION)) logging.basicConfig( level=logging.ERROR, # 定义输出到文件的log级别， format='%(asctime)s %(filename)s : %(levelname)s %(message)s', # 定义输出log的格式 datefmt='%Y-%m-%d %A %H:%M:%S', # 时间 filename=logFilename, # log文件名 filemode='a') FEED_EXPORT_ENCODING = 'UTF-8' 请求超时# 设置超时，默认180秒 DOWNLOAD_TIMEOUT = 10 处理HTTP错误 # 默认为[], 即所有的httperror请求调用errback, 配置后对应的响应将调用callback HTTPERROR_ALLOWED_CODES = [302, 404] # 默认False HTTPERROR_ALLOW_ALL = TRUE # spider中可以用 handle_httpstatus_list = [404] scrapy_redis # scrapy_redis 默认有 REDIS_PARAMS 这个参数，且会自动将你的 REDIS_PARAMS 用你的指定的值更新。所以我们可以这样写 REDIS_PARAMS = { 'db': 11, 'use_helper': True, } retry重试RETRY_ENABLED: 是否开启retry RETRY_TIMES: 重试次数 RETRY_HTTP_CODECS: 遇到什么http code时需要重试，默认是500,502,503,504,408，其他的，网络连接超时等问题也会自动retry的 # 208需要重试, twitchmetrics 发现的问题 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-03-05 13:18:53 "},"crawler/Scrapy/middlewares.html":{"url":"crawler/Scrapy/middlewares.html","title":"中间件","keywords":"","body":"中间件 MiddleWare1. HttpErrorMiddleware 处理200-300之外的返回码中间件 MiddleWare 1. HttpErrorMiddleware 处理200-300之外的返回码 https://juejin.im/post/5c10a1caf265da61380f1315 源码展示 默认情况 200-300 调用callback，除此之外ignore 指定要处理 request.meta可以指定 request.meta['handle_httpstatus_all'] = Ture # 或者 request.meta['handle_httpstatus_list'] = [301, 302...] spiders里设置 self.handle_httpstatus_list = [301, 302...] 在setting中配置 HTPERROR_ALLOWED_CODES = [403] # 默认为[] # 或 HTTPERROR_ALLOW_ALL = True # 默认为False 处理方式 callback中能够处理 200-300 上述填写的其他handle-http-error 如果使用了HTTPERROR_ALLOW_ALL = True, 所有的请求都在callback中处理，同时handle-http-error配置将被覆盖，因为本身包含了所有请求 errback处理 上述之外的，因为有异常抛出 好习惯 除非明确知道要处理的请求，否则我会将所有都在callback中处理，打log或者加入redis重试，因为可以记录错误码 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-13 10:00:22 "},"crawler/Scrapy/extensions.html":{"url":"crawler/Scrapy/extensions.html","title":"extensions","keywords":"","body":"EXTENSION 扩展!/usr/bin/env pythonencoding: utf-8EXTENSION 扩展 scrapy-redis自动关闭 脚本 ``` !/usr/bin/env python encoding: utf-8 import logging from twisted.internet import task from scrapy.exceptions import NotConfigured from scrapy import signals logger = logging.getLogger(name) class CloseSpiderRedis(object): \"\"\" Start a task to check if scrapy_redis spider is idle in every minutes, when it found the idle times granter then CLOSE_SPIDER_AFTER_IDLE_TIMES which add in settings, the spider would be closed. Usage: In settings.py EXTENSIONS = { 'prototypes.extensions.CloseSpiderRedis': 100, }, CLOSE_SPIDER_AFTER_IDLE_TIMES = 5 \"\"\" def __init__(self, crawler, idle_close_after_times): self.crawler = crawler self.stats = crawler.stats self.idle_close_after_times = idle_close_after_times self.reason = 'close spider after {0} times of spider idle'.format(self.idle_close_after_times) self.idle_count = 0 self.interval = 60.0 self.multiplier = 60.0 / self.interval @classmethod def from_crawler(cls, crawler): idle_close_after_times = crawler.settings.getint('CLOSE_SPIDER_AFTER_IDLE_TIMES') if not idle_close_after_times: raise NotConfigured o = cls(crawler, idle_close_after_times) crawler.signals.connect(o.spider_opened, signal=signals.spider_opened) crawler.signals.connect(o.spider_closed, signal=signals.spider_closed) return o def spider_opened(self, spider): self.pagesprev = 0 self.itemsprev = 0 self.task = task.LoopingCall(self.idle_close, spider) self.task.start(self.interval) def idle_close(self, spider): items = self.stats.get_value('item_scraped_count', 0) pages = self.stats.get_value('response_received_count', 0) irate = (items - self.itemsprev) * self.multiplier prate = (pages - self.pagesprev) * self.multiplier self.pagesprev, self.itemsprev = pages, items if irate == 0 and prate == 0: if self.idle_count == self.idle_close_after_times: self.crawler.engine.close_spider(spider, reason=self.reason) else: self.idle_count += 1 def spider_closed(self, spider, reason): if self.task.running: self.task.stop() ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-21 17:43:58 "},"crawler/Scrapy/scrapy_optimization.html":{"url":"crawler/Scrapy/scrapy_optimization.html","title":"Scrapy优化","keywords":"","body":"Scrapy优化增大并发数降低日志级别增大线程池禁用cookie自动调整负载Scrapy优化 增大并发数 # Configure maximum concurrent requests performed by Scrapy (default: 16) CONCURRENT_REQUESTS = 32 降低日志级别 LOG_LEVEL = 'INFO' 增大线程池 REACTOR_THREADPOOL_MAXSIZE = 20 禁用cookie # Disable cookies (enabled by default) COOKIES_ENABLED = True 自动调整负载 # Enable and configure the AutoThrottle extension (disabled by default) # See https://doc.scrapy.org/en/latest/topics/autothrottle.html AUTOTHROTTLE_ENABLED = True # The initial download delay AUTOTHROTTLE_START_DELAY = 5 # The maximum download delay to be set in case of high latencies AUTOTHROTTLE_MAX_DELAY = 60 # The average number of requests Scrapy should be sending in parallel to # each remote server AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0 # Enable showing throttling stats for every response received: AUTOTHROTTLE_DEBUG = False Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/Scrapy/deploying-scrapy-crawlers.html":{"url":"crawler/Scrapy/deploying-scrapy-crawlers.html","title":"部署爬虫scrapyd","keywords":"","body":"部署爬虫Scrapyd 的使用安装配置开通服务器的6800端口web端访问部署爬虫 部署爬虫的两种方案: 开源方案 Scrapyd Scrapy的付费方案: Scrapy Cloud scrapyd运行在服务器, 作为一个服务, 接收特定接口的包部署, 运行爬虫的每个服务器都需要 Scrapyd 的使用 服务器部署 安装 sudo python3 -m pip install scrapyd # 必须指定scrapyd的python版本, 因为之后默认按这个版本运行爬虫 sudo pip3 install scrapyd 配置 scrapyd自1.2版本后, 不再自动创建配置文件, 需手动添加 sudo mkdir /etc/scrapyd sudo nano /etc/scrapyd/scrapyd.conf 配置文件内容如下: 官方demo ``` [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 4 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 debug = off runner = scrapyd.runner application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd.webservice.DeleteProject delversion.json = scrapyd.webservice.DeleteVersion listjobs.json = scrapyd.webservice.ListJobs daemonstatus.json = scrapyd.webservice.DaemonStatus ``` 开通服务器的6800端口 启动服务器 scrapyd # 或者 systemctl enable scrapyd 并将scrapyd加入到supervisor来确保始终启用 [program:scrapyd] command=scrapyd ; the program (relative uses PATH, can take args) process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;directory=~ ; directory to cwd to before exec (def no cwd) autostart=true ; start at supervisord start (default: true) startsecs=1 ; # of secs prog must stay up to be running (def. 1) startretries=3 ; max # of serial start failures when starting (default 3) autorestart=True ; when to restart if exited after running (def: unexpected) stopsignal=KILL ; signal used to kill process (default TERM) web端访问 访问 http://localhost:6800/ Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/Scrapy/request-and-response.html":{"url":"crawler/Scrapy/request-and-response.html","title":"Request and Response","keywords":"","body":"Request and ResponseRequest1. 基本参数2. 子类FormRequest（发送post请求, 可选）3. dont_filter机制4. errback的处理Response1. 基本属性Request and Response 中文文档 cookie : scrapy 对cookie管理有很强的支持, 可以不需要去管理 Request 1. 基本参数 headers url(转义之后的，修改需用replace) method body (str，经过转义的) 当method为post时使用 import urllib import json post_data = {} # 'content-type': 'application/x-www-form-urlencoded; charset=UTF-8', post_data = urllib.parse.urlencode(post_data, doseq=True) # \"Content-Type\": \"application/json\", post_data = json.dumps(post_data) yield scrapy.Request(post_url, method='post', body=post_data, headers=self.headers, callback=self.parse, dont_filter=True, errback=self.errback_httpbin ) 如果是get方法,get请求的querystring, 需要使用: url + \"?\" + urllib.parse.urlencode(dict) 来自动拼接,不能在此处传入。 cookies # 默认生效,接受dict cookies = { 'PREF': \"TM={}277:L={}\".format(last_time, keyword_urlencode), 'RQ': \"q=&l={}&ts={}114\".format(keyword_urlencode, before_time), } request = scrapy.Request(url=url, headers=self.headers, cookies=cookies, callback=self.parse, errback=self.errback_httpbin, dont_filter=True, meta=data) callback meta 上一个请求 request.meta['item'] = item 下一个响应中response.meta['item'] 便是item errback 返回non-200时调用 比如：def errback_httpbin(self, failure): … ， failure.check(ErrorType), ErrorType有HttpError, DNSLookupError,TimeoutError, TCPTimeOutError… dont_filter(默认false, 过滤重复请求, 如果要处理302,301, 就需要设置成True) 2. 子类FormRequest（发送post请求, 可选） url formdata(此处传入dict即可) iii. call_back iv. … v. FromRequest.from_response(response, formdata, call_back) 1) 可以用来模拟用户登录 2) 在login的方法中： return scrapy.FormRequest.from_response( response, formdata={'name'='xx', 'password'='xx'}, callback=self.after_login ) 3. dont_filter机制 4. errback的处理 scrapy默认情况下, 只有status在200-300之间时才会调用callback, 其他都由errback调用可以定义要进入callback的其他返回码setting.md的处理HTTP错误, 调用方法 # 通常有三种情况会调用到errback import scrapy from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError ... def errback_httpbin(self, failure): # log all failures self.logger.error(repr(failure)) # in case you want to do something special for some errors, # you may need the failure's type: if failure.check(HttpError): # these exceptions come from HttpError spider middleware # you can get the non-200 response response = failure.value.response self.logger.error('HttpError on %s', response.url) elif failure.check(DNSLookupError): # this is the original request request = failure.request self.logger.error('DNSLookupError on %s', request.url) elif failure.check(TimeoutError, TCPTimedOutError): request = failure.request self.logger.error('TimeoutError on %s', request.url) 几种处理 重试 # 通常都是在meta中保存请求的参数，可以放回redis或直接重构请求 meta = failure.request.meta.copy() 对于HttpError的请求 # 可以取回status, url, headers(用来获取重定向的url)等 response = failure.value.response url = response.url status = response.status headers = response.headers # 如果返回302或301 location = headers.get('Location').decode('utf-8') Response 1. 基本属性 url(string) headers(dict) status(integer) body(bytes), text # body.decode('utf-8') = text utf-8可以是其他类型, meta(dict) flags(list) request产生这个response的request对象，重定向后的request是原始的，所以response.url == response.request.url 不总是成立 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-11 13:41:20 "},"crawler/Scrapy/scrapy-redis.html":{"url":"crawler/Scrapy/scrapy-redis.html","title":"Scrapy-redis","keywords":"","body":"Scrapy-Redis设置说明spider部分可以指定redis数据库到其他位置Scrapy-Redis 设置说明 ## 以下是redis配置 # 启用Redis调度存储请求队列 SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" # 确保所有的爬虫通过Redis去重 DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter' # 请求序列化器 SCHEDULER_SERIALIZER = \"scrapy_redis.picklecompat\" # 不清空Redis队列, 可以暂停/恢复爬虫 SCHEDULER_PERSIST = True #使用优先级调度请求队列 （默认使用） SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue' #可选用的其它队列 #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue' #SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue' # 设置最大等待时间来避免爬虫关闭, 只有在队列是SpiderQueue和SpiderStack是才有效 SCHEDULER_IDLE_BEFORE_CLOSE = 10 # 在redis中存储爬取的item, 以便于后续处理 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300, } # item序列化并保存这些item在此redis 键中 REDIS_ITEMS_KEY = '%(spider)s:items' # item序列化默认使用ScrapyJSONEncoder. 你可以使用其他 REDIS_ITEMS_SERIALIZER = 'json.dumps' # 指定redis的主机和端口 REDIS_HOST = 'localhost' REDIS_PORT = 6379 # 指定完整的redis连接参数,比如: 超时等 #REDIS_PARAMS = {} # Use custom redis client class. #REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient' # 如果为true, 将使用redis的spop操作,你必须使用sadd命令来增加urls到redis队列. # 如果你想在start urls的列表避免重复, 并且顺序不重要是可以使用 REDIS_START_URLS_AS_SET = False # 默认redis urls的键用于RedisSpider和RedisCrawlSpider REDIS_START_URLS_KEY = '%(name)s:start_urls' # 为redis使用其他编码, 默认utf-8 # REDIS_ENCODING = 'latin1' spider部分 spider继承RedisSpider类from scrapy_redis.spiders import RedisCrawlSpider # 可以自定义地址过滤, 暂不考虑 from scrapy_redis.spiders import RedisSpider start_urls # start_urls 来自redis, 位置由redis_key决定, 一般为 spider_name:start_urls redis_key = 'douban_spider_redis:start_urls' pipelines # item默认输出到redis一份, 位置: spider_name:items # 可以改写process_item方法来将item输出成多个格式, 也可以爬完之后再从redis获取 可以指定redis数据库到其他位置 参考位置:https://blog.csdn.net/Bone_ACE/article/details/54139500 原理: 重写scrapy_redis的调度器: scheduler.py下的from_settings()方法调用 在settings.py同级目录下新建一个文件schedulerOverwrite.py，填入下面的代码。然后在settings.py设置SCHEDULER=schedulerOverwrite.SchedulerSon，之后在settings.py中设置REDIS_DB=XXX即可指定db。 import redis from scrapy_redis.scheduler import Scheduler from scrapy.utils.misc import load_object # default values SCHEDULER_PERSIST = False QUEUE_KEY = '%(spider)s:requests' QUEUE_CLASS = 'scrapy_redis.queue.SpiderPriorityQueue' DUPEFILTER_KEY = '%(spider)s:dupefilter' IDLE_BEFORE_CLOSE = 0 REDIS_URL = None REDIS_HOST = 'localhost' REDIS_PORT = 6379 REDIS_DB = 0 def from_settings(settings): url = settings.get('REDIS_URL', REDIS_URL) host = settings.get('REDIS_HOST', REDIS_HOST) port = settings.get('REDIS_PORT', REDIS_PORT) db = settings.get('REDIS_DB', REDIS_DB) # REDIS_URL takes precedence over host/port specification. if url: return redis.from_url(url) else: return redis.Redis(host=host, port=port, db=db) class SchedulerSon(Scheduler): @classmethod def from_settings(cls, settings): persist = settings.get('SCHEDULER_PERSIST', SCHEDULER_PERSIST) queue_key = settings.get('SCHEDULER_QUEUE_KEY', QUEUE_KEY) queue_cls = load_object(settings.get('SCHEDULER_QUEUE_CLASS', QUEUE_CLASS)) dupefilter_key = settings.get('DUPEFILTER_KEY', DUPEFILTER_KEY) idle_before_close = settings.get('SCHEDULER_IDLE_BEFORE_CLOSE', IDLE_BEFORE_CLOSE) server = from_settings(settings) return cls(server, persist, queue_key, queue_cls, dupefilter_key, idle_before_close) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/Scrapy/scrapyd.html":{"url":"crawler/Scrapy/scrapyd.html","title":"Scrapyd","keywords":"","body":"Scrapyd安装启动服务API说明Scrapyd 安装 sudo python36 -m pip install scrapyd 启动服务 scrapyd 浏览器访问： http://localhost.6800 可以查看scrapyd的web页面，但是页面主要 API说明 以下是postman导出的一个collection，可以重命名为：scrapyd.postman_collection.json 导入到postman查看 { \"info\": { \"_postman_id\": \"ff84db97-b2bd-40af-9b4e-273f48f6dc4e\", \"name\": \"scrapyd\", \"description\": \"scrapyd api练习\", \"schema\": \"https://schema.getpostman.com/json/collection/v2.1.0/collection.json\" }, \"item\": [ { \"name\": \"daemonstatus\", \"request\": { \"method\": \"GET\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/daemonstatus.json\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"daemonstatus.json\" ] }, \"description\": \"查看状态\" }, \"response\": [] }, { \"name\": \"addversion\", \"request\": { \"method\": \"POST\", \"header\": [], \"body\": { \"mode\": \"file\", \"file\": {} }, \"url\": { \"raw\": \"http://localhost.6800/addversion.json?project=dianping&version=v01\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"addversion.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" }, { \"key\": \"version\", \"value\": \"v01\" } ] }, \"description\": \"为项目增加版本\" }, \"response\": [] }, { \"name\": \"schedule\", \"request\": { \"method\": \"POST\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/schedule.json?project=dianping&spider=region\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"schedule.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" }, { \"key\": \"spider\", \"value\": \"region\" } ] }, \"description\": \"启动爬虫\" }, \"response\": [] }, { \"name\": \"cancel\", \"request\": { \"method\": \"POST\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/cancel.json?project=dianping&job=ae07a0a011b711e9bb7c06c0685ba786\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"cancel.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" }, { \"key\": \"job\", \"value\": \"ae07a0a011b711e9bb7c06c0685ba786\", \"description\": \"job_id\" } ] }, \"description\": \"取消爬虫\" }, \"response\": [] }, { \"name\": \"listprojects\", \"request\": { \"method\": \"GET\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/listprojects.json\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"listprojects.json\" ] }, \"description\": \"列出所有的爬虫项目\" }, \"response\": [] }, { \"name\": \"listverisons\", \"request\": { \"method\": \"GET\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/listversions.json?project=dianping\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"listversions.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\", \"description\": \"指定项目,返回版本列表\" } ] }, \"description\": \"列出指定项目的版本\" }, \"response\": [] }, { \"name\": \"listspiders\", \"request\": { \"method\": \"GET\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/listspiders.json?project=dianping\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"listspiders.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" } ] }, \"description\": \"列出项目的所有爬虫项目\" }, \"response\": [] }, { \"name\": \"listjobs\", \"request\": { \"method\": \"GET\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/listjobs.json?project=dianping\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"listjobs.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" } ] }, \"description\": \"列出项目目前正在运行的任务\" }, \"response\": [] }, { \"name\": \"delversion\", \"request\": { \"method\": \"POST\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/delversion.json?project=dianping&version=1546759284\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"delversion.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" }, { \"key\": \"version\", \"value\": \"1546759284\" } ] }, \"description\": \"列出项目目前正在运行的任务\" }, \"response\": [] }, { \"name\": \"delproject\", \"request\": { \"method\": \"POST\", \"header\": [], \"body\": { \"mode\": \"raw\", \"raw\": \"\" }, \"url\": { \"raw\": \"http://localhost.6800/delproject.json?project=dianping\", \"protocol\": \"http\", \"host\": [ \"scrapyd\", \"maxiaoteng\", \"xyz\" ], \"path\": [ \"delproject.json\" ], \"query\": [ { \"key\": \"project\", \"value\": \"dianping\" } ] }, \"description\": \"删除指定的项目\" }, \"response\": [] } ] } Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/Scrapy/gerapy.html":{"url":"crawler/Scrapy/gerapy.html","title":"Gerapy","keywords":"","body":"安装启动服务说明安装 在aws上安装 系统amazon linux, python版本:python36 sudo python3 -m pip install gerapy 出现错误 gcc -pthread -Wno-unused-result -Wsign-compare -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c src/twisted/test/raiser.c -o build/temp.linux-x86_64-3.6/src/twisted/test/raiser.o src/twisted/test/raiser.c:4:20: fatal error: Python.h: No such file or directory #include \"Python.h\" ^ compilation terminated. **error: command 'gcc' failed with exit status 1** 解决方案: sudo yum install python36-devel libxml2-devel libxslt-devel sudo yum install gcc **注意: 安装python-devel需要指定版本, 可解决问题** 验证成功 gerapy 显示: > Usage: gerapy init [--folder=] gerapy migrate gerapy createsuperuser gerapy runserver [] 启动服务 初始化gerapy init 执行完毕后, 本地生成gerapy文件夹, 进入后, 看到projects文件夹cd gerapy gerapy migrate # 将会生成一个SQLite数据库, 同时建立数据库表 启动服务, gerapy将会运行在6800端口gerapy runserver 说明 gerapy其实是用来管理多个scrapyd的服务器, 所以没必要运行在多个服务器上, 我可以运行在巴西1 或者本地都可以. Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/uiautomator.html":{"url":"crawler/uiautomator.html","title":"UiAutomator","keywords":"","body":"UiAutomator 借助安卓测试工具爬虫UiAutomator 借助安卓测试工具爬虫 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 15:37:30 "},"crawler/mitmproxy.html":{"url":"crawler/mitmproxy.html","title":"mitmproxy","keywords":"","body":"mitmproxy1. 安装2. 使用mitmproxy 1. 安装 mac brew install mitmproxy docker https://hub.docker.com/r/mitmproxy/mitmproxy/ # docker启动 docker run --rm -it -p 8080:8080 mitmproxy/mitmproxy mitmdump # 获取ca证书可以挂载磁盘 docker run --rm -it -v ~/.mitmproxy:/home/mitmproxy/.mitmproxy -p 8080:8080 mitmproxy/mitmproxy mitmdump # 启动mitmweb docker run --rm -it -p 8080:8080 -p 127.0.0.1:8081:8081 mitmproxy/mitmproxy mitmweb 2. 使用 客户端需信任CA证书 证书位于 ./mitmproxy mitmproxy-ca.pem # PEM格式的证书私钥 mitmproxy-ca-cert.p12 # PKCS12格式的证书，适用于Windows平台 mitmproxy-ca-cert.pem # PEM格式证书，适用于大多数非Windows平台 mitmproxy-ca-cert.cer # 与mitmproxy-ca-cert.pem相同，只是改变了后缀，适用于部分Android平台 mitmproxy-dhparam.pem # PEM格式的秘钥文件，用于增强SSL安全性 脚本内容 script.py实践 ```Python import mitmproxy.http from mitmproxy import ctx from mitmproxy.http import HTTPFlow as flow class Ctrip: def __init__(self): self.num = 0 def request(self, flow: mitmproxy.http.HTTPFlow): # 修改request请求，headers或者代理 if \"xx.com\" in flow.request.url: flow.request.headers[\"User-Agent\"] = 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Mobile Safari/537.36' return def response(self, flow: mitmproxy.http.HTTPFlow): # 修改响应 if \"page/brands/\" in flow.request.url: # 将js代码注入到浏览器首页，防止无头浏览器被检测 # 所需注入的js代码 injected_javascript = ''' // overwrite the `languages` property to use a custom getter Object.defineProperty(navigator, \"languages\", { get: function() { return [\"zh-CN\",\"zh\",\"zh-TW\",\"en-US\",\"en\"]; } }); // Overwrite the `plugins` property to use a custom getter. Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5], }); // Pass the Webdriver test Object.defineProperty(navigator, 'webdriver', { get: () => false, }); // Pass the Chrome Test. // We can mock this in as much depth as we need for the test. window.navigator.chrome = { runtime: {}, // etc. }; // Pass the Permissions Test. const originalQuery = window.navigator.permissions.query; window.navigator.permissions.query = (parameters) => ( parameters.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : originalQuery(parameters) ); ''' html = flow.response.text html = html.replace('', '%s' % injected_javascript) flow.response.text = str(html) # 处理响应 if \"xx\" in flow.request.url: # http api host = flow.request.host sort = flow.request.query.get('sort') resp = flow.response.text # post方法的body content = flow.request.get_content() # 直接拦截 def http_connect(self, flow: mitmproxy.http.HTTPFlow): if flow.request.host == \"www.google.com\": flow.response = http.HTTPResponse.make(404) addons = [Ctrip()] ``` 命令行启动脚本 mitmdump -s script.py # 非登陆代理 mitmdump --mode upstream:http://127.0.0.1:7890 -s mitm_for_script.py # 使用阿布云代理, -p为指定代理端口 mitmdump --mode upstream:http-dyn.abuyun.com:9020 --upstream-auth username:password -s mitm_for_script.py -p 8090 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-11-05 11:20:16 "},"crawler/protobuf.html":{"url":"crawler/protobuf.html","title":"Protobuf","keywords":"","body":"Protobuf1. 介绍2. 使用Protobuf 1. 介绍 Protobuf（Protocol Buffers），是 Google 开发的一种跨语言、跨平台的可扩展机制，用于序列化结构化数据。与 XML 和 JSON 格式相比，protobuf 更小、更快、更便捷。protobuf 目前支持 C++、Java、Python、Objective-C，如果使用 proto3，还支持 C#、Ruby、Go、PHP、JavaScript 等语言。 官网地址：https://developers.google.cn/protocol-buffers/ GitHub 地址：https://github.com/protocolbuffers/protobuf 来自merciari美国app中的接口 优点： 性能好, 序列化和反序列化快, 占用空间小 跨语言 缺点： 二进制格式可读性差：为了提高性能，protobuf 采用了二进制格式进行编码，这直接导致了可读性差。 缺乏自描述: XML 是自描述的，而 protobuf 不是，不配合定义的结构体是看不出来什么作用的。 2. 使用 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-09-20 15:59:05 "},"crawler/proxy_server.html":{"url":"crawler/proxy_server.html","title":"使用代理服务","keywords":"","body":"代理服务器（Proxy server）1. 使用Squid配置代理服务器2. 爬虫使用代理搭建4G代理1. 工具:2. docker配置, 单个docker容器实现一个网卡的代理3. 直接从python脚本实现随机选择网卡代理服务器（Proxy server） [TOC] 1. 使用Squid配置代理服务器 系统为CentOS # yum 安装squid sudo yum -y install squid # 设置开机启动 sudo chkconfig --level 35 squid on # 在35级别上自动运行squid服务 # 修改配置文件 sudo vi /etc/squid/squid.conf http_access allow !Safe_ports #deny改成allow http_access allow CONNECT !SSL_ports #deny改成allow http_access allow all #deny改成allow http_port 27233 # 默认3128 # 启动Squid sudo service squid start 2. 爬虫使用代理 requests使用代理 import requests proxies = { \"http\": \"http://53.53.171.25:27233\", \"https\": \"http://52.56.161.222:27233\", } url = 'http://ip.filefab.com/index.php' res = requests.get(url, proxies=proxies) scrapy使用代理 第一种，在spier的Request中添加 # 初始化start_requests def start_requests(self): proxies = { \"http\": \"http://53.53.171.25:27233\", \"https\": \"http://52.56.161.222:27233\", } url = 'http://ip.filefab.com/index.php' request = scrapy.Request(url, callback=self.parse) if url.startswith(\"http://\"): request.meta['proxy']= proxies.get('http') # http代理 elif url.startswith(\"https://\"): request.meta['proxy']= proxies.get('https') yield request 第二种，scrapy代理使用中间件((参考dianping的配置) 在中间件中创建proxy，下面展示了接入阿布云代理的方法# 在middlewares.py中增加一个类，取名：ProxyMiddleware即代理中间件 # 添加阿布云代理 import base64 # 代理服务器 proxyServer = \"http://http-dyn.abuyun.com:9020\" # 代理隧道验证信息 proxyUser = \"HU80498W9ZK49KGD\" proxyPass = \"60DBD202DC5E8F35\" # for Python2 # proxyAuth = \"Basic \" + base64.b64encode(proxyUser + \":\" + proxyPass) # for Python3 proxyAuth = \"Basic \" + base64.urlsafe_b64encode(bytes((proxyUser + \":\" + proxyPass), \"ascii\")).decode(\"utf8\") class ProxyMiddleware(object): def process_request(self, request, spider): request.meta[\"proxy\"] = proxyServer request.headers[\"Proxy-Authorization\"] = proxyAuth settings.py中生效配置的中间件即可 DOWNLOADER_MIDDLEWARES = { 'dianping.middlewares.UAMiddleware': 545, } 搭建4G代理 参考: 1. [https://cuiqingcai.com/7540.html](https://cuiqingcai.com/7540.html) 2. [https://blog.csdn.net/u012731379/article/details/78744757](https://blog.csdn.net/u012731379/article/details/78744757) 1. 工具: 4G上网卡() 宿主机linux docker, centos72. docker配置, 单个docker容器实现一个网卡的代理 启动demo # 启动容器 docker run -it -d --name demo --privileged -p 3128:3128 -v /dev/bus/usb:/dev/bus/usb centos:7 bash # - v 映射宿主机usb到容器, 其实开了特权模式就不需要了 # 进入容器 docker exec -it demo bash 配置 ## docker # 删掉 -net 0.0.0.0 eth0 的路由, 不然拨号之后无法上网 route del -net 0.0.0.0 eth0 # 安装必要的服务 apt install net-tools wvdial squid vim # 不是必要的服务 apt install iputils-ping # 配置 wvdial: vim /etc/wvdial.conf # 可以新加一组配置, [Dialer SG75-01] [Dialer SG75-01] Init1 = ATZ Init2 = ATQ V1 E1 S0=0 &C1 &D2 +FCLASS=0 Init3 = ATE0V1 Init4 = ATS0=0 Init5 = AT+CGDCONT=1, \"IP\", \"3GNET\" # 这里 APN Init5中的 3GNET, 所以可以不填 # APN = 3GNET Init6 = AT+CFUN=1 Modem Type = Analog Modem # 波特率常见是 9600, 115200, 460800 Baud = 115200 New PPPD = yes # Modem 一般为一组 ttyUSB* 中的第一个 Modem = /dev/ttyUSB0 ISDN = 0 Phone = *99# # 用户名, 密码可以不填 Username = username Password = password Stupid Mode = 1 # wvdial拨号, 可以配置多组 wvdial SG75-01 # 配置 squid # 代理访问授权 echo \"http_access allow all\" > /etc/squid/squid.conf.tmp cat /etc/squid/squid.conf >> etc/squid/squid.conf mv /etc/squid/squid.conf.tmp /etc/squid/squid.conf # 关闭一些 header echo \"via off\" >> /etc/squid/squid.conf echo \"forwarded_for delete\" >> /etc/squid/squid.conf # 等同下面的方式 echo \"request_header_access Via deny all\" >> /dev/squid/squid.conf echo \"request_header_access X-Forwarded-For deny all\" >> /dev/squid/squid.conf # 设置超时 echo \"request_timeout 2 minutes\" >> /etc/squid/squid.conf echo \"write_timeout 2 minutes\" >> /etc/squid/squid.conf echo \"read_timeout 2 minutes\" >> /etc/squid/squid.conf # 关闭日志/缓存 echo \"access_log none\" >> /dev/squid/squid.conf echo \"cache_store_log none\" >> /dev/squid/squid.conf echo \"cache_log /dev/null\" >> /dev/squid/squid.conf echo \"logfile_rotate 0\" >> /dev/squid/squid.conf # 下面两个配置高版本不兼容, 低版本可以用 echo \"cache_dir no-store \" >> /dev/squid/squid.conf echo \"cache_mem 0MB\" >> /dev/squid/squid.conf # 更改默认路由, 默认使用4G的网卡上网 # 如果没有改默认路由的话，在不指定网卡的情况下，4G 网卡并不会被使用到，因为默认路由指向的是 Docker 自身的虚拟网卡，那个网卡通向你原本的内网环境。也就是说，IP 不会变 route del -net 0.0.0.0 eth0 route add -net 0.0.0.0 ppp0 打包 # 列出在运行的容器 docker ps # 把运行中的 docker 做的修改, 提交成一个镜像 docker commit -m='wvdial_4G_centos' --author='xiaoteng' d5e87290836d xiaoteng/wvdial_4g:v1 # 使用自定义提交的镜像运行容器 docker run -it -d --name for_4g_1 -privileged xiaoteng/wvdial_4g:v1 bash # 如果需要在别的机器上运行, 可以直接导出镜像文件 # 导出镜像 docker export 82xxxxx > wvdial_4G_v1.image # 导入镜像文件 docker import wvdial_4G_v1.image zhipeng/wvidal:v1 # 或者保存镜像 docker save 82xxxxx > wvdial_4G_v1.image docker load wvdial_4G_v1.image 启动流程 3. 直接从python脚本实现随机选择网卡 参考 https://stackoverflow.com/questions/12585317/requests-bind-to-an-ip https://blog.csdn.net/u012731379/article/details/78711549 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-15 20:36:49 "},"crawler/random_user_agent.html":{"url":"crawler/random_user_agent.html","title":"随机user-agent","keywords":"","body":"随机User Agent随机User Agent 运行爬虫时，通常使用随机而正确的请求头,fake_useragent这个第三方库可以用来快速实现 from fake_useragent import UserAgent ua = UserAgent() headers['User-Agent'] = ua.random 访问移动网页时，需要加移动的浏览器头，android和ios https://developers.whatismybrowser.com/useragents/explore网站提供了所有的user-agent mobile_user_agents = [...] def get_ua(is_mobile=False): return random.choice(mobile_user_agents) if is_mobile else ua.random Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/verification_code.html":{"url":"crawler/verification_code.html","title":"验证码破解","keywords":"","body":"Verification code 验证码破解1. 类型2. 对待策略3. 破解方案参考Verification code 验证码破解 1. 类型 数字，字母，字母数字组合 中文验证码 极验 2. 对待策略 试探，尽量不触发 接入第三方解决方案(比如：云打码平台) 自己解决，参考破解方案 3. 破解方案参考 https://github.com/luyishisi/Anti-Anti-Spider (多项目汇总) https://github.com/nladuo/captcha-break https://medium.com/@ageitgey/how-to-break-a-captchasystem-in-15-minutes-with-machine-learning-dbebb035a710 https://github.com/nickliqian/cnn_captcha https://github.com/muchrooms/zheye https://github.com/zhongyiio/crack-geetest https://github.com/darbra/geetest https://www.aneasystone.com/archives/2018/03/pythonselenium-geetest-crack.html Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/cookies.html":{"url":"crawler/cookies.html","title":"Cookie","keywords":"","body":"Cookies1. set cookie2. get cookie3. get set_cookieCookies [TOC] 1. set cookie requests scrapy scrapy 自动管理cookie, 无需过多干预 2. get cookie requests scrapy 3. get set_cookie ``` import requests from http.cookies import SimpleCookie res = requests.get('https://www.xiaohongshu.com/') cookie = SimpleCookie(res.headers.get('Set-Cookie')) for i in cookie.values(): i.key i.value # scrpay cookies = response.headers.getlist('Set-Cookie') for cookie in cookies: cookie = cookie.decode('utf-8') cookie_dict = SimpleCookie(cookie) for i in cookie_dict.values(): i.key i.value ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-26 11:59:29 "},"crawler/anti_spider_strategy.html":{"url":"crawler/anti_spider_strategy.html","title":"反爬虫策略","keywords":"","body":"反爬虫策略 Anti-spider strategy1. 反爬措施收集反爬虫策略 Anti-spider strategy 通过header User-Agent, Cookie, Referer 访问行为(比如高并发) JavaScript动态计算 验证码 蜜罐(设置一些普通用户访问不到的内容) 识别到爬虫，返回假数据 1. 反爬措施收集 |Cookie |验证用户cookie |大众点评| |上行加密 |加密了上行请求的参数，导致无法分析请求 |航旅纵横| |Headers中加Shield实时加密 |对每个请求都做了加密和验证 |小红书| |Protobuf |请求和响应采用protobuf传输, 如果找不到加密模版, 就无法理解返内容 |Mercari(美国)| |阻止web调试 |用脚本检测是否打开了浏览器调试工具 |同程旅游| |多次请求 |通过中间请求获取token给目标请求 |JAL(日航国内版)| 东航反爬记录 先发送http://observer.ceair.com/ta.png? 发送\"seriesid\":\"8980ed209e5811eab0aebfab2f9b3d9e\", 可以随机 不需要cookie 返回set-cookie: user_ta_session_id, Webtrends 发送http://www.ceair.com/otabooking/flight-search!doFlightSearch.shtml cookie: 带user_ta_session_id, Webtrends post_data中包含seriesid seriesid {\"adtCount\":1,\"chdCount\":0,\"infCount\":0,\"currency\":\"CNY\",\"tripType\":\"OW\",\"recommend\":false,\"reselect\":\"\",\"page\":\"0\",\"sortType\":\"a\",\"sortExec\":\"a\",\"seriesid\":\"8980ed209e5811eab0aebfab2f9b3d9e\",\"segmentList\":[{\"deptCd\":\"PEK\",\"arrCd\":\"SHA\",\"deptDt\":\"2020-05-26\",\"deptAirport\":\"\",\"arrAirport\":\"\",\"deptCdTxt\":\"北京\",\"arrCdTxt\":\"上海\",\"deptCityCode\":\"BJS\",\"arrCityCode\":\"SHA\"}],\"version\":\"A.1.0\"} 有的ip可能需要发送两次返回结果 web端用js检测用户浏览器是否开启调试面板 实现参考知乎 github-用法 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-28 15:01:47 "},"crawler/crawler_optimization.html":{"url":"crawler/crawler_optimization.html","title":"爬虫优化","keywords":"","body":"Crawler optimization 爬虫优化Crawler optimization 爬虫优化 并发爬虫 多线程 多进程 concurrent.futures asyncio 使用连接池(urllib3) 实现了连接池和支持数据的压缩 为什么要使用连接池呢？ 保持链接可以节省网络链接的时间和资源 开启gzip 使用消息队列 布隆过滤器(Bloom Filter) scrapy使用了 参考链接 结合redis的去重 熟悉抓取相关框架的参数 批量存到数据库 设置请求超时时间 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"crawler/ocr_baidu.html":{"url":"crawler/ocr_baidu.html","title":"ocr调用百度接口","keywords":"","body":"OCR 百度api调用OCR 百度api调用 [TOC] 文档 https://ai.baidu.com/ai-doc/OCR/fk3h7xu7h 注册 控制台选择文字识别 --> 通用文字识别 --> 创建应用 --> 获取到api key 和secrec key 获取access token, 有效期一个月 # encoding:utf-8 import requests # client_id 为官网获取的AK， client_secret 为官网获取的SK host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=【官网获取的AK】&client_secret=【官网获取的SK】' response = requests.get(host) if response: print(response.json()) 发请求 def general_basic(pic_path): ''' 百度文字识别接口 通用文字识别 ''' request_url = \"https://aip.baidubce.com/rest/2.0/ocr/v1/general_basic\" # 二进制方式打开图片文件 f = open(pic_path, 'rb') img = base64.b64encode(f.read()) params = {\"image\":img} access_token = '------' request_url = request_url + \"?access_token=\" + access_token headers = {'content-type': 'application/x-www-form-urlencoded'} response = requests.post(request_url, data=params, headers=headers) print('百度api返回:{}'.format(response.json())) if response: words = response.json().get('words_result') if words == []: return '' else: return words[0].get('words') 费用: 通用文字每天5w免费 https://ai.baidu.com/ai-doc/OCR/fk3h7xu7h Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-01 17:54:28 "},"crawler/others.html":{"url":"crawler/others.html","title":"others","keywords":"","body":"琐碎琐碎 下载日本邮编地址https://www.post.japanpost.jp/zipcode/dl/roman-zip.html Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/http/http.html":{"url":"python-web/http/http.html","title":"HTTP","keywords":"","body":"Http 协议HyperText Transfer Protocol基本概念请求方法状态码请求头Cookie/Session身份验证代理HTTP/2Http 协议 HyperText Transfer Protocol 超文本传输协议 基本概念 URL: Uniform Resource Locator 统一资源标识符 协议类型:[//服务器地址[:端口号]][/资源层级UNIX文件路径]文件名[?查询][#片段ID] eg:https://www.douban.com/note/5264875/?type=like#sep 端口默认: http 80 https 443 |#用作锚点, 定位页面的特定位置 https: 超文本传输安全协议(HyperText Transfer Protocol Secure) 防止电信运营商的劫持 安全 用户体验 请求方法 GET 从制定的资源请求数据 POST 向指定的资源提交要被处理的数据 PUT 替换(更新)资源 DELETE 删除指定资源 HEAD 获得报文首部 OPTIONS 返回服务器支持的HTTP方法 状态码 1xx (信息性状态码)接收的请求正在处理 2XX200 OK 成功204 No Content 代表服务器已经对请求成功处理, 但是没有返回内容206 Partial Content 表示客户端进行了范围请求 3XX301 Moved Permanently 永久性重定向, 配合Location使用 302 Found 临时性重定向304 Not Modified 使用本地资源(比如缓存) 4XX 400 Bad Request 请求的内容有问题401 Unauthorized 未验证的请求403 Forbidden 请求资源的访问被服务器拒绝404 Not Found 服务器上无法找到请求的资源 5XX500 Internal Server Error 服务器端在执行请求时发生了错误502 Bad Gateway nginx或者网关后面没有上游服务器能相应请求503 Service Unavailable 服务器暂时处于超负荷或正在进行停机维护504 Gateway Timeout 对nginx或者网关请求超时了 请求头 通用头部字段Cache 控制缓存的行为Connection 逐条首部,连接的管理Date 创建报文的日期Pragma 报文指令Trailer 报文末端的首部一览Upgrade 升级为其他协议 Via 代理服务器的相关信息Warning 错误通知 常用字段Cache-Control减少网络延迟和提升性能 # 禁止缓存 Cache-Control: no-store Cache-Control: no-store, no-cache, must-revalidate # 强制确认缓存 Cache-Control: no-cache # 私有缓存和公共缓存 Cache-Control: private Cache-Control: public # 缓存过期机制 Cache-Control: max-age=31536000 # 表示资源能够被缓存的最大时间(s) # 缓存验证确认 Cache-Control: must-revalidate 常用字段Connection可以保持连接, 减少关闭网络连接的开销 Connection: keep-alive Connection: close 常用字段Via可以保持连接, 减少关闭网络连接的开销 Connection: keep-alive Connection: close Cookie/Session HTTP是无状态协议, 需要设置机制记录用户状态, 用来: 登录状态记录, 限制登陆后显示页面 Cookie 通过在请求和相应报文中写入Cookie信息来控制客户端的状态服务端给客户端发送的报文有Set-Cookie项客户端解析Set-Cookie, 下次请求时, 就在请求报文中添加Cookie发送给服务器, 服务器就可以判断 Session 由于Cookie明文请求, 容易被伪造, 信息太多是影响请求效率 等原因, 引入session机制.session保存在服务器端, 加密后将加密id作为Cookie返回给客户端 身份验证 适用场景: 移动设备API调用 第三方网站使用本网站进行身份认证, 且不希望直接注册本网站账号 主流方案: 基本认证(Basic access authentication)认证步骤: 客户端请求一个需要身份验证的页面 如果输入错误, 返回401应答 用户输入用户名和密码后, 请求增加认证消息头, 服务器认证通过后, 返回正常页面 摘要式的认证(Digest access authentication) 在提交之前, 对密码进行一个加密函数 OAuth 允许用户让第三方应用访问该用户在某一网站上存储的私密资源, 而无需将用户名和密码提供给第三方应用 JWT(JSON Web Token) 头部 代理 HTTP/2 优化了性能, 兼容htpp1的语义 多路复用 二进制协议 表头压缩 服务端推送 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/http/post.html":{"url":"python-web/http/post.html","title":"POST","keywords":"","body":"POSTPOST HTTP协议中的post请求，由状态行、请求头(headers)和消息主体(entity-body)组成post的消息主体（entity-body）的说明 消息主体(entity-body)是在HTTP协议中规定，以ASCII码传输，但协议没有规定使用何种编码，所以POST请求包含了Content-Type(编码说明)和entity-body(消息主体)两部分 Content-Type样式： x-www-form-urlencoded # headers样式 # 消息主体发送浏览器form表单，消息主体进行url编码 headers = { 'Content-Type':'application/x-www-form-urlencoded' } # url编码后的样式 title=test&sub%5B%5D=1&sub%5B%5D=2&sub%5B%5D=3 requests样式import requests payload = {} # 一个字典，requests在发送时会自动编码 res = requests.post(url=url, data=payload) requests发送listpayload = (('key1', 'value1'), ('key1', 'value2')) r = requests.post('http://httpbin.org/post', data=payload) >\"form\": { > \"key1\": [ > \"value1\", > \"value2\" > ] > }, application/json # headers样式 # 消息主题是序列化后的JSON字符串 headers = { 'Content-Type':'application/json' } requests样式 import requests payload = {} # 一个字典，requests在发送时会自动编码 r = requests.post(url, data=json.dumps(payload)) multipart/form-data text/xml Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/restful.html":{"url":"python-web/restful.html","title":"RESTful API","keywords":"","body":"Restful API1. SOAP 和 RESTful架构2. 如何设计RESTful APIRestful API 核心是面向资源 所有事务都可以抽象为资源 每个资源都有唯一标识 资源: 歌曲,文章,文本,视频 1. SOAP 和 RESTful架构 效率和易用性, soap易用性差 安全性方面: RESTful使用安全要求不高的资源型服务接口. 2. 如何设计RESTful API 资源路径 资源常用复数 https://api.example.com/v1/posts // 博客资源 HTTP动词 GET /posts/ID POST /posts PUT / PATCH 都是update 幂等性, 多次请求结果一致 PUT 要求幂等性, 如果位置不存在,则创建一个, 要求带有完整的信息 PATCH 不是幂等的, 如果位置不存在,则出现错误, 也不会自动重新尝试失败的请求 /posts/ID DELETE /posts/ID 过滤信息 api提供参数进行筛选 ?offset=10 ?page=2&per_page=100 ?sortby=name&order=asc ?post_id=1 状态码 成功状态码 200 OK 服务器成功返回用户请求的数据 201 CREATED 新建或修改数据成功 204 NO CONTENT 删除数据成功 客户端请求有问题 400 BAD REQUEST 用户发出的请求有错误 401 Unauthorized 表示用户没有认证,无法进行当前操作 403 Forbidden 表示当前用户访问是被禁止的 422 Unprocesable Entity 表示当创建一个对象时,发生一个验证错误 服务端错误 500 INTERNAL SERVER ERROR 服务器发生错误,用户将无法判断发出的请求是否成功 错误处理 响应提示 { \"error\": \"参数错误\" } 返回结构 GET /collections 返回资源对象的列表 GET /collections/identity 返回单个资源对象 POST /collections 返回新生成的资源对象 PUT /collections/identity 返回完整的资源对象 PATCH /collections/identity 返回被修改的属性 DELETE /collections/identity 返回一个空文档 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/ajax.html":{"url":"python-web/ajax.html","title":"Ajax","keywords":"","body":"AjaxHTTP瓶颈Ajax的实现ajax函数具体操作domAjax Asynchronous JavaScript + XML HTTP瓶颈 表单提交一次操作渲染整个页面 有些操作将会增加带宽(点赞)有的操作会影响使用当前使用(视频网站的收藏\\点赞等) 内容的实时更新通知/私信等提醒 Ajax的实现 只做局部更新 一个demo ajax函数 var log = function() { console.log.apply(console, arguments) } var e = function(sel) { return document.querySelector(sel) } /* ajax 函数 */ var ajax = function(method, path, data, responseCallback) { var r = new XMLHttpRequest() // 设置请求方法和请求地址 r.open(method, path, true) // 设置发送的数据的格式为 application/json // 这个不是必须的 r.setRequestHeader('Content-Type', 'application/json') // 注册响应函数 r.onreadystatechange = function() { if(r.readyState === 4) { // r.response 存的就是服务器发过来的放在 HTTP BODY 中的数据 responseCallback(r.response) } } // 把数据转换为 json 格式字符串 data = JSON.stringify(data) // 发送请求 r.send(data) } // TODO API // 获取所有 todo var apiTodoAll = function(callback) { var path = '/api/todo/all' ajax('GET', path, '', callback) } // 增加一个 todo var apiTodoAdd = function(form, callback) { var path = '/api/todo/add' ajax('POST', path, form, callback) } // 删除一个 todo var apiTodoDelete = function(id, callback) { var path = '/api/todo/delete?id=' + id ajax('GET', path, '', callback) // get(path, callback) } // 更新一个 todo var apiTodoUpdate = function(form, callback) { var path = '/api/todo/update' ajax('POST', path, form, callback) // post(path, form, callback) } // load weibo all var apiWeiboAll = function(callback) { var path = '/weibo/all' ajax('GET', path, '', callback) } // 增加一个 微博 var apiWeiboAdd = function(form, callback) { var path = '/weibo/add' ajax('POST', path, form, callback) } // 删除一个微博 var apiWeiboDelete = function(weibo_id, callback) { var path = '/weibo/delete?id='+weibo_id ajax('GET', path, '', callback) } // 更新一个微博 var apiWeiboUpdate = function(form, callback) { var path = '/weibo/update' ajax('POST', path, form, callback) } // 增加一条微博评论 var apiCommentAdd = function(form, callback){ var path = '/weibo/commentAdd' ajax('POST', path, form, callback) } // 删除一条微博评论 var apiCommentDelete = function(comment_id, callback){ var path = '/weibo/commentDelete?id='+comment_id ajax('GET', path, '', callback) } 具体操作dom var timeString = function(timestamp) { t = new Date(timestamp * 1000) t = t.toLocaleString('ja-JP') return t } var commentsTemplate = function(comments) { var html = '' for(var i = 0; i [${c.username}]:${c.content} [创建时间]: ${timeString(c.ct)} 删除评论 ` html += t } return html } var WeiboTemplate = function(Weibo) { var content = Weibo.content var id = Weibo.id var ct = Weibo.ct var visitor = Weibo.visitor var comments = commentsTemplate(Weibo.comments) var t = ` [${visitor}]: ${content} [留言时间]: ${timeString(ct)} 删除留言 编辑留言 现有评论: ${comments} 评论内容: 评论人(默认匿名): 添加评论 ============================================================ ` return t } var insertWeibo = function(Weibo) { var WeiboCell = WeiboTemplate(Weibo) // log('单个微博html标签', WeiboCell) // 插入 Weibo-list var WeiboList = e('.Weibo-list') WeiboList.insertAdjacentHTML('beforeend', WeiboCell) } var insertEditForm = function(cell, content) { var form = ` // 更新 ` cell.insertAdjacentHTML('afterBegin', form) var input = e('.Weibo-edit-input') input.value = content } var loadWeibos = function() { // 调用 ajax api 来载入数据 apiWeiboAll(function(r) { // console.log('load all', r) // 解析为 数组 var Weibos = JSON.parse(r) // 循环添加到页面中 for(var i = 0; i 2){ apiWeiboAdd(form, function(r) { // 收到返回的数据, 插入到页面中 var Weibo = JSON.parse(r) insertWeibo(Weibo) }) } else{ alert(\"留言内容太少了吧!\") } }) } var bindEventWeiboDelete = function() { var WeiboList = e('.Weibo-list') // 注意, 第二个参数可以直接给出定义函数 WeiboList.addEventListener('click', function(event){ var self = event.target if(self.classList.contains('Weibo-delete')){ // 删除这个 Weibo var WeiboCell = self.parentElement var Weibo_id = WeiboCell.dataset.id log('点击到了删除按钮, 微博id是:', Weibo_id) apiWeiboDelete(Weibo_id, function(r){ log('删除成功', Weibo_id) var return_wb = JSON.parse(r) if(return_wb.id==Weibo_id){ WeiboCell.remove() } }) } }) } var bindEventWeiboEdit = function() { var WeiboList = e('.Weibo-list') // 注意, 第二个参数可以直接给出定义函数 WeiboList.addEventListener('click', function(event){ var self = event.target if(self.classList.contains('Weibo-edit')){ // 删除这个 Weibo var WeiboCell = self.parentElement // 从微博cell中选出展示微博内容的那部分,缩小了查找范围 var weibo_content = WeiboCell.querySelector('.Weibo-content') var content = weibo_content.innerHTML // 将原来的展示内容改为修改框 weibo_content.parentElement.style.display=\"none\" // 插入更新框 insertEditForm(WeiboCell, content) } }) } var bindEventWeiboUpdate = function() { var WeiboList = e('.Weibo-list') // 注意, 第二个参数可以直接给出定义函数 WeiboList.addEventListener('click', function(event){ var self = event.target if(self.classList.contains('Weibo-update')){ log('点击了 update ') // var editForm = self.parentElement // querySelector 是 DOM 元素的方法 // document.querySelector 中的 document 是所有元素的祖先元素 var input = editForm.querySelector('.Weibo-edit-input') var content = input.value // 用 closest 方法可以找到最近的直系父节点 var WeiboCell = self.closest('.Weibo-cell') var Weibo_id = WeiboCell.dataset.id var form = { 'id': Weibo_id, 'content': content, } apiWeiboUpdate(form, function(r){ log('更新成功', Weibo_id) var Weibo = JSON.parse(r) log(Weibo) var selector = '#Weibo-' + Weibo.id var WeiboCell = e(selector) var contentSpan = WeiboCell.querySelector('.Weibo-content') contentSpan.innerHTML = Weibo.content // 显示微博内容框 contentSpan.parentElement.style.display=\"\" // 将编辑框删除 var edit_form = WeiboCell.querySelector(\".Weibo-edit-form\") edit_form.remove() // WeiboCell.remove() }) } }) } // 插入评论函数 var insertCommentCell = function(comment){ var comments = [comment] var commentCell = commentsTemplate(comments) var weibo_id = comment.weibo_id var weiboCell = e('#Weibo-' + weibo_id) // log('weiboCell:', weiboCell) var commentsList = weiboCell.querySelector('.comment-list') commentsList.insertAdjacentHTML('beforeend', commentCell) } // 绑定添加评论事件 var bindEventCommentAdd = function(){ var WeiboList = e('.Weibo-list') WeiboList.addEventListener('click', function(event){ var self = event.target if(self.classList.contains(\"comment-add\")){ // log('点击到了评论按钮', self) var comment_form = self.parentElement var input1 = comment_form.querySelector('.comment-content') var comment_content = input1.value var input2 = comment_form.querySelector('.comment-username') var comment_username= input2.value var WeiboCell = comment_form.parentElement var Weibo_id = WeiboCell.dataset.id // log('当前微博的id', Weibo_id) var form = { 'weibo_id': Weibo_id, 'content': comment_content, 'username': comment_username, } input1.value = '' input2.value = '' if(comment_content.length>0){ apiCommentAdd(form, function(r){ // 收到响应, 先log出来 var new_comment = JSON.parse(r) // log('评论已添加', new_comment) insertCommentCell(new_comment) }) }else{ alert('评论内容不可为空') } } }) } // 绑定删除评论事件 var bindEventCommentDelete = function(){ var WeiboList = e('.Weibo-list') WeiboList.addEventListener('click', function(event){ var self = event.target if(self.classList.contains(\"comment-delete\")){ log('点击到了删除评论按钮', self) var comment_form = self.parentElement comment_id = comment_form.dataset.id log('报告,当前要删除的评论id', comment_id) apiCommentDelete(comment_id, function(r){ var delete_comment = JSON.parse(r) comment_form = e('#comment-id-'+comment_id) // log('报告,我要移除的评论标签:', comment_form) comment_form.remove() }) } }) } var bindEvents = function() { bindEventWeiboAdd() bindEventWeiboDelete() // bindEventWeiboEdit() // bindEventWeiboUpdate() bindEventCommentAdd() // bindEventCommentDelete() } var __main = function() { bindEvents() loadWeibos() } __main() Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/jinja.html":{"url":"python-web/jinja.html","title":"jinja","keywords":"","body":"jinjajinja demo ```python3 from jinja2 import Template tem_str = ''' version: \"3\" services: master: build: ./master ports: - \"3128:3128\" - \"5000:5000\" networks: - mobile_network_proxy redis: image: redis expose: - \"6379\" networks: - mobile_network_proxy networks: mobile_network_proxy: nics = [{\"num\": num, \"tty_usb_num\": (num + 1) * 4 - 1} for num in range(5)] text = template.render(nics=nics) with open(\"docker-compose.yml\", \"w\") as f: f.write(text) ''' ``` more Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-14 16:53:34 "},"python-web/huan-jing-guan-li.html":{"url":"python-web/huan-jing-guan-li.html","title":"环境管理","keywords":"","body":"包管理和虚拟环境1. virtualenv安装2. pip 包管理3. pipenv4. pipenv 和autoenv的组合5. conda学习EmacsPycharm安装和使用使用IPythonWeb开发环境配置包管理和虚拟环境 包安装方法通过Python社区开发的pip, easy_install等工具使用系统本身自带的包管理器(yum, apt-get)通过源码安装 1. virtualenv安装 虚拟环境的包是对真实环境包的一个复制 virtualenv是python2使用的, python3.3引入了pyvenv, 作为自带模块 virtualenv默认有python可执行文件, 常用标准库等. python2 sudo pip install virtualenv # 安装virtualenv # 创建一个project mkdir project cd project virtualenv venv # 启动一个虚拟环境(名为:venv, 也可以带一些路径, 默认当前文件夹), 默认复制系统所有的第三方包 virtualenv --no-site-packages venv # 启动一个虚拟环境 virtualenv venv --python=python3.6 ,不包含任何第三方包 source venv/bin/activate # 生效一个虚拟环境 (venv)>which python > /home.../bin/python (venv)>deactivate # 退出虚拟环境 python3.3+ python3 -m venv env_name --no-site-packages # 不包含第三方包 --python=python3.6 # 指定python版本 # 启动一个虚拟环境(名为:env_name, 也可以带一些路径, 默认当前文件夹), 默认复制系统所有的第三方包 # 创建一个project source venv/bin/activate # 生效一个虚拟环境 (venv)>which python > /home.../bin/python (venv)>deactivate # 退出虚拟环境 windows启动虚拟环境 # 直接命令行运行active文件执行 C:\\Users\\jizhu>task\\mxt_blogs\\Scripts\\activate (mxt_blogs) C:\\Users\\jizhu> 2. pip 包管理 说明 使用 # 列出 pip list # 列出所有的第三方包 pip freeze > requirement.txt # 导出当前环境下的所有第三方包 # 安装 pip install requests # 安装包 pip install requests==3.6.0 pip install -r requirement.txt # 根据配置文件生成相同的环境 # 卸载 pip uninstall requests pip uninstall -r requirements.txt # 升级 pip install -U requests pip install -U pip # 升级pip # 显示包所在目录 pip show -f requests # 搜索包 pip search # 查询可升级的包 pip list -o # 下载包而不安装 pip install -d pip install -d -r requirements.txt # 打包 pip wheel 有的环境中, python3/2与pip没有绑定, 用pip/pip3 或python -m pip 指定安装源 单次安装源 pip install -i http://pypi.douban.com/simple 全局修改 unix和macos: $HOME/.pip/pip.conf windows: %HOME%\\pip\\pip.ini[global] timeout = 6000 index-url = http://pypi.douban.com/simple 3. pipenv 是python项目的依赖管理器 根据pipfile自动寻找项目根目录 如果不存在,自动生成pipfile和pipfile.lock 自动在项目目录的.venv目录创建虚拟环境. 自动管理pipfile新安装和删除的包 自动更新pip 使用pipenv代替pip安装包 4. pipenv 和autoenv的组合 autoenv可以在切换文件目录的同时, 自动完成激活虚拟环境 用法 ``` sudo pip install autoenv source /usr/local/bin/activate.sh mkdir test cd test touch .env echo 'source /home/xx/venv/bin/activate' > .env cd cd test # 就会自动激活虚拟环境 ``` 5. conda 1. 安装 ``` # miniconda wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh # conda wget https://repo.continuum.io/miniconda/Anaconda-latest-Linux-x86_64.sh bash Anaconda-latest-Linux-x86_64.sh ``` 学习Emacs 安装 两种模式 GUI模式emacs # 默认启动GUI emacd -nw FILE # 终端中启动 Daemon模式 学习lisp Pycharm安装和使用 使用IPython Web开发环境配置 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-28 14:58:10 "},"python-web/django/django.html":{"url":"python-web/django/django.html","title":"Django","keywords":"","body":"DjangoDjango Django入门 项目设置 HTTP Model Views Urls Templates Admin 静态文件 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 14:31:22 "},"python-web/django/django_start_up.html":{"url":"python-web/django/django_start_up.html","title":"Django入门","keywords":"","body":"开始一个项目0安装:1. 准备目录2. 创建虚拟环境3. 创建django项目4. 创建一个app5. 创建Modelsmodels.md6. 配置viewsviews.md7. 配置urlsurls.md8. 初始化数据库9. 启动服务10. 调试11. 设置settings.md开始一个项目 0安装: sudo pip install Django sudo python -m pip install Django 开发流程 搭建开发环境 创建Django工程, 基础设置(数据库版本, 模板路径, 语言等), 创建超级用户 建立应用app, 并注册到setting 创建数据库模型models, 迁移数据库 设计app需要的urls, 添加到工程的urls中 设计url对应的视图函数views, 编辑需要的模板 部署流程 clone代码 启动虚拟环境, 安装依赖 收集静态文件 1. 准备目录 mkdir mysite cd mysite 2. 创建虚拟环境 virtualenv --no-site-packages venv source venv/bin/activate # 安装包 pip -r requirement.txt 3. 创建django项目 django-admin startproject mysite cd mysit # 可以指定应用访问的web ip和端口 python36 manage.py runserver 0:8000 开发服务器自动重载代码的修改, 但是文件的添加需要重启. django提供了自动生成一个app的目录结构的功能. 4. 创建一个app python manage.py startapp polls # 将会创建出一个目录 app文件夹结构 blog\\ __init__.py admin.py apps.py migrations\\ __init__.py models.py tests.py views.py 配置文件中注册应用 blogproject/blogproject/settings.py ## 其他配置项... INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'blog', # 注册 blog 应用 ] ## 其他配置项.. 5. 创建Modelsmodels.md 6. 配置viewsviews.md 7. 配置urlsurls.md 8. 初始化数据库 ``` python manage.py makemigrations python manage.py migrate ``` 9. 启动服务 python3 manage.py runserver localhost:8000 10. 调试 python manage.py shell 11. 设置settings.md Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 14:03:46 "},"python-web/django/settings.html":{"url":"python-web/django/settings.html","title":"项目设置","keywords":"","body":"setting.pysetting.py 设置语言 blogproject/blogproject/settings.py ## 其它配置代码... LANGUAGE_CODE = 'en-us' # 'zh-hans' TIME_ZONE = 'UTC' # 'Asia/beijing' ## 其它配置代码... 设置模板路径 TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', # 重点是这句 'DIRS': [os.path.join(BASE_DIR, 'templates')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] 选择数据库版本 查看默认设置# blogproject/settings.py DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), } } 修改MySql数据库DEFAULT_CHARSET = 'utf8' DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'labelme', 'USER': 'root', 'PASSWORD': 'XXX', 'HOST': '127.0.0.1', 'PORT': '3306', 'OPTIONS': { 'sql_mode': 'traditional', # \"init_command\": \"SET sql_mode='STRICT_TRANS_TABLES'\", 'charset': 'utf8' }, } } Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 13:54:59 "},"python-web/django/HTTP.html":{"url":"python-web/django/HTTP.html","title":"HTTP","keywords":"","body":"urlapp中修改注册到project中编辑视图函数url app中修改 添加urls # blog/urls.py # url: 正则表达式匹配空字符, index: views的内容, name是index的别名 from django.conf.urls import url urlpatterns = [ url(r'^$', views.index, name='index'), ] 添加views # blog/views.py from django.http import HttpResponse # render根据返回的内容构造httpResponse from django.shortcuts import render # Create your views here. def index(request): return HttpResponse(\"欢迎访问我的博客首页！\") 注册到project中 from django.urls import path, include urlpatterns = [ path('admin/', admin.site.urls), # 此处的path, url添加后, 可以和后面的urls组合 path('', include('blog.urls')), ] 编辑视图函数 from django.urls import get_object_or_404 # 返回数据库的对象, 不存在返回404 - ... post = get_object_or_404(Post, pk=pk) ... Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"python-web/django/models.html":{"url":"python-web/django/models.html","title":"Model","keywords":"","body":"models.py0. 前言1. 初始化Models2. Django操作数据库1. 增2. 查3. 改, 更新4. 删5. 排序3. Django中的Q对象和复杂查询两个Q默认为and的关系等价SQL语句3. 迁移数据库4. 选择数据库版本5. 创建超级用户models.py [参考:https://www.cnblogs.com/sss4/p/7070942.html] 0. 前言 ORM是什么？：(在django中，根据代码中的类自动生成数据库的表也叫--code first)ORM：Object Relational Mapping(关系对象映射)类名对应------》数据库中的表名类属性对应---------》数据库里的字段类实例对应---------》数据库表里的一行数据obj.id obj.name.....类实例对象的属性 Django orm的优势：Django的orm操作本质上会根据对接的数据库引擎，翻译成对应的sql语句；所有使用Django开发的项目无需关心程序底层使用的是MySQL、Oracle、sqlite....，如果数据库迁移，只需要更换Django的数据库引擎即可； 1. 初始化Models 创建models.py来表示数据库关系 数据库关系 - ForeignKey # 外键, 设置一对多, 例如: models.ForeignKey(User, on_delete=models.CASCADE) # on_delete在1.10版本上不需要 - ManyToManyField # 多对多关系,tags = models.ManyToManyField(Tag, blank=True) 数据库内容类型 - CharField # 字符串 - TextField # 很长的字符串 - DateTimeField # 时间日期 - DateField # 日期 - 枚举字段 choice=( (1,'男人'), (2,'女人'), (3,'其他') ) lover=models.IntegerField(choices=choice) 3 内容参数 ``` - max_length=100 - blank=True # 默认False ``` 例子 from django.db import models class Post(models.Model): # 文章标题 tille = models.CharField(max_length=70) body = models.TextField() modified_time = models.DateTimeField() category = models.ForeignKey(Category, on_delete=models.CASCADE) tags = models.ManyToManyField(Tag, blank=True) 内置Model # django.contrib.auth 是 Django 内置的应用，专门用于处理网站用户的注册、登录等流程，User 是 Django 为我们已经写好的用户模型。 from django.contrib.auth.models import User 外键的创建 people_id = ForeignKey() 2. Django操作数据库 1. 增 1. 增加单个 ``` # 方法一 from blog.models import Tag p1 = Publisher(name='Apress', address='2855 Telegraph Avenue',city='Berkeley', state_province='CA', country='U.S.A.',website='http://www.apress.com/') p1.save() # 方法二 p1 = Publisher.objects.create(name='Apress',address='2855 Telegraph Avenue',city='Berkeley',state_province='CA', country='U.S.A.',website='http://www.apress.com/') ``` 2. 批量增加 ``` t1 = Tag(name='name1') t2 = Tag(name='name2') t3 = Tag(name='name3') t4 = Tag(name='name4') tags = [t1, t2, t3, t4] TAG.objects.bulk_create(tags) ``` 2. 查 1. 查询全部 ``` Tag.objects.all() > ]> # 返回类型 # 可迭代类型,每个都是model实例 ``` 2. 条件查询 ``` t = Tag.objects.get(title='test') # 返回一条数据, 当有多条或没有错误, 抛出异常 ``` 3. where 或 where not 查询 ``` t = Tag.objects.filter(condition) t = Tag.objects.exclude(condition) # condition语句支持 = 大于 小于, 与或非操作 ``` 4. 连锁查询 需要同时进行过滤和排序查询的操作时，可以简单地写成这种“链式”的形式： ``` Publisher.objects.filter(country=\"U.S.A.\").order_by(\"-name\") [, ] ``` 5. 限制返回的语句 ``` >>> Publisher.objects.order_by('name')[0] # 类似的，可以用Python的range-slicing语法来取出数据的特定子集： >>> Publisher.objects.order_by('name')[0:2] # 这个例子返回两个对象，等同于以下的SQL语句： SELECT id, name, address, city, state_province, country, website FROM books_publisher ORDER BY name OFFSET 0 LIMIT 2; # 注意，不支持Python的负索引(negative slicing)： >>> Publisher.objects.order_by('name')[-1] Traceback (most recent call last): ... AssertionError: Negative indexing is not supported. # 虽然不支持负索引，但是可以使用其他的方法。 比如，修改order_by() 语句来实现： >>> Publisher.objects.order_by('-name')[0] ``` 3. 改, 更新 1. save()方法 ``` # save方法将会更新对象的所有信息,不管有没有更改 t = Tag.objects.get(name='test').update(age=18) t.name = 'new_name' t.save() # 等效SQL UPDATE books_publisher SET name = 'Apress Publishing', address = '2855 Telegraph Ave.', city = 'Berkeley', state_province = 'CA', country = 'U.S.A.', website = 'http://www.apress.com' WHERE id = 52; ``` 2. update()方法 ``` Publisher.objects.filter(id=52).update(name='Apress Publishing') # update()对任何结果集都有效,可以同时更新多条 Publisher.objects.all().update(country='USA') # 等效SQL语句 UPDATE books_publisher SET name = 'Apress Publishing' WHERE id = 52; # 返回一个int型,表示更改的条数 ``` 4. 删 ``` # 删除数据库中的对象只需调用该对象的delete()方法即可： >>> p = Publisher.objects.get(name=\"O'Reilly\") >>> p.delete() >>> Publisher.objects.all() [] #少了一条记录 # 同样我们可以在结果集上调用delete()方法同时删除多条记录。这一点与我们上一小节提到的update()方法相似： >>> Publisher.objects.filter(country='USA').delete() >>> Publisher.objects.all().delete() >>> Publisher.objects.all() [] # 删除数据时要谨慎！ 为了预防误删除掉某一个表内的所有数据，Django要求在删除表内所有数据时显示使用all()。 # 比如，下面的操作将会出错： >>> Publisher.objects.delete() Traceback (most recent call last): File \"\", line 1, in AttributeError: 'Manager' object has no attribute 'delete' # 而一旦使用all()方法，所有数据将会被删除： >>> Publisher.objects.all().delete() # 如果只需要删除部分的数据，就不需要调用all()方法。再看一下之前的例子： >>> Publisher.objects.filter(country='USA').delete() ``` 5. 排序 1. 自定义排序 ``` # 升序排列： >>> Publisher.objects.order_by(\"name\") [, ] # 降序排列： >>> Publisher.objects.order_by(\"-name\") [, ] # 按多个字段排列（第二个字段会在第一个字段的值相同的情况下被使用到）： >>> Publisher.objects.order_by(\"state_province\", \"address\") [, ] ``` 2. 缺省默认排序 ``` class Publisher(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50) city = models.CharField(max_length=60) state_province = models.CharField(max_length=30) country = models.CharField(max_length=50) website = models.URLField() def __unicode__(self): return self.name class Meta: ordering = ['name'] # 你可以在任意一个 模型 类中使用 Meta 类，来设置一些与特定模型相关的选项。 如果你设置了ordering这个选项，那么除非你检索时特意额外地使用了 order_by()， 否则，当你使用 Django 的数据库 API 去检索时，Publisher对象的相关返回值默认地都会按 name 字段排序。 ``` 3. Django中的Q对象和复杂查询 多次查询 # 一般我们在Django程序中查询数据库操作都是在QuerySet里进行进行，例如下面代码: >>> q1 = Entry.objects.filter(headline__startswith=\"What\") >>> q2 = q1.exclude(pub_date__gte=datetime.date.today()) >>> q3 = q1.filter(pub_date__gte=datetime.date.today()) 组合 ``` 两个Q默认为and的关系 News.objects.get( Q(question__startswith='Who'), Q(pub_date=date(2005, 5, 2)) | Q(pub_date=date(2005, 5, 6)) ) 等价SQL语句 SELECT * from news WHERE question LIKE 'Who%' AND (pub_date = '2005-05-02' ORpub_date = '2005-05-06') # Q语句结合关键字查询时,需要关键字在前 #正确的做法 News.objects.get( Q(pub_date=date(2005, 5, 2)) | Q(pub_date=date(2005, 5, 6)), question__startswith='Who') ``` 3. 迁移数据库 # django判断做了哪些改变 # migrations\\ 目录下生成了一个 0001_initial.py 文件 python manage.py makemigrations # 操作数据库, 建表 python manage.py migrate python manage.py sqlmigrate blog 0001 # 将显示经django翻译后的SQL语句 4. 选择数据库版本 见设置文件 5. 创建超级用户 python manage.py createsuperuser Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 14:04:00 "},"python-web/django/views.html":{"url":"python-web/django/views.html","title":"Views","keywords":"","body":"views1. django orm三种查询方法2. views三种返回3. 对cookie和session的处理4. request的几个属性views 1. django orm三种查询方法 直接导入执行SQL from django.db import connection cursor = connection.cursor() cursor.execute(\"insert into hello_author(name) VALUES ('郭敬明')\") cursor.execute(\"update hello_author set name='韩寒' WHERE name='郭敬明'\") cursor.execute(\"delete from hello_author where name='韩寒'\") cursor.execute(\"select * from hello_author\") cursor.fetchone() cursor.fetchall() 使用raw books=Book.objects.raw('select * from hello_book') for book in books: print book 使用orm方法 # 使用extra：查询人民邮电出版社出版并且价格大于50元的书籍 Book.objects.filter(publisher__name='人民邮电出版社').extra(where=['price>50']) 2. views三种返回 HttpResponse() 直接返回 from django.http import HttpResponse def index(request): return HttpResponse(\"欢迎访问我的博客首页！\") render from django.shortcuts import render def index(request): post_list = Post.objects.all().order_by('-created_time') context = { 'post_list': post_list, 'title': '博客首页', 'welcome': '欢迎访问我的博客首页', } return render(request, 'blog/index.html', context=context) JsonResponse() redirect('/other_url') 3. 对cookie和session的处理 cookie request.COOKIES.get('islogin') response.set_cookie('islogin', 'yes') session ``` 在django—session表中创建一条记录: session-key session-data ltv8zy1kh5lxj1if1fcs2pqwodumr45t 更新数据 else: 1 生成随机字符串 ltv8zy1kh5lxj1if1fcs2pqwodumr45t 2 response.set_cookie(\"sessionid\",ltv8zy1kh5lxj1if1fcs2pqwodumr45t) 3 在django—session表中创建一条记录: session-key session-data ltv8zy1kh5lxj1if1fcs2pqwodumr45t {\"is_login\":True,\"username\":\"yuan\"} ``` 4. request的几个属性 Path method GET COOKIES Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 13:33:47 "},"python-web/django/urls.html":{"url":"python-web/django/urls.html","title":"Urls","keywords":"","body":"URLSURLS 路径配置位于urls.py 每个应用有单独的urls.py,如blog/urls.py from django.urls import path from . import views from django.conf.urls import url urlpatterns = [ path(r'', views.index, name='index'), # or # url(r'hello/', view.hello), ] 配置到总目录中, projects/urls.py from django.contrib import admin from django.urls import path, include urlpatterns = [ path('admin/', admin.site.urls), path(r'blog/',include('blog.urls')), ] Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 13:59:25 "},"python-web/django/templates.html":{"url":"python-web/django/templates.html","title":"Templates","keywords":"","body":"Templates 模板1. 使用django模板Templates 模板 1. 使用django模板 位置，项目根目录下/templates 每个应用存放在单独的文件夹中 /project/settings.py中生效 ... TEMPLATES = [ { ... 'DIRS': [os.path.join(BASE_DIR, 'templates'),], # 修改位置 ... }, ] 使用render渲染 # 见views render(request, 'blog/index.html', context=context) 静态文件static_files.md Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 14:30:27 "},"python-web/django/static_files.html":{"url":"python-web/django/static_files.html","title":"静态文件","keywords":"","body":"静态文件1. 开发使用2. 部署使用静态文件 官方文档 1. 开发使用 位置: 存放在对应的应用下,django静态服务器会收集所有static目录下的文件 /blog/static/blog/jsor /blog/static/blog/css settings 中配置 STATIC_URL = '/static/' 引用 {\\% load staticfiles \\%} 2. 部署使用 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-11 16:57:22 "},"python-web/django/admin.html":{"url":"python-web/django/admin.html","title":"Admin","keywords":"","body":"优化admin优化admin 后台显示数据库详情class PostAdmin(admin.ModelAdmin): list_display = ['title', 'created_time', 'modified_time', 'category', 'author'] # Register your models here. admin.site.register(Post, PostAdmin) Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-10 14:03:52 "},"python-web/webying-yong-an-quan.html":{"url":"python-web/webying-yong-an-quan.html","title":"WEB应用安全","keywords":"","body":"WEB应用安全SQL注入跨站伪装请求CSRF跨站脚本攻击XSSWEB应用安全 SQL注入 绝不信任用户提交的数据, 拿到用户的输入后, 应该对值进行转义, 通过参数传递 跨站伪装请求CSRF 在访问到当前页面时, 服务器发送一个token, 在删除请求时验证token是否存在. 记得及时删除 使用正确的请求方法, 不该用get的地方不用 检查请求头的Referer字段 跨站脚本攻击XSS 攻击者在网页中嵌入JavaScript脚本, 当用户浏览此网页, 脚本就会在用户的浏览器上执行, 从而达到攻击者的目的,比如: 盗取cookie, 会话信息等内容. jinja2过滤了js代码等内容, 自动转义成文本. flask关闭js访问cookie的权限 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-14 15:21:59 "},"python-web/web-server-nginx.html":{"url":"python-web/web-server-nginx.html","title":"WEB服务器Nginx","keywords":"","body":"WEB服务器Nginx1. 概述2. 使用3. 其他WEB服务器Nginx 1. 概述 web服务器和应用服务器的区别 web服务器负责处理HTTP协议 web服务器用于处理静态页面的内容, 对于python产生的动态内容,通过WSGI接口交给应用服务器来处理 一般应用服务器都集成了web服务器 尽管集成了web服务器, 但大部分用来做调试, 出于性能和稳定性的考虑,并不能在生产环境中使用. 为什么选择Nginx 处理静态文件,索引文件效率非常高 高并发连接 低的内存损耗 强大的反响代理和负载均衡功能,平衡集群中各个服务器的负载压力 2. 使用 安装 sudo yum install nginx sudo systemctl status nginx # 查看当前状态 sudo systemctl start nginx # 查看当前状态 sudo systemctl stop nginx # 查看当前状态 sudo systemctl enable nginx # 设置开机自启动 sudo systemctl restart nginx # 重启nginx 配置 # 好的习惯是先检查配置文件,再重启服务器 sudo nginx -t # 验证Nginx配置是否正确 # 修改配置文件后的重载 sudo nginx -s reload 配置文件 配置目录 # 查找主配置文件, 里面包含日志文件路径和子配置文件路径 # 优先配置方案参考https://serverfault.com/questions/527630/what-is-the-different-usages-for-sites-available-vs-the-conf-d-directory-for-ngi find / -name nginx.conf # 子路径配置文件 /etc/nginx/conf.d/*.conf 配置文件demo # nginx.conf user root; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; } # 子配置文件 server { charset utf-8; listen 80; server_name book.maxiaoteng.xyz; server_name book.maxiaoteng.tk; # 重定向 rewrite ^/(.*)$ https://maxiaoteng001.github.io/maxiaoteng-book/$1 permanent; # 代理 location /static { alias /var/www/mxt_blogs_project/static; } location / { proxy_pass http://localhost:4000; } } 日志位置 /var/log/nginx 3. 其他 什么时候选择源码安装？二进制包安装和源码安装应用场景: 对软件精简度有要求, 性能非常高的要求 源码安装自由度高 对软件打过补丁 源码安装提供了统一的安装方式,可以跨平台应用 反向代理和正向代理 通常部署web应用的时候, 都会选择一个叫做WSGI的应用服务器,搭配Nginx来使用 正向代理: 作为一个媒介将互联网获取的资源返回给相关联的客户端, 代理和客户端在一个局域网,对于服务端是透明的 反向代理: 根据客户端的请求, 从后端的服务器上获取资源, 然后再将这些资源返回给客户端. 代理和服务器在一个局域网,对客户端是透明的. 反向代理的作用 提高动态语言的I/O处理能力 加密和SSL加速 安全 负载均衡 缓存静态内容 支持压缩 负载均衡算法 round-robin # 按请求顺序依次分配, 如果某台服务器自动宕机,自动剔除 least_conn # 请求发送到活跃连接最少的服务器上 ip_hash # 按ip的hash值分配 hash 通过Gunicorn启动Flask应用 nginx代理出现502 检查日志发现 # log文件显示 15 connect() to 127.0.0.1:2018 failed (13: Permission denied) while connecting to upstream 解决办法: 关闭selinuxsudo vim /etc/selinux/config # 将SELINUX=enforcing改为SELINUX=disabled 重启即可 临时解决: setenforce 0 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-04-02 11:48:34 "},"python-web/pythonying-yong-fu-wu-qi.html":{"url":"python-web/pythonying-yong-fu-wu-qi.html","title":"Python应用服务器","keywords":"","body":"Python 应用服务器WSGI协议常见的WSGI容器- GunicornPython 应用服务器 WSGI协议 (Python Web Server Gateway Interface) Web框架和Web服务器之间进行通信, 如果在设计时他们不匹配, 那选择框架就会限制服务器的选择. WSGI在PEP 333中定义并被许多框架实现, 规定了Web服务器与Web应用程序/框架之间推荐的标准接口, 确保Web应用程序在不同的Web服务器之间具有可移植性. WSGI协议,使Web开发者能够任意选择自己的组合, 而Web服务器和Web框架的开发者也可以把精力集中到各自的领域. 常见的WSGI容器- Gunicorn 易于配置, 兼容性好, CPU消耗少 # Work模式 1. 同步worker: 默认模式, 一次只处理一个请求 2. 异步Worker: 通过Eventlet, Gevent实现异步模式 3. 异步IO Worker: 目前支持gthread和gaiohttp两种模式 # 启动命令 # > gunicorn [OPTIONS] MODULE_NAME: VARIABLE_NAME > gunicorn --workers=3 section19.app:app -b 0.0.0.0:9000 # worker 个数: 推荐cpu个数*2 +1 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"DataAnalysis/summary.html":{"url":"DataAnalysis/summary.html","title":"概述","keywords":"","body":"数据分析数据分析 mysql excel_csv Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-05 15:54:24 "},"DataAnalysis/postgresql/postgresql_basic.html":{"url":"DataAnalysis/postgresql/postgresql_basic.html","title":"Postgresql知识","keywords":"","body":"Postgresql1. 相关2. 常用语法3. 权限管理Postgresql [TOC] 1. 相关 greenplum redshift 2. 常用语法 建表 去重和类型转换 insert into dw_app_sku select distinct on (sku_id) sku_id,seller_id,buyer_id,status,price::float8, -- 类型转换 substring(updated,1,7) as updated,substring(created,1,7) as created ,batch_id from ods_mercari_app_sku order by sku_id, batch_id desc; 权限分配 grant usage on schema aspex_mercari to data_loader; grant all on aspex_mercari.ods_mercari_app_sku to data_loader; grant select, insert on aspex_mercari.ods_mercari_app_user to data_loader; 创建语句 CREATE TABLE public.final_buyer_txn ( id serial8 NOT NULL , -- 自增id user_id text, create_date text, create_month text, create_quarter text, yearmonth text, sku int8, -- price float8, value int8, update_date text )WITH (APPENDONLY=TRUE, ORIENTATION=column, compresstype=ZLIB) DISTRIBUTED BY (user_id); 分区表 CREATE TABLE aspex_mercari.ods_mercari_app_sku ( sku_id text , seller_id text , buyer_id text , status text , price text , updated text , created text , batch_id date ) WITH (APPENDONLY=TRUE, ORIENTATION=column, compresstype=ZLIB) DISTRIBUTED BY (seller_id) PARTITION BY RANGE (batch_id) ( START (date '2019-01-01') INCLUSIVE END (date '2021-01-01') EXCLUSIVE EVERY (INTERVAL '1 day')); 计算 select seller_id,ff.create_date::text, ff.create_month, ff.create_quarter, '2019-11-01', count(*)filter(where substring(tt0.created, 1, 7) = '2019-11') as sku_new, count(*)filter(where substring(tt0.created, 1, 7) <> '2019-11' and substring(tt0.updated, 1, 7) = '2019-11') as sku_update, sum(price::int)filter(where substring(tt0.created, 1, 7) = '2019-11') as value_new, sum(price::int)filter(where substring(tt0.created, 1, 7) <> '2019-11' and substring(tt0.updated, 1, 7) = '2019-11') as value_update, num_sell_items, 0,'2019-11-19' from aspex_mercari.ods_mercari_app_sku tt0 left join aspex_mercari.ods_mercari_app_user tt1 on tt0.seller_id = tt1.user_id left join aspex_mercari.app_user_info ff on tt0.seller_id=ff.user_id where tt1.batch_id='2019-11-01' group by seller_id, num_sell_items , ff.create_date, ff.create_month, ff.create_quarter; 类型判断 set num_sell_items = decode(aaa.now_sell_items, '', '0', aaa.now_sell_items)::int - decode(bbb.now_sell_items, '', '0', bbb.now_sell_items)::int 3. 权限管理 information_schema.table_privileges表记录着所有用户的权限信息。 查看权限 修改权限 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-06 16:21:20 "},"DataAnalysis/mysql/summary.html":{"url":"DataAnalysis/mysql/summary.html","title":"Mysql知识","keywords":"","body":"主要课程1. 查询 （当前课程主要是查询）主要课程 用MySQL管理大数据 1. 查询 （当前课程主要是查询） 查询语法 其他语法 Jupyter连接数据库 Mysql优化 Tips Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-14 15:24:57 "},"DataAnalysis/mysql/query_syntax.html":{"url":"DataAnalysis/mysql/query_syntax.html","title":"查询语法","keywords":"","body":"Query Syntax1. 六个查询关键词2. 数据类型(https://support.hostgator.com/articles/specialized-help/technical/phpmyadmin/mysql-variable-types)3. 查看数据(look at the data)4.mysql shell5. 查询Query Syntax 1. 六个查询关键词 SELECT FROM WHERE GROUP HAVING ORDER 2. 数据类型(https://support.hostgator.com/articles/specialized-help/technical/phpmyadmin/mysql-variable-types) text char(0-255 固定长度) varchar(0-255 可变长度) tinytext(最长255的字符串) TEXT A string with a maximum length of 65535 characters. BLOB A string with a maximum length of 65535 characters. MEDIUMTEXT A string with a maximum length of 16777215 characters. MEDIUMBLOB A string with a maximum length of 16777215 characters. LONGTEXT A string with a maximum length of 4294967295 characters. LONGBLOB A string with a maximum length of 4294967295 characters. BLOB stands for Binary Large OBject, and can be used to store non-text information that is encoded into text. number TINYINT ( ) -128 to 127 normal 0 to 255 UNSIGNED SMALLINT( ) -32768 to 32767 normal 0 to 65535 UNSIGNED MEDIUMINT( ) -8388608 to 8388607 normal 0 to 16777215 UNSIGNED INT( ) -2147483648 to 2147483647 normal 0 to 4294967295 UNSIGNED BIGINT( ) -9223372036854775808 to 9223372036854775807 normal 0 to 18446744073709551615 UNSIGNED FLOAT A small number with a floating decimal point. DOUBLE( , ) A large number with a floating decimal point. DECIMAL(M,D) A DOUBLE stored as a string, allowing for a fixed decimal point. M表示整个长度，D表示小数点后的位数 datetime DATE YYYY-MM-DD DATETIME YYYY-MM-DD HH:MM:SS TIMESTAMP YYYYMMDDHHMMSS TIME HH:MM:SS YEAR YYYY 3. 查看数据(look at the data) 查看数据库 USE db_name SHOW tables 查看表 SHOW columns FROM table_name SHOW columns FROM table_name FROM db_name SHOW columns FROM db_name.table_name DESCRIBE columns FROM table_name DESCRIBE columns FROM table_name FROM db_name DESCRIBE columns FROM db_name.table_name KEY的类型 empty 表示这列既不是索引，也不会作为多列索引的辅助索引 PRI 表示该列是PRIMARY KEY，或者是多列PRIMARY KEY中的一列 UNI 表示该列是唯一索引的首列 MUL 表示该列是非唯一索引的首列 4.mysql shell 查询执行任务 show full processlist; select * from information_schema.innodb_trx\\G 5. 查询 截取字符串 where substring(cs_name, 4) == 'kat'; # 从第4个开始 where substring(cs_name, -4) == 'kat'; # 从倒数第4个开始 where substring(cs_name, 4, 2) == 'kat'; # 从第4个开始，截取两个 where right(str, length); # 右截取 where left(str, length); # 左截取 连接字符 CONCATE(column1, column2, '--'); # 连接 字符长度 length(cs_name) 正则 where name like '%A%' join inner join left join right join full join limit limit 5 返回五条 limit 10, 50 返回符合结果的11-60条数据 排序 @row_number变量 set @row_number=0; insert into talabat.tmp_count select null, cc.city_name, cc.area_name, cc.cnt, @row_number:=@row_number+1, 'hungerstation' from ( select aa.area_name, aa.city_name, bb.* from hungerstation.areas aa left join (select city_id, area_id, count(*) as cnt from hungerstation.restaurants_by_area group by city_id, area_id) bb on aa.area_id=bb.area_id and aa.city_id=bb.city_id ) cc order by cc.cnt desc; datetime的计算 -- MySQL 为日期增加一个时间间隔：date_add() -- now() //now函数为获取当前时间 select date_add(now(), interval 1 day); - 加1天 select date_add(now(), interval 1 hour); -加1小时 select date_add(now(), interval 1 minute); - 加1分钟 select date_add(now(), interval 1 second); -加1秒 select date_add(now(), interval 1 microsecond);-加1毫秒 select date_add(now(), interval 1 week);-加1周 select date_add(now(), interval 1 month);-加1月 select date_add(now(), interval 1 quarter);-加1季 select date_add(now(), interval 1 year);-加1年 MySQL adddate(), addtime()函数，可以用date_add() 来替代。 2. MySQL 为日期减去一个时间间隔：date_sub() MySQL date_sub() 日期时间函数 和date_add() 用法一致。 MySQL 中subdate(),subtime()函数，建议，用date_sub()来替代。 9. Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-03-04 17:39:28 "},"DataAnalysis/mysql/others_syntax.html":{"url":"DataAnalysis/mysql/others_syntax.html","title":"其他语法","keywords":"","body":"Other Syntax 其他语法1. 建表2. 插入数据3. 更新数据4. 删除数据5. 索引6. 修改表结构Other Syntax 其他语法 1. 建表 ``` ``` 建表编码使用utf8mb4, 祥见Tips 2. 插入数据 3. 更新数据 4. 删除数据 5. 索引 6. 修改表结构 修改编码 alter database datebase_name character set utf8mb4; alter table table_name character set utf8mb4; Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-14 15:24:58 "},"DataAnalysis/mysql/jupyter_mysql.html":{"url":"DataAnalysis/mysql/jupyter_mysql.html","title":"Jupyter连接数据库","keywords":"","body":"Jupyter MysqlJupyter Mysql 加载mysql库 # 加载sql库 %load_ext sql # 建立连接 %sql mysql://user:pw@mysqlserver/db_name # 选择查询的数据库 %sql USE db # 每一次操作都要如下开头 %sql ... Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-06-25 13:55:55 "},"DataAnalysis/mysql/mysql_optimization.html":{"url":"DataAnalysis/mysql/mysql_optimization.html","title":"Mysql优化","keywords":"","body":"mysql_optimizationmysql_optimization 查询时优先考虑降低查询结果的大小 比如：group_by > join select 出需要的信息减少表容量 查看后台进程 show full processlist; select * from information_schema.innodb_trx\\G Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-30 15:58:52 "},"DataAnalysis/mysql/tips.html":{"url":"DataAnalysis/mysql/tips.html","title":"Tips","keywords":"","body":"Tips 技巧总结Tips 技巧总结 不是会连接mysql，会增删改查语句就够了，因为就像所有的工具，MYSQL也有很多注意事项，需要明确注意，才能避免在运行一段时间后由于错误应用而损失 增删改查SQL语句中的转义 utf8和utf8mb4 缘由: 处理小红书的数据，有的笔记中带有emoji表情，使用utf8保存会报错 解决办法: 修改数据库的编码为utf8mb4 注意数据库和表的字符集，在mysql和MariaDB中都要使用utf8mb4，而不是utf8，详见mysql中永远不要使用utf8 建表时要注意编码，连接数据库操作时也要指定编码，才能解决 conn = pymysql.connect(host=host, port=port, user=user, password=password, database=database, charset=\"utf8mb4\" ) 数据库同步策略 带时间戳的数据, 只有insert操作 按时间戳同步 不带时间戳的 全表update的, 全部同步 增量更新的, 按id查询后添加 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-11-18 16:31:24 "},"DataAnalysis/mysql/docker_mysql.html":{"url":"DataAnalysis/mysql/docker_mysql.html","title":"docker_mysql","keywords":"","body":"docker mysqldocker mysql 启动 docker create -p 3306:3306 --name docker_mysql -v /my/own/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=password mysql # 环境变量说明 -v /my/custom:/etc/mysql/conf.d # conf.d 是配置文件, 位于custom, 注意只需要路径 创建mysql dump docker exec some-mysql sh -c 'exec mysqldump --all-databases -uroot -p\"$MYSQL_ROOT_PASSWORD\"' > /some/path/on/your/host/all-databases.sql 导入mysql dump docker exec -i some-mysql sh -c 'exec mysql -uroot -p\"$MYSQL_ROOT_PASSWORD\"' Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-03 12:25:42 "},"Database/mongodb/install.html":{"url":"Database/mongodb/install.html","title":"Mongodb","keywords":"","body":"安装yum安装运行配置卸载使用Robo 3T 通过ssh连接mongodb安装 参考: https://docs.mongodb.com/manual/tutorial/install-mongodb-on-amazon/可以选择多平台的安装, 本次安装环境: Amazon linux yum安装 $ sudo nano /etc/yum.repos.d/mongodb-org-4.0.repo paste: [mongodb-org-4.0] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/amazon/2013.03/mongodb-org/4.0/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc $ sudo yum install -y mongodb-org 运行 # 启动服务 sudo service mongod start systemctl start mongod sudo tail /var/log/mongodb/mongod.log # 验证是否运行 # 将在系统重启后自启 sudo chkconfig mongod on sudo systemctl ennable mongod sudo service mongod stop # 停止服务 sudo service mongod restart # 重启服务 kill -15 mongo_id # 使用-15杀mongod进程避免锁死 配置 yum安装后的mongdbmongod.conf 的位置: /etc/ $ sudo nano /etc/mongod.conf /var/lib/mongod # 默认存储位置 /var/log/mongodb/mongod.log # 默认log位置 port: 27017 bindIp: 127.0.0.1 → 0.0.0.0 # 默认只允许本地访问, 修改后对互联网开放 卸载 关闭mongodsudo service mongod stop 删除安装包sudo yum erase $(rpm -qa | grep mongodb-org) 删除数据文件sudo rm -r /var/log/mongodb sudo rm -r /var/lib/mongo 使用Robo 3T 通过ssh连接mongodb AWS安全组设置, 添加入站端口 27017 Robo 3T添加链接, 使用ssh验证 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Database/mongodb/security_deployment.html":{"url":"Database/mongodb/security_deployment.html","title":"安全部署","keywords":"","body":"安全设置创建管理员用户配置启动安全认证添加数据库用户pymongo连接安全设置 对于mongodb的安全部署, 可以参考一篇文章: 安全部署MongoDB最佳实践 创建管理员用户 $ mongo # 进入mongodb shell > use admin # 进入admin 数据库 > db.createUser({user:\"maxiaoteng\",pwd:\"******\",roles:[{role:\"root\",db:\"admin\"}]}) # admin是用户的身份验证数据库, 用户在当前数据库进行验证, 但也可以在其他数据库有角色 > exit Successfully added user: { \"user\" : \"maxiaoteng\", \"roles\" : [ { \"role\" : \"root\", \"db\" : \"admin\" } ] } 配置启动安全认证 sudo nano /etc/mongod.conf # 更改默认端口为27010 port : 27010 # 生效认证 # 2.6 3.2 3.6 以上版本用法 security: authorization: enabled # 重启生效 sudo service mongod restart 验证 mongo --port=27010 # 进入mongo shell show dbs # 显示结果 2018-09-11T08:27:32.759+0000 E QUERY [js] Error: listDatabases failed:{ \"ok\" : 0, \"errmsg\" : \"command listDatabases requires authentication\", \"code\" : 13, \"codeName\" : \"Unauthorized\" } : # 说明登陆失败, db.auth(\"username\",\"password\") # 返回1 说明登陆成功 **注意: 登陆认证某个数据库时, 要先`use db`, 然后再执行auth** use admin db.auth(\"username\",\"password\") 添加数据库用户 用户可以访问特定数据库 # mongo shell下 > use ifood > db.createUser({user: \"testdb1u1\", pwd: \"xyz123\", roles: [{ role: \"dbOwner\", db: \"testdb1\" }]}) ###显示: Successfully added user: { \"user\" : \"yunfutech\", \"roles\" : [ { \"role\" : \"dbOwner\", \"db\" : \"ifood\" } ] } pymongo连接 见pymongo Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Database/mongodb/mongodb_pymongo.html":{"url":"Database/mongodb/mongodb_pymongo.html","title":"pymongo","keywords":"","body":"数据库 pymongo1. 数据库操作1. 连接2. 查看数据库信息2. 集合操作1. 获取集合2. 删除集合3. 统计3. 文档操作1. 插入文档2. 修改3. 查询文档4. 删除4. 创建索引同步操作mongodb中的正则表达式mongodb中的范围查询心得值的比较数据库 pymongo 1. 数据库操作 1. 连接 默认连接 dbconfig = { 'host': MONGODB_SERVER, \"port\": MONGODB_PORT, 'database': MONGODB_DATABASE, } class Mongodb(object): # 初始化 def __init__(self, dbconfig): self.host = dbconfig.get('host') self.port = dbconfig.get('port') self.database = dbconfig.get('database') self._client = pymongo.MongoClient(self.host, self.port) self._db = self._client[self.database] # 退出 def __exit__(self): self._client.close() 用户名密码连接 方法1 client = pymongo.MongoClient(host, port) db = client.mydb db.authenticate(user, password) 方法2 uri = \"mongodb://{username}:{password}@{host}:{port}/{db_name}?authMechanism=MONGODB-CR\".format(username=user_name,password=user_pwd,host=host,port=port,db_name=db_name) mongo_client = pymongo.MongoClient(uri) mongo_db = mongo_client[db_name] mongo_coll = mongo_db[coll_name] 密钥连接 2. 查看数据库信息 查看数据库的文档 db.collection_names(include_system_collections=False) 2. 集合操作 1. 获取集合 def get_collection(self, name): collection = self.db[name] # collection = self.db.name # 获取集合时, 如果name == 'test' # 也可写成: collection = self.db.test 2. 删除集合 name = 'test' db.drop_collection(name) // 或者 collection = db[name] collection.drop() # 成功返回true 3. 统计 num = collection.count_documents() # 集合的count() 方法返回集合中文档的个数 num = collection.find(query),count() # 和上面的速度是一样快的 num = collection.update_many(...).modified_count() # 查看更新操作修改了多少行 num = collection.deleted_many(...).deleted_count() # 查看操作删除了多少行 num = collection.insert_many(...).inserted_count() # 查看插入了多少行 3. 文档操作 1. 插入文档 插入一条 item = { 'name': 'xxx', 'age': 22 } insert_id = collection.insert_one(item).inserted_id // 返回InsertOneResult实例, 就是item增加了inserted_id, 最终返回: ObjectId('...') 批量插入 items = [ {...}, {...}, ... ] result = collection.insert_many(items) // result 是一个InsertManyResult实例，支持迭代 # 如果插入时数据库有唯一约束，可以添加ordered为无序，使用try except来忽略重复插入的异常，其他的就可以继续插入 result = collection.insert_many(items, ordered=False) 批量不重复插入 for r in rs: query = { \"unique_colum\": r.get('unique_colum'), } update = { \"$set\": r, } rs = collection.find_one_and_update(query, update, upsert=True) 2. 修改 参考： https://blog.csdn.net/user_longling/article/details/52398667 update共有四个参数： - query # 条件 - updata # 更新内容 - upsert # 默认false，如果不存在要插入，则为true - multi # 默认false，要修改所有符合条件的查询，则为true 基本操作， updata是一个完整的值，将整个document修改 不存在插入，存在则不操作 \"$setOnInsert\" 修改某个key，有则修改，没有就新增 \"$set\" 修改符合条件的第一条 query = { \"name\": \"xxx\" } newvalue = { \"$set\": { \"name\": \"sss\" } } collection.update_one(query, newvalue) # 修改单条 result = collection.update_many(query, newvalue) # 修改多条 result.modified_count() # 返回修改内容的数量 删除某个键 \"$unset\" 增加计算 \"$inc\" newvalue = { \"$inc\": { \"key\" : 3, # 或者-7 } } \"$push\" # field的值必须是list updata = { \"$push\": { \"field\": \"value\", } } \"pushAll\" # 将list的每个值push进去，和push区别 pushall接受list，将每个值放到目标数组中 如果push一个list，list将作为整体加入到目标数组中 原始：['aa', 'bb'] push ['cc', 'dd'] # 结果 ['aa', 'bb'，['cc', 'dd']] pushall ['cc', 'dd'] # 结果 ['aa', 'bb'，'cc', 'dd'] 3. 查询文档 查询第一条 query = { 'kk': 'vv', ... } collection.find_one(query) // 返回一个dict, 没有符合条件, 返回None 查询多条 result = collection.find(query) #　**注意:** 此处返回result类型``是可迭代对象,可以使用list(result)转换, 也可以使用for循环迭代 num = result.count() # Cursor有一个count方法, 返回查询结果的个数 范围查询 参考： https://blog.csdn.net/yangguangxiadeshu/article/details/45096007 query = { \"date\": { # 范围查询 \"$gt\": d, # 大于 \"$lt\": d, # 小于 \"$gte\": d, # 大于等于 \"$lte\": d, # 小于等于 \"$ne\": d, # 不等于 }, \"value1\": { # 并列查询 \"$lt\": 20, # 小于 \"$gte\": 3, # 大于等于 }, \"value2\": { # in or not in \"$in\": [2, 3], \"$nin\": [2, 3], }, \"value3\": { # 取模运算 \"$mod\": [10, 1], # 等价于 value3 % 10 == 1 }, \"value4\": { # list大小 \"$size\": 5, # value4这个list的大小，官网不建议查找size的范围，如果想要找size 随机查询 query = [{ '$sample': { \"size\": 2, }, }] rs = list(collection.aggregate(pipeline=query)) 过滤显示 filter = { 'id': 0, # 0表示不显示, 1表示显示, 0和1只能存在一种 'url': 0, } result = collection.find({}, filter) 4. 删除 删除一条 query = {\"name\": \"maxiaoteng\"} collection.delete_one(query) # 删除符合条件的第一条 删除多条 query = { \"name\": { \"$regex\": \"^F\" # 使用正则表达式:name中以F开头的 } } x = collection.delete_many(query) count = x.deleted_count # 返回删除文档的个数 删除所有 query = {} # 即可 4. 创建索引 添加索引可以加快查询速度 使用mongoShell操作，使用3.0之后版本 # 创建索引，1表示升序，-1表示降序 # option # 1. 创建索引 db.COLLECTION_NAME.createIndex({\"name\":1}) # 2. 组合索引 db.COLLECTION_NAME.createIndex({\"name\":1, \"age\": -1}) # 3. 后台创建索引 db.COLLECTION_NAME.createIndex({\"name\":1, \"age\": -1}, {\"background\": 1}) # 4. 查看索引 db.COLLECTION_NAME.getIndexes() db.COLLECTION_NAME.getIndexkeys() # 5. 删除索引,根据name db.COLLECTION_NAME.dropIndex(\"name_1\") db.COLLECTION_NAME.dropIndex(\"name_1_age_1\") db.COLLECTION_NAME.dropIndexes() # 删除所有索引 # 6. 创建唯一索引 db.COLLECTION_NAME.createIndex({\"name\":1, \"age\": -1}, {unique: true}) # 7. 修改索引 db.COLLECTION_NAME.reIndex({\"name\":1, \"age\": -1}) # 将数据插入带有唯一约束的表中，在insert_many中设置ordered=false # pymongo创建 result = collection.create_index([('user_id', pymongo.ASCENDING)], unique=True) // user_id的值为递增, 且唯一, 之后再插入, 则抛出错误 同步操作 find_one_and_update find_one_and_replace find_one_and_delete mongodb中的正则表达式 query = { 'name': { \"$regex\": \"^f\", # 以f开头的 \"$regex\": \"f\", # 包含f的 } } mongodb中的范围查询 query = { 'age': { \"$gt\": 25, # 大于25岁 } } 心得 请使用最新的, 且规范的方法来操作. 比如: 统计使用collection.find().count(), 增删改查使用xx_one 和 xx_many, 模糊的update, insert等已经被弃用. 查询并更新使用find_one_and_update(), delete, replace... 还可以根据参数, 选择返回的是修改前还是修改后的内容 避免重复插入的方法: 一个一个插入, 每次插入时使用find_one_and_update(query, update, upsert=True).(即不存在则插入, 存在则更新) 值的比较 比较 MongoDB pymongo 1 true True 2 false False 3 null/不存在 None Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-04-01 15:08:43 "},"Database/mongodb/mongo_shell.html":{"url":"Database/mongodb/mongo_shell.html","title":"mongo shell","keywords":"","body":"Mongo ShellMongo Shell 进入shell mongo # 直接进入, 默认端口27017的数据库 mongo --port=27010 # 指定端口的数据库 mongo host:port/admin -u root -p password 基本操作 查看版本 db.version() 登陆 use db_name # 登陆admin, use admin db.auth(\"username\",\"password\") 查看操作命令 db.help() 查看数据库列表 show dbs 切换或创建数据库 use db_name 显示所有用户 show users 显示当前数据库的集合 show collections 显示集合操作命令 db.yourCollection.help() # 比如 db.readme.help() 显示当前所用的数据库 > db 其他操作 db.collection.find() db.collection.find().count 退出 > exit bye 数据导出导入 # 导出csv mongoexport --host xx --port xx -u root -p xx --authenticationDatabase admin --db HotelRoomCounts --collection CtripRoomCounts --csv --out CtripRoomCounts.csv --fields 'hotelId,address,city,lat,lng,name,room_counts' # 导出其他文件 mongoexport --host xx --port xx -u root -p xx --authenticationDatabase admin --db HotelRoomCounts --collection CtripRoomCounts -o co.dat # 导入 mongoimport --host xx --port xx -u root -p xx --authenticationDatabase admin --db HotelRoomCounts --collection CtripRoomCounts --upsert file_name # 发现其实这种导入有些问题，推荐使用python手动导入，起因是我通过导出添加唯一索引再导入去重，能够将满足要求的导入，其他的数据会停在外面，进程也不结束。 # 而且Python导入也更快一些， 4w条insert_many Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-05 15:27:48 "},"Database/mongodb/sharding.html":{"url":"Database/mongodb/sharding.html","title":"分片","keywords":"","body":"ShardingSharding 当单表数据量较大（几十GB）或读写吞吐量大的时候，必须进行分片(Sharding)操作。MongoDB分片需要手动指定片键。好的片键需要尽量平滑，否则会出现数据在多个节点中倾斜，导致集群性能下降。直接用 _id作为片键的话，因为 _id是内含时间戳的，所以不是按次递增的，多个帖子都不建议以 _id作为片键。然而，mongodb在3.0(?)版里新增了哈希索引功能，哈希后的 _id是平均的，所以多数情况下，用hashed_id作为片键是好的选择。 下面的sharding流程最好在表为空的时候进行，否则有可能很慢： ```shell # 1.选择数据库 use kaola # 2.打开该数据库分片功能 sh.enableSharding(\"kaola\") # 3.片键需要指定一个索引，所以需要创建hashed_id索引。这行意思是以_id字段，在背景创建一个名叫_id_hashed的哈希值的索引。这一步根据该表数据量而定，有可能需要数小时 db.getCollection(\"kaola_page_detail\").createIndex( { _id: \"hashed\" }, { name: \"_id_hashed\", background: true } ) # 4.指定该索引为片键 sh.shardCollection( \"kaola.kaola_page_detail\", { _id: \"hashed\" } ) # 5.以上命令运行成功后，可以用这个命令来check现在分片的状态 sh.status() ``` Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-07-03 14:46:42 "},"Database/Redis/install.html":{"url":"Database/Redis/install.html","title":"Redis","keywords":"","body":"1. Redis简介2. 安装3. 配置Redis4. 使用服务方式运行redis5. 使用service来管理redis-server6. 打开系统配置文件, 修复redis内存低的问题7. 测试redis服务器是否启动正常8. 客户端连接Redis9. 可视化RedisDesktopManager1. Redis简介 Redis是一个基于内存的键值对存储系统，常用作数据库、缓存和消息代理 。它支持字符串、字典、列表、集合、有序集合、位图（Bitmaps）、地理位置等多种数据结构，所以常常被称为数据结构服务器。Redis支持事务、分片、主从复制，支持RDB（将内存中的数据保存在文件中）和AOF（类似于MySQL的binlog）两种持久化方式，还支持订阅分发、Lua 脚本、集群等特性。 数据存放在内存中，访问速度快 支持丰富的数据结构：string,list, set, map... 原子性操作，单线程，安全，支持事务(session) 可以设置过期时间 相对于Memcached的优势 redis可以持久化数据 Web应用中常需要将一些重要数据持久化到硬盘，避免宕机等原因导致数据丢失。Redis会周期性把更新的数据写入磁盘或者追加到命令日志中，并且在此基础上实现了主从同步。而Memcached在进程关闭之后数据就会丢失 memcached支持数据类型简单 一些业务为了简化工作，需要使用列表、集合这样只有Redis才支持的数据结构。相对于Memcahced，Redis有更多的应用场景 Redis速度快，并提供了丰富的命令 Redis应用场景 取Top N 操作 实时统计 计数器 显示最新的项目列表 秒杀活动 2. 安装 ubuntu安装可以使用apt install redis-server一键安装 以下方法用于在aws ec2 上安装 参考教程: $ sudo yum -y update $ sudo yum -y install gcc make # 安装gcc make用来编译 # 下载文件 cd /usr/local/src sudo wget http://download.redis.io/releases/redis-4.0.11.tar.gz sudo tar -xvzf redis-4.0.11.tar.gz cd redis-stable make test # 可选 # 编译安装 make # 安装到环境变量 sudo make install # 等价于: sudo cp src/redis-server /usr/local/bin/ sudo cp src/redis-cli /usr/local/bin/ # 安装tcl sudo yum install -y tcl 3. 配置Redis 参考 http://www.runoob.com/redis/redis-conf.html 初始配置文件位于解压目录 .../redis-stable/redis.conf cd ./redis-stable/redis.conf sudo nano redis.conf bind 127.0.0.1 # 绑定本地ip, 要想公网访问,注释掉 requirepass xxx # 添加链接密码, 在允许公网访问之前, 必须设置密码 protected-mode no # 禁用保护模式, 这个是在无密码时禁止公网访问使用的 daemonize yes # 后台运行 logfile \"/var/log/redis_6379.log\" pidfile /var/run/redis.pid dir /var/redis/6379 移动配置文件: cp redis.conf /etc 4. 使用服务方式运行redis 编辑配置脚本 vim /etc/systemd/system/redis.service # 注意执行命令和配置文件的路径 [unit] Description=The redis-server Process Manager After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/var/run/redis.pid ExecStart=/usr/local/bin/redis-server /etc/redis.conf ExecReload=/bin/kill -s HUP $MAINPID ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target 重载systemctlsystemctl daemon-reload 启动重启等操作systemctl start/stop/restart/status redis redis.server修改后重启systemctl enable redis.service 5. 使用service来管理redis-server 下载init脚本 sudo wget https://raw.githubusercontent.com/saxenap/install-redis-amazon-linux-centos/master/redis-server sudo mv redis-server /etc/init.d sudo chmod 755 /etc/init.d/redis-server /etc/init/ 和 /etc/init.d 两个文件里的脚本, 是可以使用service mmm start 来启动某项服务的 修改redis_server的配置文件 sudo nano /etc/init.d/redis-server # 编辑文件以匹配配置文件 REDIS_CONF_FILE =“/etc/redis/6379.conf” 设置redis服务器自启动 sudo chkconfig --add redis-server sudo chkconfig --level 345 redis-server on sudo service redis-server start 6. 打开系统配置文件, 修复redis内存低的问题 sudo nano /etc/sysctl.conf #confure redis background save issue vm.overcommit_memory = 1 systctl vm.overcommit_memory = 1 7. 测试redis服务器是否启动正常 redis-cli ping # 响应为 PONG 8. 客户端连接Redis redis-cli # 启动redis客户端 连接到本地redis 服务器redis 127.0.0.1:6379> # 连接到本地的redis服务器 连接到远程redis服务器 redis-cli -h host -p port -a password ``` $redis-cli -h 127.0.0.1 -p 6379 -a \"mypass\" redis 127.0.0.1:6379> redis 127.0.0.1:6379> PING PONG ``` 9. 可视化RedisDesktopManager 使用ssh连接 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:03:53 "},"Database/Redis/python_redis.html":{"url":"Database/Redis/python_redis.html","title":"Python连接Redis","keywords":"","body":"1. Redis命令参考1. Python连接Redis1. redis连接2. 连接池3. 创建Redis对象时，支持一些参数2. redis 数据操作1. redis key操作2. redis String 操作3. redis list 操作4. redis set 操作5. redis Sorted Set 操作6. redis Hash(dict) 操作1. Redis命令参考 官网提供的可搜索的命令 翻译中文文档 1. Python连接Redis 源代码 1. redis连接 redis提供两个类Redis和StrictRedis, StrictRedis用来实现大部分官方的命令,并使用官方的语法和命令,Redis是StrictRedis的子类,用于向后兼容旧版本的redis-py. redis连接实例是线程安全的,可以直接将redis设置为全局变量,直接使用. 安装redis依赖 sudo pip install redis 连接redis时,加上decode_response=True,写入的键值对中的value为str类型,不加这个参数写入的则为字节类型. import redis r = redis.StrictRedis(host='localhost', port=6379, password='xxx', decode_responses=True, db=0) # host是redis主机，需要redis服务端和客户端都启动 redis默认端口是6379, 密码验证 r.set('name', 'junxi') # key是\"foo\" value是\"bar\" 将键值对存入redis缓存 print(r['name']) print(r.get('name')) # 取出键name对应的值 print(type(r.get('name'))) 2. 连接池 默认,每个Redis实例都会维护一个自己的连接池.可以直接建立一个连接池,实现多个Redis实例共享一个连接池 import redis # 导入redis模块，通过python操作redis 也可以直接在redis主机的服务端操作缓存数据库 pool = redis.ConnectionPool(host='localhost', port=6379, password='xxx', decode_responses=True) # host是redis主机，需要redis服务端和客户端都起着 redis默认端口是6379 r = redis.StrictRedis(connection_pool=pool) r.set('gender', 'male') # key是\"gender\" value是\"male\" 将键值对存入redis缓存 print(r.get('gender')) # gender 取出键male对应的值 3. 创建Redis对象时，支持一些参数 host port password db # 选择存到第几个数据库 socket_timeout=None # 超时时间 connection_pool=None # 可以直接使用连接池 charset='utf-8' # redis入库默认utf-8，如果需要编码，结果第八项使用 decode_responses=False # 修改编码，这一项为True，(charset='GBK ', decode_responses=True) errors='strict' unix_socket_path=None 2. redis 数据操作 参考教程：https://www.w3cschool.cn/redis_all_about/redis_all_about-jrvk26ug.html 1. redis key操作 列出所有key 1. r.keys() 2. keys * # redis-cli 判断某个key是否存在 1. r.exists('key') 2. exists key # redis-cli 删除指定的key 1. r.delete('key') 2. del key1 key2 ... # redis-cli 3. redis批量删除（通配符) r.delete(*r.keys('/vender*')) 返回指定key的value类型 1. r.type('key') 2. type key # redis-cli (none不存在，string，list，set) 随机取一个key 1. r.randomkey() 2. randomkey # redis-cli 重命名一个key 1. r.rename('key1', 'key2' ), r.renamenx('key1', 'key2') 2. rename oldkey newkey | renamenx oldkey newkey # redis-cli 区别，前面的如果newkey存在，则会被覆盖，后面的则会返回错误 key的超时设置 1. redis-python # 设置过期时间 r.expire('key1', 10 ), # 设置10s后过期 extime = datetime.datetime(2018, 7, 25, 15, 19, 10) extime.strftime('%Y-%m-%d %H:%M:%S %f') r.expireat('key2', extime) # 可指定datetime和时间戳 # 查看剩余过期时间(秒) r.ttl('key1') r.pttl('key1') # 毫秒 2. redis-cli - expire key seconds 单位是秒。返回1成功，0表示key已经设置过过期时间或者不存在。如果想消除超时则使用persist key。如果希望采用绝对超时，则使用expireat命令。 - ttl key 返回设置过过期时间的key的剩余过期秒数 -1表示没有设置过过期时间，对于不存在的key,返回-2。 - pexpire key mseconds 设置生命周期。 - pttl key 以毫秒返回生命周期。 2. redis String 操作 设值 set 单个值r.set(name, value, ex=None, px=None, nx=False, xx=False) Redis中设置值,默认为不存在则创建, 存在则修改 # ex 过期时间(s) # px 过期时间(ms) # nx 如果为True, name不存在时set才操作(set只支持新增) # xx 如果设置为True, name存在时, set才操作(set只支持修改) mset 批量添加# 支持两种方式 mset(*args, **kwargs) r.mset(k1=\"v1\", k2=\"v2\") # 这里k1 k2不能带引号,一次设置多个键值对 r.mset({\"k1:\"v1\", \"k2\":\"v2\"}) # 这里k1 k2不能带引号,一次设置多个键值对 取值 get 获取单个值 r.get('useful_proxy') mget 批量获取 # 同样支持两种 mget(keys, *args) r.mget(\"k1\", 'k2') r.mget([\"k1\", \"k2\"]) 读取后重设 getset(name, value) # 设置新值,并获取旧值r.getset('food', 'barbecue') 增减操作，如果value不是int，则错误 r.incr('key') # 加1 不存在默认为0 r.decr('key') # 减1 r.incrby('key', 5) # r.decrby('key', 4) # 也可以加一个负数实现 r.incrbyfloat('key', 5.1) # 增减浮点数 追加，截取和改写 字符串 r.set(\"name\", \"strangename\") r.append('name', 'afterfix') # 追加：结果为strangenameafterfix r.getrange(\"name\", 0, 3) # 改写：结果为:stra r.substr('name', 2, 4) # 截取，不修改值，结果为：ran 获取字符串长度 r.strlen('key') 3. redis list 操作 添加列表 r.lpush('list', '1') # 左push 右pop 江湖规矩 r.lpushx('list', '1') # 如果存在才push r.linsert(name, where, refvalue, value) # 特定位置插入（after，before） 查看长度 r.llen('list') 查看元素 r.lindex('list', 0) # 返回特定位置的元素 查看一段列表 r.lrange('list', 0, -1) # 返回一段列表，这个返回全部列表 截取列表 N = r.ltrim('list', '0','2') # 截取一段列表，改写，返回删掉的元素个数 删除元素 r.lrem('list', count, value) # 删除value的值，count为删除的个数，正数从左删除，负数从右删除 r.lpop('list') # 删除左侧第一个元素 r.rpop('list') # 指定某个下标的值 r.lset('list', 0, '5') # 返回Ture 阻塞队列 blpop key1...keyN timeout # 从左到右扫描返回对第一个非空list进行lpop操作并返回，比如blpop list1 list2 list3 0 ,如果list不存在list2,list3都是非空则对list2做lpop并返回从list2中删除的元素。如果所有的list都是空或不存在，则会阻塞timeout秒，timeout为0表示一直阻塞。当阻塞时，如果有client对key1...keyN中的任意key进行push操作，则第一在这个key上被阻塞的client会立即返回（返回键和值）。如果超时发生，则返回nil。有点像unix的select或者poll。 4. redis set 操作 所有操作 # 1. 添加元素 r.sadd('set', '1s') # 成功返回1， 已存在返回0，key不对应set则返回错误 # 2. 移除元素 r.srem('set', '1s') # 成功返回1， 不存在返回0，key不对应set则返回错误 # 3. 删除并返回元素 r.spop('set') # 不存在报错 # 4. 随机取一个元素, 但不删除 r.srandmember('set') # 不存在报错 # 5. 集合间移动元素 r.smove('set1', 'set2', 'member') # 6. 查看集合大小 r.scard('set') # 7. 判断是否在集合中 r.sismember('set', '2s') # 8. 求交集 r.sinter('set', 'set2') r.sinterstore('new-set', 'set', 'set2') # 将交集保存在新的集合中 # 9. 求并集 r.sunion('set', 'set2') r.sunionstore('new-set', 'set', 'set2') # 将并保存在新的集合中 # 10. 求差集 r.sdiff('set', 'set2') r.sdiffstore('new-set', 'set', 'set2') # 将差集保存在新的集合中 # 11. 查看所有元素 r.smembers('new-set') 5. redis Sorted Set 操作 6. redis Hash(dict) 操作 所有操作 # 1. 设置hash值 r.hset('hash', 'key1', 'value1') # 成功返回1， field已存在则修改，key不对应hash则返回错误 r.hsetnx('hash', 'key1', 'value1') # nx 是no exist，如果field存在，返回0，不修改 r.hmset('hash', 'key1', 'value1', 'key2', 'value2') # 批量设置多个值 # 2. 获取hash值 r.hget('hash', 'key1') # r.hmget('hash', 'key1', 'key2') # 批量获取多个值 # 3. 递增某个域的值 r.hincrby('hash', 'key1', 2) # 不是int抛出错误 # 4. 判断某个区域是否存在 r.hexists('hash', 'key1') # # 5. 删除 r.hdel('hash', 'key1') # 6. 查看域的大小 r.hlen('hash') # 7. 获取所有域名 r.hkeys('hash') # 8. 获取所有域的值 r.hvals('hash') # 9. 获取整个dict r.hgetall('hash') Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-09 11:10:21 "},"Database/Redis/macos_redis.html":{"url":"Database/Redis/macos_redis.html","title":"Macos安装Redis","keywords":"","body":"macos redismacos redis 安装 brew install redis 启动 # 推荐 brew services start redis # 手动，不推荐 redis-server /etc/redis.conf 配置 见/etc/redis.conf 自启动和后台运行 brew安装后提示： To have launchd start redis now and restart at login: brew services start redis Or, if you don't want/need a background service you can just run: redis-server /usr/local/etc/redis.conf 常用命令 开机启动redis命令 ln -sfv /usr/local/opt/redis/*.plist ~/Library/LaunchAgents 使用launchctl启动redis server launchctl load ~/Library/LaunchAgents/homebrew.mxcl.redis.plist 使用配置文件启动redis server redis-server /usr/local/etc/redis.conf 停止redis server的自启动 launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.redis.plist redis 配置文件的位置 /usr/local/etc/redis.conf 卸载redis和它的文件 brew uninstall redis rm ~/Library/LaunchAgents/homebrew.mxcl.redis.plist 测试redis server是否启动 redis-cli ping Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 13:58:57 "},"Effective_Python/summary.html":{"url":"Effective_Python/summary.html","title":"Summary","keywords":"","body":"Effective Python 编写高质量Python代码的59个有效方法1. 用Pythonic方式来思考Effective Python 编写高质量Python代码的59个有效方法 1. 用Pythonic方式来思考 确认自己使用的Python版本 遵循PEP8风格指南 了解bytes、str与unicode的区别 用辅助函数来取代复杂的表达式 了解切割序列的办法 列表推导式 生成器表达式 尽量用enumerate代替range zip平行遍历多个迭代器 for 和while之后不要接else 合理使用try/except/else/finally Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-04-30 11:12:58 "},"Configurations/pycharm.html":{"url":"Configurations/pycharm.html","title":"pycharm 设置","keywords":"","body":"安装设置安装 官网安装安装位置: pycharm安装位置:C:\\Program Files\\JetBrains\\PyCharm Community Edition 2017.3.3 设置 设置默认解释器: default setting -> project interpreter -> C:\\ProgramData\\Anaconda3\\python.exe 设置Tab 为4个空格 显示空格 设置换行符默认为\\n 设置文件头信息 File -> Other Setting -> Default Setting -> Editor -> File and Code Templates -> Includes -> File Header 快捷键设置 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/gitbook.html":{"url":"Configurations/gitbook.html","title":"Gitbook配置","keywords":"","body":"Gitbook的坑原因1. 目前已解决, 使用github登陆2. gitbook 安装和开始3. gitbook 对多级目录的支持4. 部署gitbook5. gitbook配置文件6. 启动两个gitbook服务7. gitbook插件1. 使用2. 插件列表Gitbook的坑 原因 本来使用github登陆, 但是由于hotmail把gitbook的验证邮件放到了垃圾箱, 好像是自动将邮箱注册了一个新账号, 再使用新账号关联github时便不能绑定. 1. 目前已解决, 使用github登陆 每次建立新的space和project以后 点击左下角的更多, 选择integrations,在选择github即可. gitbook自动同步github上的内容 2. gitbook 安装和开始 教程 3. gitbook 对多级目录的支持 说明 gitbook通过根目录下的SUMMARY.md来管理目录结构, 最多三级目录 4. 部署gitbook 将gitbook配置在自己的服务器 安装nodejs和gitbook-cli # 安装依赖 sudo yum install epel-release # 安装nodejs sudo yum install nodejs # 验证 node -v # 安装gitbook sudo npm install gitbook -g 导入一本书 # 将在该目录下创建一本书, 包含两个文件README.md和SUMMARY.md两个文件 gitbook init ./directory_name ## 导入书籍可以直接clone到指定目录下 启动它 # 在书籍的根目录下, 即有readme的目录下 gitbook serve # 结束后将运行来localhost:4000下面 编写cronlab定时任务自动更新gitbook内容 自动部署到github pages #!/usr/bin/env sh # 拉取代码 echo '开始执行命令' git pull # 生成静态文件 echo '执行命令：gitbook build .' gitbook build . # 进入生成的文件夹 echo \"执行命令：cd ./_book\\n\" cd ./_book # 初始化一个仓库，仅仅是做了一个初始化的操作，项目里的文件还没有被跟踪 echo \"执行命令：git init\\n\" git init # 保存所有的修改 echo \"执行命令：git add -A\" git add -A # 把修改的文件提交 echo \"执行命令：commit -m 'deploy'\" git commit -m 'deploy' # 如果发布到 https://.github.io/ echo \"执行命令：git push -f https://github.com/yulilong/book.git master:gh-pages\" git push -f git@github.com:maxiaoteng001/maxiaoteng-book.git master:gh-pages # 返回到上一次的工作目录 echo \"回到刚才工作目录\" cd - 5. gitbook配置文件 gitbook的插件 book.json中不能有注释， 注释在这里 // 安装插件 // 1. 折叠目录， 2. 分块显示 3. 自由移动目录侧 \"plugins\": [\"toggle-chapters\", \"sectionx\", \"splitter\"] # 安装插件 gitbook install 6. 启动两个gitbook服务 参考： https://blog.csdn.net/moxiaomomo/article/details/53026645gitbook web服务端口:4000 重启服务端口:35729 gitbook serve --lrport 35288 --port 4001 /path2/your_another_doc_dir/ 7. gitbook插件 1. 使用 插件需要在book.json中配置 位于plugins中, 添加-将会取消插件 \"plugins\": [\"-toggle-chapters\", \"sectionx\", \"splitter\"] 安装 gitbook install 2. 插件列表 copy-code-button # 为代码添加复制按钮 edit-link # 直接链接到源文件 \"plugins\": [\"edit-link\"], \"pluginsConfig\": { \"edit-link\": { \"base\": \"https://github.com/USER/REPO/edit/BRANCH\", \"label\": { \"en\": \"Edit This Page\", \"ch\": \"编辑本页\" } } } anchor-navigation-ex # 添加toc到侧边悬浮,并添加回到顶部 simple-page-toc # 自动生成本页的目录结构 { \"plugins\" : [ \"simple-page-toc\" ], \"pluginsConfig\": { \"simple-page-toc\": { \"maxDepth\": 3, \"skipFirstH1\": true } } } local-video # 使用Video.js 播放本地视频 \"plugins\": [ \"local-video\" ] 使用示例：为了使视频可以自适应，我们指定视频的width为100%，并设置宽高比为16:9，如下面所示 To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video 另外我们还要再配置下css，即在website.css中加入 .video-js { width:100%; height: 100%; } 代码示例 To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video search-plus # 支持中文搜索 { \"plugins\": [\"-lunr\", \"-search\", \"search-plus\"] # 同时禁用lunr和search } include-csv # 显示csv内容 { \"plugins\": [\"include-csv\"] } 使用方法 ## {-% include**Csv src=\"../../assets/csv/test.csv\", useHeader=\"true\" %-} ## {-% endincludeCsv %-} disqus # 支持disqus评论 \"plugins\": [ \"disqus\" ], \"pluginsConfig\": { \"disqus\": { \"shortName\": \"gitbookuse\" } } tbfed-pagefooter # 添加页脚 \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy zhangjikai.com 2017\", \"modify_label\": \"该文件修订时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" } } expandable-chapters-small # 折叠左侧目录 theme-default # 左侧目录加序号等默认主题 目录前面加序号 \"pluginsConfig\": { \"theme-default\": { \"showLevel\": true }, } Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-09 10:13:28 "},"Configurations/vagrant.html":{"url":"Configurations/vagrant.html","title":"Vagrant","keywords":"","body":"使用Vagrant说明安装并开始vagrant box管理创建虚拟机虚拟机ssh连接配置不推荐的自动创建虚拟机虚拟机的一个解释使用Vagrant 使用virtualbox的api操作虚拟机 说明 Box就是一个箱子, 实际就是一个zip包, 里面包含了vagrant的配置信息, 虚拟机镜像文件以及构建环境等. 特点 快速完成部署 支持多种虚拟机系统 内置配置管理支持 缺点 维护更新不及时 积累issue太多,解决不及时 解决办法: 查找稳定版本的副本, 或者配置自己的副本并保持更新 安装并开始 软件安装 官网下载, 正常安装即可安装目录: C:\\Program Files (x86)\\vagrant vagrant box管理 选择box 选择网站: https://app.vagrantup.com/boxes/search 在官网上搜索合适的box, 我选择了官方的centos/7 列出box vagrant box list 新增box vagrant box add box_name # 系统将会自动搜索并拉取下来 vagrant box add name box_path # 添加本地的box 删除box vagrant box rm box_name 注意: 创建虚拟机时, vagrant会复制并解压一份到特定目录(C:\\Users\\jizhu.vagrant.d\\boxes\\centos-VAGRANTSLASH-7\\0\\hyperv), 删除box不会影响虚拟机 手动下载box, 然后添加 使用下面命令尝试下载, 可以得到box的urlvagrant up --provide hyperv # 默认virtualbox 下载之后放在任何文件夹下执行vagrant box add centos/7 box_path # 执行完之后就可以删除原始box 创建虚拟机 添加好box之后, 就能在本机基于box去创建虚拟机. 创建一个目录, 为项目目录 初始化 vagrant init box-name # box-name可以通过vagrant box list来查看 启动虚拟机 vagrant up # 默认virtualbox vagrant up --provide hyperv # 指定hyperv类型 查看虚拟机状态 vagrant status 暂停虚拟机 vagrant suspend 恢复虚拟机 vagrant resume 彻底关闭虚拟机 vagrant halt 删除虚拟机 vagrant destroy 打包虚拟机 vagrant package vagrant package --output xx.box # 指定box的名称 虚拟机ssh连接 虚拟机网络配置-- 启动时会显示虚拟机的ip地址 默认是密钥登陆, 默认位置: C:\\Users\\jizhu\\vagrant\\centos7\\.vagrant\\machines\\default\\hyperv\\private_key 配置 虚拟机网络配置 虚拟机默认NAT网络, 虚拟机通过宿主机上网 配置公开路由, 可以由局域网内其他设备访问:config.vm.network \"public_network\", ip: \"192.168.2.222\" # 其他设备便可通过22端口访问 内存和cpu配置config.vm.provider \"virtualbox\" do |v| v.memory = 1024 v.cpus = 2 end 不推荐的自动创建虚拟机 cd到要创建虚拟机的位置(我的位于: ~/vagrant/centos7)命令行下: vagrant init centos/7 # 下载vagrant配置文件 启动: vagrant up # 将会下载镜像, 配置文件 因为windows10安装docker必须开启Hyper-V, 所以虚拟机要以hyperv类型运行 设置vagrant启动时,虚拟机的类型: vagrant up --provide hyperv # 默认virtualbox # 会自动配置好 虚拟机的一个解释 hypervisor虚拟层 VirtualBox and VMware Workstation (or VMware Player) 被称为虚拟管理程序2级. Hyper-V(hyper-v是windows自带的虚拟化技术) or VMware ESXi 是虚拟管理程序1级. 虚拟二级是运行在一个OS中的应用, 而虚拟1级就是这个系统. 当windows10开启了hyperv, 就变成了一台虚拟机. 那么为什么VirtualBox和VMware不能运行在Hyper-V虚拟机里面呢,因为作为一个VM，Intel VT-X指令不再从你的虚拟机中访问，只有主机才能拥有它. Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/docker.html":{"url":"Configurations/docker.html","title":"Docker","keywords":"","body":"Docker1. 基本概念2. 安装和开始1. windows2. linux(centos)3. macos3. Docker常用命令1. 命令2. 启动容器注意4. 用户管理Docker [TOC] 1. 基本概念 LXC(Linux Containers)操作系统层面化的虚拟技术, 只运行1个内核, 一个虚拟化的可执行内核就是一个容器,可以绑定CPU和内存的使用, 分配特定比例的CPU的时间, IO时间, 容器技术的本质是对对计算机系统资源的隔离和控制 Docker 是一个虚拟环境的容器. Docker特点和应用场景 个人开发机和服务器不是同一个操作环境 应用程序需要不同的应用环境 耗费资源很小 虚拟机 or 用容器技术 一个应用通常需要启动很多服务, 每个服务器需要一个容器, 开发麻烦 Docker桌面环境支持少 所以开发使用vagrant虚拟机, 部署使用docker 2. 安装和开始 1. windows 安装软件 官网安装, 目录: C:\\Program Files\\Docker 文件目录docker pull下载后的文件位置: C:\\Users\\Public\\Documents\\Hyper-V\\Virtual Hard Disks 说明: windows上的docker本质上还是借助与windows平台的hyper-v创建一个linux虚拟机，你执行的所有命令都是在这个虚拟机里执行的，所有pull到本地的image都会在虚拟机的Virtual hard disks目录的文件中，这个文件就是虚拟硬盘文件（有点类似与vmware的原理）。 因此你打开hyper-v管理器，可以找到docker创建的虚拟机，点击左侧的虚拟机名称，然后再点击右边的移动选项，按照向导将虚拟机移动到其他目录即可。 另外还可以在菜单栏点击:操作->Hyper-v设置，来调整你所有虚拟机的虚拟硬盘文件的默认存储位置 说明 windows10安装了docker,要求启用hyper-v来运行运行docker的虚拟机MobyLinuxVM, 所以需要重启 启用关闭Hyper-V的方法: 管理员身份打开PowerShell bcdedit # 查看Hyper-V是否启用 bcdedit /set hypervisorlaunchtype auto # 设置为启动 2. linux(centos) 使用仓库安装 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce 使用脚本安装 curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh 查看启动 sudo systemctl start docker # 启动docker RHEL 安装 踩坑参考(https://stackoverflow.com/questions/45415524/installing-docker-ce-in-redhat/47903707#47903707) 安装 sudo yum install -y docker 错误 (报错: No package docker available. 或 Package: docker-ce-17.06.0.ce-1.el7.centos.x86_64 (docker-ce-stable) Requires: container-selinux >= 2.9) 对于RHEL 7.x以上的系统, docker在 rhel-7-server-extras-rpms仓库中, 可以使用如下命令生效: subscription-manager repos --enable=rhel-7-server-extras-rpms 如果系统(如: aws上的RHEL)没有任何subscription, 可以使用centos的扩充yum源, 方法如下: sudo vim /etc/yum.repos.d/centos.repo [CentOS-extras] name=CentOS-7-Extras mirrorlist=http://mirrorlist.centos.org/?release=7&arch=$basearch&repo=extras&infra=$infra #baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/ gpgcheck=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 3. macos 参考菜鸟教程https://www.runoob.com/docker/macos-docker-install.html homebrew安装 brew cask install docker > 3. Docker常用命令 参考docker命令详解全面教程 1. 命令 查看docker版本 docker --version image 查看本地有哪些镜像 docker image ls search 搜索docker镜像 docker search ubuntu pull 下载需要的docker镜像 docker pull ubuntu:rolling # The centos: **latest** tag is always the most recent version currently available. docker pull centos:latest rmi 删除映像 docker rmi image_name # 删除映像 docker rmi image_name -f # 强制删除映像 ps/container 查看本地有哪些容器 docker container ls # 只列出正在运行的 docker container ls --all # 列出所有, 包括运行结束的 docker ps # 查看正在运行的容器 docker ps --all # 查看所有的容器 create 创建容器 docker create -i # 交互式操作 -t # 终端 --rm # --rm 等价于 --rm=True, 默认False,若容器内的进程终止，则自动删除容器，此选项不能与-d选项一起使用, 一旦退出则删除, 一般用于测试 --attach=\"stdin\" # 将标准输入、标准输出、标准错误链接到容器 --add-host=hello:192.168.0.233 # 向容器的/etc/hosts添加主机名与IP地址 --link mysql-server:mysql # 进行容器连接，格式为: --name # 设置容器名称 --net=\"bridge\" # 设置容器的网络模式（选项可以是：bridge,none,container,host） -P、--publish-all=false # 将连接到主机的容器的所有端口暴露在外 -p、--publish=[] # 如:-p 3128:3128 将连接到主机的容器的特定端口暴露在外。一般主要用于暴露web服务器的端口 -v、--volume=[] # 设置数据卷。设置要与主机共享目录，不将文件保存到容器，而直接保存到主机。在主机目录后添加 :ro、:rw进行读写设置，默认为:rw -w、--workdir=\"\" # 设置容器内部要运行进程的目录 --privileged # 特权模式 -d、--detach # Detach模式，一般为守护进程模式，容器以后台方式运行 --sig-proxy=true # 将所有信号传递给进程（非TTY模式时也一样），但不传递SIGCHLD、SIGKILL、SIGSTOP信号 run 启动容器 # 参数同create docker run start/attach/restart 启动并登陆指定容器 docker start 858fd7c6a9d5(container_id) # 启动一个容器 docker attach container_name/container_id # 重新登陆指定容器 pause/unpause 暂停/启动容器 exit 退出容器exit # 即可退出并关闭 或使用 ctrl + D # 退出并后台运行: ctrl + P + Q 或: Ctrl + P , Ctrl + D logs 查看容器日志docker logs 858fd7c6a9d5(container_id) # 查看容器的log -f # 持续输出 --tail=7 # 指定数量 -t # 显示时间戳 port 查看容器开放的端口docker port 858fd7c6a9d5(container_id) cp 容器和外界文件传输sudo docker cp contain_id:/root/DTP/output.xlsx ~/new_dir/ top 查看容器进程信息docker top 858fd7c6a9d5(container_id) aux # 查看容器的进程 exec 从外部运行内部的命令docker exec -it 858fd7c6a9d5(container_id) /bin/bash # 连接容器 ps ax export 容器持久化# 不保存历史的导入导出 docker export contain_id > ./new_contain.tar save # 保存历史的导入导出 docker save contain_id > ./new_contain.tar import 容器的导入 docker import dtp_container.tar dtp # 容器的名称将是dtp commit 容器制作镜像 # 根据容器的改变生成一个新的镜像 docker commit contain_id ./new_image.tar rm 删除容器操作 docker rm container_id/container_name # 删除容器 rmi 删除镜像 docker rmi image_name # 删除镜像 info 显示当前系统信息、docker容器、镜像个数、设置等信息。 docker info 使用特权运行dockerhttps://blog.51cto.com/lizhenliang/1975466 docker run -d -name centos7 --privileged=true centos:7 /usr/sbin/init 2. 启动容器注意 配置参数可以看官方说明, 一般映射左侧为宿主机, 右侧为docker内: --volume /path/to/config:/config, 数据卷挂载, 统一挂载到/docker_data/contain_name/ --port 3306:3306, 端口转发 --restart=unless-stopped 重启选项, 一直重启或一直重启除非停止 其他 启动参考 docker run -p 3306:3306 --restart=always --name mymysql -v /srv/dev-disk-by-label-other/docker_data/mysql/conf:/etc/mysql/conf.d -v /srv/dev-disk-by-label-other/docker_data/mysql/logs:/var/log/mysql -v /srv/dev-disk-by-label-other/docker_data/mysql/db:/var/lib/mysql -v /srv/dev-disk-by-label-other/docker_data/mysql/mysql_files:/var/lib/mysql-files -e MYSQL_ROOT_PASSWORD=maxiaoteng -d mysql:latest 端口映射 自动重启 unless-stopped/always 名称 挂载 db, conf, log, mysql-files(用于mysql dump) root密码 docker run -d -p 5901:5901 -v /mnt/md0/public/baiduyun:/mnt/drive_d -e vnc_password=92Hi4aJ6Fp6aLDj5 johnshine/baidunetdisk-crossover-vnc:latest 4. 用户管理 Docker守候进程绑定的是一个unix socket，而不是TCP端口。这个套接字默认的属主是root，其他是用户可以使用sudo命令来访问这个套接字文件。因为这个原因，docker服务进程都是以root帐号的身份运行的。 为了避免每次运行docker命令的时候都需要输入sudo，可以创建一个docker用户组，并把相应的用户添加到这个分组里面。当docker进程启动的时候，会设置该套接字可以被docker这个分组的用户读写。这样只要是在docker这个组里面的用户就可以直接执行docker命令了。 查看docker相关的分组 sudo cat /etc/group | grep docker 创建分组, 添加用户 sudo groupadd -g 999 docker # 999是分组id, 可不指定 # 添加用户 sudo usermod -aG dockerroot ec2-user sudo usermod -aG docker ec2-user 确认创建成功 cat /etc/group 重启 sudo systemctl restart docker 确认能否直接运行 docker info # 如果提示权限不足 sudo chmod a+rw /var/run/docker.sock Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-13 15:20:05 "},"Configurations/virtual_machine.html":{"url":"Configurations/virtual_machine.html","title":"虚拟机配置","keywords":"","body":"虚拟机中的具体配置安装系统安装软件虚拟机和本机共享文件虚拟机中的具体配置 安装系统 centos7 安装软件 epel yum 扩展源 python36 pip virtualenv nano git 虚拟机和本机共享文件 虚拟机安装sshfs sudo yum install sshfs # 如果无法安装, 则需要扩展epel sudo yum install epel-release windows安装win-sshfs 安装说明 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/mxtaws.html":{"url":"Configurations/mxtaws.html","title":"mxt_aws","keywords":"","body":"通过利用swap file实现虚拟内存项目 epel-release python36 python -m pip nano tmux wget shadowsocks(逗比根据地的一键安装教程) lnmp git nginx, supervisor virtualenv mongodb minicanda docker 安装python开发版本sudo yum install python-devel sudo yum install python36-devel 通过利用swap file实现虚拟内存 http://www.cnblogs.com/owenyang/p/4282283.html 项目 wechat_api nginx代理:2000 bbs nginx代理:2001 gitbook nginx代理:4000 jaurnal nginx代理:4001 gerapy nginx代理:8000 scrapyd nginx代理:6800 portia nginx代理:9001 shadowsocks 预开放端口 27233 - 27400, 当前27333, 多开放27334, 27335, 备用端口: 27400 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/aws.html":{"url":"Configurations/aws.html","title":"aws运维","keywords":"","body":"aws 服务器配置环境1. yum说明2. 基本软件- 先查看可安装包, 比如:安装python36 和 pip- python34- 安装 pip, requests, lxml, bs4aws 服务器配置 环境 Red Hat Enterprise Linux (RHEL) 7 1. yum说明 Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装 安装并启用EPEL rpm软件包 ELEP: Extra Packages for Enterprise Linux的简称，是为企业级Linux提供的一组高质量的额外软件包 sudo yum install –y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 安装软件包 sudo yum install -y python36 # -y为自动应答 升级软件包 sudo yum check-update # 检查更新 sudo yum update # 升级所有 sudo yum update tomcat # 升级特定软件包 查看已安装软件 yum list installed 查看软件包的描述信息 yum info tomcat 卸载软件包 sudo yum remove tomcat 2. 基本软件 - 先查看可安装包, 比如: sudo yum list| grep python3 安装python36 和 pip sudo yum install python36 # 虽然3.4之后默认包含pip, 但我的版本太纯净没有pip python36 -m pip --version > /usr/bin/python36: No module named pip # 安装pip sudo wget https://bootstrap.pypa.io/get-pip.py sudo python3.6 get-pip.py - python34 发现可以安装的最新版本是python36, 但是没有python36-pip, 不便于管理, 安装python34 $ sudo yum install python34 $ type -a python34 //type 命令查看命令的路径 python34 is /usr/bin/python34 $ sudo ln /usr/bin/python34 /usr/bin/python3 //建立软连接, 更便于访问 $ type -a python3 python3 is /usr/bin/python3 - 安装 pip, requests, lxml, bs4 $ sudo yum install python34-pip $ sudo pip install pip -U -q # 静默安装, 升级 $ pip --version $ sudo python3 -m pip install requests lxml bs4 //要想使用这种, 需要安装 第二种方法: // 如果想安装setuptools, 使用以下方法: $ sudo yum install python36-setuptools $ sudo easy_install-3.6 pip $ sudo pip3 install requests lxml bs4 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-26 14:39:31 "},"Configurations/vscode.html":{"url":"Configurations/vscode.html","title":"VScode","keywords":"","body":"快捷键设置1. 自动同步2. windows10快捷键 设置: 文件 → 首选项 → 键盘快捷方式 --> 可以搜索关键字, 也可以直接按键 windows下的快捷键 Ctrl + [ ] # 缩进 Ctrl + D # 删除整行 Ctrl + Delete # 删除光标右侧 Alt + ⬆/⬇ # 整行上移下移 Ctrl+ ⬆/⬇ # 行滚动页面 Ctrl+ pagedown/pageup # 滚动页面 Ctrl + s # 保存 Ctrl + n # 创建新文件 Alt + Shift + ⬆/⬇ # 复制该行内容到上一行或下一行 Alt + Shift + 鼠标 # 多行编辑 Ctrl + L + K # 格式化代码 Macos快捷键 control + shift + ⬇️/⬆️ # 往下/往上一行 option + ⬆️/⬇️ # 移动行 option + shift + ⬆️/⬇️ # 往上/下复制代码 option + shift + F # 格式化代码 option + Shift + P # 打开搜索, language设置 Display language command + D # 删除整行 command + Delete # 删除光标右侧 command + backspace # 删除光标左侧 设置 Ctrl + Shift + p # 打开搜索, language设置 Display language 语言 { // Defines VS Code's display language. // See https://go.microsoft.com/fwlink/?LinkId=761051 for a list of supported languages. // \"locale\":\"en\" // Changes will not take effect until VS Code has been restarted. \"locale\":\"zh-CN\" // Changes will not take effect until VS Code has been restarted. } 如果重启还是不行, 需要安装中文包, 扩展搜索: chinese / language 设置tab 默认缩进4个空格 文件 → 首选项 → 设置 → 用户设置 → 打开setting.json → tabsize 设置启动的终端 setting.json中 // \"terminal.integrated.shell.windows\":\"C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe\", // \"terminal.integrated.shell.windows\": \"C:\\\\Windows\\\\System32\\\\cmd.exe\", \"terminal.integrated.shell.windows\": \"D:\\\\soft\\\\git\\\\bin\\\\bash.exe\" 导入导出配置文件 位于：/yonghu/Appdata/Roaming/code/user/setting.json 1. 自动同步 参考： https://juejin.im/post/5b9b5a6f6fb9a05d22728e36 2. windows10 WinDefend Windows Defender Antivirus Service占用内存高！怎么解决？ Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-05-13 10:29:53 "},"Configurations/mysql.html":{"url":"Configurations/mysql.html","title":"Mysql","keywords":"","body":"windows7 安装mysqlpymysql查询windows7 安装mysql 启动: 服务中找到mysql, 点击启动即可 char varchar char(n)和varchar(n)的n表示字符的个数, 不表示字节数, char不管实际值都会占用n个空间, 而varchar占用实际字符+1 text char存储时会截断尾部空格, varchar和text不会 varchar 255 tinytext 500 text 20000 mediumtext blob pymysql查询 # 建立数据库, 数据来自三江源在阿里云的服务器 connect = pymysql.connect( host='39.107.95.132', db='sanjiangyuan', user='root', passwd='jAqFum_sMusIE6E8', charset='utf8', use_unicode=True ) # 如果不使用Dict, 返回的是list, 每个元素都是元祖 cursor = connect.cursor(cursor=pymysql.cursors.DictCursor) try: cursor.execute( \"select * from yunfu_crawler_news where created_at >= '{}'\".format(yesterday) ) data = cursor.fetchall() connect.commit() return data except Exception as e: print(str(e)) logging.info(str(e)) finally: connect.close() Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/shadowsocks.html":{"url":"Configurations/shadowsocks.html","title":"Shadowsocks","keywords":"","body":"ShadowsocksShadowsocks 安装参考教程：https://doub.io/ss-jc5/ 一键安装脚本 sh ssr.sh可以用来配置各种参数 2019.3.1 发现vpn无法使用 之前的办法是重启aws来更换ip， 这次通过尝试将443换为27233之后可以正常访问，这样就不用修改域名的绑定啦！ 之前27233运行着squid，没什么用就关闭了 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/domain.html":{"url":"Configurations/domain.html","title":"域名绑定","keywords":"","body":"域名管理域名管理 最开始我从腾讯购买.xyz域名,11块钱/年 freenom可以免费注册域名, 我又注册了maxiaoteng.tk https://my.freenom.com/ 域名解析: 登录 邮箱,密码在chrome保存 我的域名下 → 域名管理 → manage freenom dns 可以设置域名解析, 设置多个二级域名 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"Configurations/macos.html":{"url":"Configurations/macos.html","title":"Macos使用","keywords":"","body":"Macos 使用1. 快捷键Macos 使用 1. 快捷键 command代替大多ctrl操作 command + X # 剪切 command + C # 复制 command + V # 黏贴 command + Z # 撤销 锁屏 control + command + Q 大小写 长按中英切换，或按住shift + 字母 双指滚动，三指切换应用，四指列出桌面 Command + Opthon + Esc # 强制退出应用程序 截屏 Command + Ctrl + Shift + 3 # 整个屏幕截屏，剪贴板 Command + Shift + 4 # 对选定内容截屏，到桌面文件， 使用space可以对某个窗口截屏 外接鼠标之后，滚轮和滑动加速度不匹配，参考 https://sspai.com/post/40596 解决 查看窗口坐标 command + shift + 4 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-18 15:13:25 "},"Configurations/iterm.html":{"url":"Configurations/iterm.html","title":"Iterm2","keywords":"","body":"iterm21. Iterm快捷键2. 使用mac进行ssh登陆linux服务器3. 设置iterm2 mac下使用iterm作为终端模拟工具 1. Iterm快捷键 Command + d # 垂直分屏 Command + shift + d # 水平分屏 Command + ] /[ # 最近使用的分屏切换 Command + opthon + 方向键 # 切换到指定的分屏 Command + 数字/方向键 # 切换标签页 shift + Command + s # 保存当前窗口快照 Command + option + b # 快照回放 ctrl + u # 删除整行 ctrl + w # 删除前一个单词 ctrl + c # 直接下一行重新输入 2. 使用mac进行ssh登陆linux服务器 直接命令行操作 ssh xiaoteng@11.11.1.1 # password ..即可 配置profile实现一键登录 https://blog.csdn.net/Eden_M516/article/details/73741290 编写expect脚本，vi ～/.ssh/profile1 #!/usr/bin/expect -f set user set host set password set timeout -1 spawn ssh $user@$host expect \"*assword:*\" send \"$password\\r\" interact expect eof iterm2 preference profiles new 填写name(profile1), command(expect ~/.ssh/profile1) 启动iterm时选择profile1的名字即可 对于需要密钥登陆的服务器，可以自动登陆 3. 设置 代理设置 export http_proxy=socks5://127.0.0.1:1080 # 配置http 代理访问 export https_proxy=socks5://127.0.0.1:1080 # 配置https 代理访问 export all_proxy=socks5://127.0.0.1:1080 # 配置http和https访问 unset http_proxy # 取消http 代理访问 unset https_proxy # 取消https 代理访问 # 简易化设置proxy alias proxy_charls='export http_proxy=http://127.0.0.1:8888 https_proxy=http://127.0.0.1:8888' alias proxy_mitmproxy='export http_proxy=http://127.0.0.1:8080 https_proxy=http://127.0.0.1:8080' alias proxy_system='export http_proxy=http://127.0.0.1:7890 https_proxy=http://127.0.0.1:7890' alias unproxy='unset http_proxy https_proxy' Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-05-12 16:55:09 "},"Configurations/xposed.html":{"url":"Configurations/xposed.html","title":"Xposed","keywords":"","body":"记录魅族安装xposed的过程1. root2. 安装xposed3. 安装xposed模块记录魅族安装xposed的过程 1. root 设置 --> 指纹，面部和安全 root权限 按照说明下载app 登陆网站请求root 在app获取root权限 在设置中打开root 2. 安装xposed 找到对应8.0的xposed installer（3.1.5） 进入xposed后点击下方的安装/更新即可安装 3. 安装xposed模块 搜索安装 在模块中勾选，重启生效 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-05-13 13:45:59 "},"Configurations/nas.html":{"url":"Configurations/nas.html","title":"Nas","keywords":"","body":"Nas1. 选购2. 配置3. 系统OMVNas 契机是和LP讨论生日礼物，想要移动硬盘，我的理解是这种东西安全性很差，数据很容易丢失，遂研究了公有云，又到NAS 1. 选购 1k左右的西部数据，功能太少 2k群晖 ARM架构的处理器，不支持docker 发现了公司用的terra master，同样价格用的Intel的处理器，2G内存 最终选定F2-220，搭配单硬盘共1950元 2. 配置 系统 安装系统， 但十分不好用，资源也少，其实是不推荐的 网络 由于用光猫拨号，所以设置路由器为DMZ主机，又将NAS作为路由器的DMZ主机 动态IP解决(每次断电，再次拨号都会换IP)，域名解析 花生壳，不考虑，域名太长，据说有流量限制，收费限制多 -- 失败 dyndns 反应慢，没有成功 -- 失败 路由器的ddns，TPlink不能登陆TPlinkID -- 失败 最终使用dnspod动态解析 -- 成功 配置好二级域名解析nas,类型A 获取动态解析的API token 采用脚本部署到nashttps://github.com/kkkgo/dnspod-ddns-with-bashshell TOS垃圾，curl命令都没有，ssh登陆到root账号，创建了centos7的docker系统，然后配置脚本和定时任务，OK Docker系统很残，还是需要docker来做 mxt_centos rundocker run -itd -p 22222:22 -v /root/centos:/root --name mxt_centos --privileged=true docker/mxt_centos /usr/sbin/init docker exec -it docker_id /bin/bash crontab 系统缺少curl等命令, 但是docker对crontab的支持不好用, 最终解决如下/mnt/md0/application/bin/docker exec mxt_centos /bin/bash -c \"cd /root & /usr/bin/sh /root/dnspod_ddns.sh >/root/cron.log\" dnspod 动态ip绑定域名 # 定时任务crontab来配置域名动态解析 */10 * * * * sh /home/dnspod_ddns.sh >/home/cron.log # /home/dnspod_ddns.sh #Dnspod DDNS with BashShell #Github:https://github.com/kkkgo/dnspod-ddns-with-bashshell #More: https://03k.org/dnspod-ddns-with-bashshell.html #CONF START API_ID=111374 API_Token=2551604e10be0d939b50e56d4ea1ae3f domain=maxiaoteng.xyz host=nas CHECKURL=\"http://ip.03k.org\" #OUT=\"pppoe\" #CONF END . /etc/profile date if (echo $CHECKURL |grep -q \"://\");then IPREX='([0-9]{1,2}|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\\.([0-9]{1,2}|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\\.([0-9]{1,2}|1[0-9][0-9]|2[0-4][0-9]|25[0-5])\\.([0-9]{1,2}|1[0-9][0-9]|2[0-4][0-9]|25[0-5])' URLIP=$(curl -4 -k $(if [ -n \"$OUT\" ]; then echo \"--interface $OUT\"; fi) -s $CHECKURL|grep -Eo \"$IPREX\"|tail -n1) if (echo $URLIP |grep -qEvo \"$IPREX\");then URLIP=\"Get $DOMAIN URLIP Failed.\" fi echo \"[URL IP]:$URLIP\" dnscmd=\"nslookup\";type nslookup >/dev/null 2>&1||dnscmd=\"ping -c1\" DNSTEST=$($dnscmd $host.$domain) if [ \"$?\" != 0 ]&&[ \"$dnscmd\" == \"nslookup\" ]||(echo $DNSTEST |grep -qEvo \"$IPREX\");then DNSIP=\"Get $host.$domain DNS Failed.\" else DNSIP=$(echo $DNSTEST|grep -Eo \"$IPREX\"|tail -n1) fi echo \"[DNS IP]:$DNSIP\" if [ \"$DNSIP\" == \"$URLIP\" ];then echo \"IP SAME IN DNS,SKIP UPDATE.\" exit fi fi token=\"login_token=${API_ID},${API_Token}&format=json&lang=en&error_on_empty=yes&domain=${domain}&sub_domain=${host}\" Record=\"$(curl -4 -k $(if [ -n \"$OUT\" ]; then echo \"--interface $OUT\"; fi) -s -X POST https://dnsapi.cn/Record.List -d \"${token}\")\" iferr=\"$(echo ${Record#*code}|cut -d'\"' -f3)\" if [ \"$iferr\" == \"1\" ];then record_ip=$(echo ${Record#*value}|cut -d'\"' -f3) echo \"[API IP]:$record_ip\" if [ \"$record_ip\" == \"$URLIP\" ];then echo \"IP SAME IN API,SKIP UPDATE.\" exit fi record_id=$(echo ${Record#*\\\"records\\\"\\:\\[\\{\\\"id\\\"}|cut -d'\"' -f2) record_line_id=$(echo ${Record#*line_id}|cut -d'\"' -f3) echo Start DDNS update... ddns=\"$(curl -4 -k $(if [ -n \"$OUT\" ]; then echo \"--interface $OUT\"; fi) -s -X POST https://dnsapi.cn/Record.Ddns -d \"${token}&record_id=${record_id}&record_line_id=${record_line_id}\")\" ddns_result=\"$(echo ${ddns#*message\\\"}|cut -d'\"' -f2)\" echo -n \"DDNS upadte result:$ddns_result \" echo $ddns|grep -Eo \"$IPREX\"|tail -n1 else echo -n Get $host.$domain error : echo $(echo ${Record#*message\\\"})|cut -d'\"' -f2 fi mysqldocker create -p 3306:3306 --name docker_mysql -v /root/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=password mysql baiduyun 参考地址：https://github.com/john-shine/Docker-CodeWeavers_CrossOver-VNC/tree/master/BaiduNetdiskdocker run -d -p 5901:5901 -v /mnt/md0/public/baiduyun:/mnt/drive_d -e vnc_password=password johnshine/baidunetdisk-crossover-vnc:latest 3. 系统OMV 安装 常用安装 使用U盘刻录omv系统镜像, 软件balena 将系统和系统U盘插入, 自动安装和启动 也可以用虚拟机也可以安装后插入到主机上, 见无显示器安装 网络配置有问题 配置 概述 文件服务 SMB: 局域网主要通过SMB访问 kodexplorer: 文档和图片需要简单预览 备份电脑以及手机文件时 多平台共享文件 网络服务 开启服务 web页面 omv-firstaid设置 端口: 8181 ssh 端口: 9222 docker ftp 21 22 smb maxiaoteng 共享两个 for_guest 公开视频目录 maxiaoteng 整个目录 jellyfin maxiaoteng docker安装 8096 mysql docker安装 3306 kodexplore https://www.jianshu.com/p/4731a1ef01d1 通过/data访问宿主机目录 设置防止crtf登录, 甚至可以加上验证码 除了管理员, 其他用户要想访问/data, 需要单独配置php docker run -d -p 9000:80 --name kodexplorer \\ -v /srv/dev-disk-by-label-kulh2t/appdata/www:/var/www/html \\ -v /srv:/data \\ qinkangdeid/kodexplorer 插件 lvm omv extra wget http://omv-extras.org/openmediavault-omvextrasorg_latest_all4.deb dpkg -i openmediavault-omvextrasorg_latest_all4.deb 文件系统 wipe擦写磁盘 创建raid(可选) 创建lvm分区, 并挂载 在分区上建立共享目录 2T硬盘分两个 storage 保存所有自由数据 docker_lib保存docker相关的,镜像, 数据库文件等 public1 500G 用来备份time machine 配置time machine 备份mac https://dannyda.com/2019/07/17/how-to-create-apple-time-machine-in-open-media-vault-omv/ Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-02 11:47:40 "},"Configurations/gmail.html":{"url":"Configurations/gmail.html","title":"Gmail","keywords":"","body":"Gmail的管理Gmail的管理 归档 理解Inbox/Archive Inbox 待处理/未处理 Archive 已处理/不处理 所以将所有通知右键，包括calender，teambition等直接归档即可 设置- 过滤器- 创建条件- 应用到(标签,归档,删除,标为已读等) 转发 为了统一管理右键，将qq,hotmail统一转发到gmail 用法：设置- 账号和导入- 查收其他帐号的邮件 qq的处理 hotmail的处理 及时退订广告邮件 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-08-21 17:53:11 "},"Configurations/media_manager.html":{"url":"Configurations/media_manager.html","title":"照片管理","keywords":"","body":"多媒体管理1. 基本2. google Photo多媒体管理 [TOC] 多媒体包含照片和视频 1. 基本 基本思路是 google photo和NAS双备份 2. google Photo 上传 mac和windows, 使用备份工具上传 选择所在文件夹, 以高画质上传照片和视频 手机 google 相册, 选择自动备份, 勾选需要的设备文件夹 以高画质上传 定期释放空间 下载 右键下载单张 https://takeout.google.com/?pli=1 导出所有 归档 首先把没用的截图,表情包 进行归档, 也可加入到相应的影集 确保时间线没有杂乱 时间错乱的也要归档 照片养成好的习惯,每个月归档上个月的, 按时间建立影集 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-03 15:50:12 "},"Configurations/google.html":{"url":"Configurations/google.html","title":"Google","keywords":"","body":"google相关1. google 平台api调用2. google相册google相关 [TOC] 1. google 平台api调用 进入到google cloud platform的控制台 创建一个特定的项目 api综合管理 进入api和服务 -> 在库中搜索要启用的api服务 -> 查看价格、文档和说明后选择启用 信息中心可以看启用的api和访问情况 凭据的api密钥用来访问api, 可以限定密钥允许访问的api 可以修改api的限制, 比如ip, 客户端等 单个api管理 点击特定api, 可以通过配额和凭据限定api的访问 google map api https://maps.googleapis.com/maps/api/geocode/json?latlng=40.714224,-73.961452&key=*** 费用: 5.0USD/1000次 2. google相册 具体内容见媒体管理 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-03 15:48:11 "},"Interview/resume.html":{"url":"Interview/resume.html","title":"我的简历","keywords":"","body":"马晓腾马晓腾 爱好技术,喜欢运动,实践做一个终身学习者 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-12-11 14:09:56 "},"Interview/summary.html":{"url":"Interview/summary.html","title":"面试汇总","keywords":"","body":"面试知识点1. HTTP基础1. HTTP的三次握手2. 转发(Forward)和重定向(Redirect)3. 从浏览器输入url到显示页面, 整个过程会使用哪些协议?4. 为什么要使用线程池?2. Python基础1. 常用库2. Python进程的内存结构3. Python的垃圾回收机制4. 小的知识点：5. 进程、线程和协程6. 装饰器(Decorators)3. django1. 描述django一次请求的过程4. Flask1.面试知识点 1. HTTP基础 1. HTTP的三次握手 参考链接: https://blog.csdn.net/qzcsu/article/details/72861891 示意图解析 客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端 为什么三次握手? 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常。 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己接收正常，对方发送正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送接收正常 为什么断开要四次? 断开一个 TCP 连接则需要“四次挥手”： 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 2. 转发(Forward)和重定向(Redirect) - 转发是服务器行为, 服务器访问一个url,然后将内容返回给浏览器 - 重定向是客户端(浏览器)根据服务器返回的状态吗,配合Location进行重定向 - 从数据来看, forward能够共享request的数据 - 从效率来看, forward比redirect高 - 从客户端来看, forward不能察觉 - 使用上, 一般转发用于登录后,根据用户角色转发到相应模块. 跳转用于注销登录后跳转首页或其他页面 3. 从浏览器输入url到显示页面, 整个过程会使用哪些协议? 1. DNS解析. - 浏览器查找域名的IP地址 - DNS查找过程: 浏览器缓存,路由器缓存,DNS缓存 2. 与服务器建立TCP连接 3. 发送HTTP请求 4. 服务器处理请求并返回报文 5. 浏览器解析渲染页面 6. 链接结束 4. 为什么要使用线程池? 使用线程池的好处： - 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 - 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 - 提高线程的可管理性。 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 2. Python基础 1. 常用库 - requests - hashlib - demjson | json - bs4 - lxml - selenium 2. Python进程的内存结构 参考: https://www.jianshu.com/p/4e1c751d85f8 1. 进程分布如下: 1. text (code segment/text segment) 存放程序执行代码的一块内存区域,也包含一些只读常量. 2. data 通常用来存放程序中已初始化的全局变量数据,属于静态内存分配 3. bss (Block Started by Symbol) 通常用来存放程序中未初始化的全局变量 4. 堆(heap) 堆用于存放动态变量, 大小不固定,可动态扩张 5. 栈(stack) 3. Python的垃圾回收机制 参考： https://www.jianshu.com/p/1e375fb40506 1. Python采用Garbage collection垃圾收集机制，以`引用计数`为主,`标记-清除`和`分代收集`两种机制为辅 2. 对于`引用计数`机制,每个对象都是一个`PyObject`结构体,包含ob_refcnt作为引用计数,当ob_refcnt为0时,将被GC回收. - √简单 - √实时性, 平摊内存回收的时间 - X维护引用计数占用资源 - X循环引用会导致永久占用 4. 小的知识点： Float类型高精度转低精度 # 字符串化： d = ({:0.2f}).format(f) # round内置函数 round(d, 2) 快速排序sorted list x = [2, 9, 100, -1] sorted(x, reverse=True) >> [100, 9, 2, -1] # 但是x不变 , reverse=True为降序 # 如果x是复杂元祖 x = [(2, 9), (100, -1)] sorted(x, reverse=True, key= lambda item: item[1]) # lambda表达式 lambda item: item[1] ## 等价 def f(item): return item[1] dict x = [{'a':1}, {'b', 24}, {'c':44}, {'d':64}, {'e': 5}] # 默认按key排序 sorted(x, reverse=True) # 按value排序需指定key sorted(x, reverse=True, key=lambda k: x[k]) args 和 * kwargs *args 接收若干个(0-N)值 # 使用 def fun(*args): for arg in args: print arg fun(2, 3) fun(*[2, 3]) # *[2, 3, 5] 等价与 2, 3, 5 **kwargs 接收若干个(0-N)key=value # 用法 def fun(**kwargs): for key in kwargs: print(key, kwargs.get(key)) # 此时的kwargs将输入参数转成了dict fun(name='XX', age=18) fun(**{\"name\":\"xx\", \"age\":18}) # **{\"name\":\"xx\", \"age\":18} 等价与 name='XX', age=18 测试 def demo(*args, **kwargs): print(args, kwargs) >>> demo(2, 3, 5, name='mm', **{\"age\": 8}) (2, 3, 5) {'name': 'mm', 'age': 8} 5. 进程、线程和协程 6. 装饰器(Decorators) 3. django 1. 描述django一次请求的过程 描述 接收请求web服务器接收到来自客户端的请求 代理请求代理传给Python应用服务器(uWSGI或Gunicorn) 解析请求Python应用服务器根据WSGI协议解析成一个dict, 发送给Web框架 路由匹配Web框架初始化Handler对象, 进行路由匹配(urls.py) 视图函数进入url匹配的视图函数(views.py) 如果不涉及数据, 直接返回tmplate或Json响应 如果设计数据, 由Model 从数据库取数据, 填充到template后返回给视图函数, 生成响应 返回返回响应经中间件, 由应用服务器处理成响应response 处理请求response发给Web服务器 代理转发Web服务器将响应发回给客户端 4. Flask 1. Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "},"others/juzhuzheng.html":{"url":"others/juzhuzheng.html","title":"居住证办理","keywords":"","body":"办理居住证1. 网上申报2. 现场受理3. 签注办理居住证 身份证到期, 而异地更换身份证需要先办理居住证 服务网站 1. 网上申报 https://jzz.gaj.beijing.gov.cn/bjsjzzfwpt/usernew/spredire 点击申领居住卡 --> 填报 如实填写个人信息, 房东信息等 所属派出所和办理地点, 如果不清楚可以拨打客服电话查询 为了避免多跑一趟, 可以选择邮寄到家 准备资料: 房东提供 身份证复印件 房产证复印件(我只复印了带不动产编号的那一页, 最好都有) 个人信息: 电话, 现居住地等 自己准备 租房合同, 剩余租期要长于6个月 身份证 在京连续缴纳6个月社保（五险）的凭证，(6月的居住证办理, 打印11-4共6个月) 打印地址, 提前一天预约(http://fuwu.rsj.beijing.gov.cn/csibiz/indinfo/login.jsp) 参考流程 https://zhuanlan.zhihu.com/p/66020555 2. 现场受理 注意申报是预约的受理地点, 带好物品现场办理即可 3. 签注 关注北京居住证微信公众号 选择居住证签注, 如果地址未变更可直接替换, 变更需要上传租房合同照片即可 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-01 11:20:21 "},"others/fitness.html":{"url":"others/fitness.html","title":"健身相关","keywords":"","body":"Fitness1. 原则2. 现状3. 计划4. 动作Fitness [TOC] 1. 原则 持续关注的几点 安全 持续 进步 科学 2. 现状 目前已有 跑步三年多, 自我感觉良好的心肺能力 恢复能力和耐力较好的下肢肌肉(体现着靠墙静蹲锻炼后无酸痛感, 而上肢就不行) 瘦, 肩颈容易酸痛 偷懒拉伸导致岌岌可危的膝盖 一个月的游泳 3. 计划 近一个月的健身房体验和半年的规划 有氧保持水平(每周三次, 椭圆机、跑步、游泳、有氧操等)(半小时以上) 核心 平板支撑练习核心力量(每周三次)(60*3) 4. 动作 胸部 动作详解 蝴蝶机夹胸 4-6组, 12-15次 双杠臂屈伸 4-6组, 8-12次 坐姿卧推 4-6组, 8-12次 肩部 肩胛骨放松 弹力带前拉 肩胛后收 Y形上举 肩胛上旋转 直臂下拉 肩胛下旋转 悬挂肩胛下沉 强化斜方肌下部 背部 背部解释 动作图解 发力练习, 想象肘部发力上啦, 练习肩胛骨收紧 引体向上 4-6组, 12-15次 颈前下拉 颈后下拉 坐姿划船 腰臀 臀桥 3-4组, 30次以上 练习: 臀大肌, 髋部, 下背部 髋部放置杠铃负重(前期高容量训练, 后期高负重低次数) 山羊挺身 3-4组, 15-30次 锻炼部位: 腰脊肌 上挺时吸气,1s, 前曲时呼吸, 2s, 停留1s 俯卧两头起 3-4组, 30-60s 锻炼部位: 腰脊肌 俯卧交替挺身 3-4组, 左右各15-30s 锻炼部位: 下背后腰, 臀部 腹部 靠背举腿 2-3组, 10-30次 仰卧举腿 2-3组, 10-30次 更多动作 腿部 深蹲 2-3组, 10-30次 靠墙静蹲 4组, 50-100s 更多动作 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-06-01 11:22:58 "},"others/swiming.html":{"url":"others/swiming.html","title":"游泳","keywords":"","body":"游泳1. 水性2. 关节3. 协调联系游泳 1. 水性 换气 漂浮 趴着漂和躺着飘 前滚翻和后滚翻, 倒立 踩水 2. 关节 压肩 正压肩 反压肩 侧压肩 肩绕环 压踝关节 3. 协调联系 交叉跳 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2020-01-07 10:37:25 "},"00.html":{"url":"00.html","title":"00","keywords":"","body":" Few things I'd like to touch upon # 有几件事我想说一下 Copyright © maxiaoteng.xyz 2019 all right reserved，powered by Gitbook该文件修订时间： 2019-03-25 11:01:52 "}}