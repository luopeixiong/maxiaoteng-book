# crawlab 试用

Crawlab 是一个分布式爬虫管理平台, 主要解决的是大量爬虫管理困难的问题, [参考](https://www.crawlab.cn/why-crawlab/index.html)

## 1. 调研爬虫管理框架, 主要希望通过系统来解决如下问题

1. 爬虫管理
    1. 目前基本是谁开发, 谁部署, 谁管理
    2. crawlab
        1. 权限管理, 目前支持管理用户和普通用户(查看并操作自己的数据)
        2. 公共爬虫, 所有用户都可以进行看到, 但只有管理员可以修改爬虫
2. 爬虫部署
    1. 目前部署, 使用github同步代码到特定服务器
    2. crawlab部署
        1. 上传可以通过本地zip上传, 也可以在安装crawlab-sdk后, 命令行执行`crawlab upload`
        2. 文件存储在主节点的MongoDB GridFS中, 主节点5s更新爬虫信息, 60s更新一次文件
        3. 主节点通过redis发布爬虫文件上传信息, 通知工作节点获取爬虫文件
        4. 工作节点获取到爬虫文件的消息后, 从MongoDB GridFS获取zip文件并解压到本地
        5. 持续部署
            1. 支持git管理, 可以从github, gitlab和gitee上拉取代码
            2. 可选手动和自动同步爬虫
            3. Crawlab 的主节点（Master Node）将利用 git pull 操作将 Git 仓库的爬虫代码拉取（Pull）到主节点本地
            4. 主节点将拉取下来的爬虫代码通过 zip 打包的方式上传到 MongoDB GridFS
            5. 爬虫代码将被自动同步到所有在线节点
    3. 结果展示上, crawlab可以可视化查看所有项目已经各项目的运行状态
3. 爬虫调度
    1. 目前的做法, 使用命令行手动操作,配置crontab或依赖apscheduler启动进程来控制爬虫
    2. crawlab
        1. 通过可视化配置定时任务, 类crontab配置
4. 日志采集
    1. 目前的做法, 日志保留在本地
    2. crawlab
        1. `捕获日志`通过Stdout标准输出流来捕获日志的，因此如果希望在 Crawlab 的界面中看到日志，就需要让日志内容输出到Stdout中。最简单的做法就是打印出来
        2. `异常检测`原理是通过正则表达式来完成的。默认会用 error、exception、traceback 来匹配日志内容判断该日志文本是否为错误日志
        3. `存储`日志和异常日志是分别储存在 MongoDB 数据库的 logs 和 error_logs collection 中的
        4. 支持不好, 可能`需要考虑其他监控方案`
5. 数据监控和写入
    1. 目前的做法, 通过爬虫汇报爬取结果数量, 结合监控系统来对爬取结果做监控
    2. crawlab:
        1. Crawlab 默认是用 MongoDB 作为储存结果的数据库，而且要求结果数据必须与 Crawlab 运行数据库是同一个数据库
        2. `专业版` 做了更多数据源的集成, 目前支持了 MySQL、Postgres、ElasticSearch 数据库，以及 Kafka 消息队列
    3. 考虑到目前大部分爬虫如kafka, 可以灵活安排,不必须集成:
        1. 此处crawlab其实支持的也不好, 至少需要配置双写到mongo, 可能`需要考虑其他监控方案`
6. 服务器管理
    1. 目前的做法不存在子节点管理, 每个服务器都作为单独存在, 资源监控以及新的爬虫部署在哪台服务器, 依赖个人选择
    2. crawlab子节点管理
        1. 添加子节点可以无需再登录这台设备部署和启动爬虫
        2. 添加子节点支持直接部署, docker和Kubernetes
        3. 通过给redis发送心跳消息来保持在线状态
        4. `专业版`可以对MongoDB 数据库、Redis 数据库、主节点、工作节点的性能数据进行[监控](https://docs.crawlab.cn/zh/Monitor/)
    3. 环境管理
        1. 目前每台电脑都是独立的, 可以随时配置修改依赖
        2. crawlab的子节点安装依赖
            1. 自动安装依赖, 爬虫根目录读取`requirements.txt`
            2. 通过可视化界面给子节点发送安装依赖命令
            3. 可以添加脚本, 在配置节点时完成依赖安装
7. 灵活性担忧
    1. 目前crawlab多节点启动任务, 只支持随机/指定节点/全部节点这三种, 意味着如果需要2-5个节点同时运行, 是不是就得创建同样多的任务
    2. 如果被识别为scrapy爬虫, 执行命令默认`scrapy crawl`, 显示中如果有需要同步执行一些顺序操作, 之前会写在一个`.py`文件中执行, 现在可能需要考虑其他方案


# 问题
1. 随机的逻辑如何选择
    1. 看代码
2. 对性能的整合
3. 灵活度
    1. 如果在spider的close里整合下一个流程, 影响灵活性 
    2. spider之间会有关联, 不是完全解藕